<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/linux/ffmpeg/A053/">
    <link rel="shortcut icon" href="../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>FFMPEG API 사용법 - 소프트웨어 융합</title>
    <link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../css/highlight.css">
    <link href="../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "FFmpeg API \uac1c\uc694", url: "#_top", children: [
          ]},
          {title: "FFmpeg \ub77c\uc774\ube0c\ub7ec\ub9ac \ucd08\uae30\ud654", url: "#ffmpeg", children: [
          ]},
          {title: "\uc785\ub825 \ud30c\uc77c \uc5f4\uae30", url: "#_1", children: [
          ]},
          {title: "\ucf54\ub371 \uc815\ubcf4 \uac80\uc0c9", url: "#_2", children: [
          ]},
          {title: "\ud328\ud0b7 \ub514\ucf54\ub529", url: "#_3", children: [
          ]},
          {title: "\uba54\ubaa8\ub9ac \uad00\ub9ac", url: "#_4", children: [
          ]},
          {title: "FFmpeg\uc758 \uc2dc\uac04 \ucd95 \ubcc0\ud658", url: "#ffmpeg_1", children: [
          ]},
          {title: "\ube44\ub514\uc624 \ubc0f \uc624\ub514\uc624 \uc2a4\ud2b8\ub9bc \ucc98\ub9ac", url: "#_5", children: [
              {title: "\ube44\ub514\uc624 \uc2a4\ud2b8\ub9bc \ucc98\ub9ac", url: "#_6" },
              {title: "\uc624\ub514\uc624 \uc2a4\ud2b8\ub9bc \ucc98\ub9ac", url: "#_7" },
              {title: "\ube44\ub514\uc624 \ubc0f \uc624\ub514\uc624 \uc2a4\ud2b8\ub9bc\uc758 \ub3d9\uae30\ud654", url: "#_8" },
          ]},
          {title: "FFmpeg API\uc758 \ud655\uc7a5 \ubc0f \uc0ac\uc6a9\uc790 \uc815\uc758 \ud544\ud130", url: "#ffmpeg-api_1", children: [
          ]},
          {title: "\ube44\ub514\uc624 \ubc0f \uc624\ub514\uc624 \ud504\ub808\uc784\uc758 \ucc98\ub9ac \ud750\ub984", url: "#_9", children: [
              {title: "\ub514\ucf54\ub529", url: "#_10" },
              {title: "\ud544\ud130\ub9c1", url: "#_11" },
              {title: "\uc778\ucf54\ub529", url: "#_12" },
          ]},
          {title: "\uc2dc\uac04 \uc870\uc815\uacfc \ub3d9\uae30\ud654", url: "#_13", children: [
          ]},
          {title: "FFmpeg\uc758 \ub9ac\uc0d8\ud50c\ub9c1 \ubc0f \uc2a4\ucf00\uc77c\ub9c1", url: "#ffmpeg_2", children: [
              {title: "\uc624\ub514\uc624 \ub9ac\uc0d8\ud50c\ub9c1", url: "#_14" },
              {title: "\ube44\ub514\uc624 \uc2a4\ucf00\uc77c\ub9c1", url: "#_15" },
          ]},
          {title: "FFmpeg\uc758 \uba40\ud2f0\uc2a4\ub808\ub4dc \ucc98\ub9ac", url: "#ffmpeg_3", children: [
              {title: "\uc2a4\ub808\ub4dc \uac1c\uc218 \uc124\uc815", url: "#_16" },
              {title: "\uc2a4\ub808\ub4dc \ucc98\ub9ac \ubaa8\ub4dc", url: "#_17" },
              {title: "\uba40\ud2f0\uc2a4\ub808\ub4dc \ub514\ucf54\ub529 \uc608\uc2dc", url: "#_18" },
          ]},
          {title: "FFmpeg\uc758 \ubc84\ud37c \uad00\ub9ac", url: "#ffmpeg_4", children: [
              {title: "AVBufferRef \uad6c\uc870\uccb4", url: "#avbufferref" },
              {title: "AVFrame\uacfc AVBufferRef", url: "#avframe-avbufferref" },
              {title: "\ubc84\ud37c \ubcf5\uc0ac \ubc0f \uacf5\uc720", url: "#_19" },
          ]},
          {title: "FFmpeg API\uc5d0\uc11c \uc624\ub958 \ucc98\ub9ac", url: "#ffmpeg-api_2", children: [
              {title: "\uc624\ub958 \ucf54\ub4dc\uc758 \ucc98\ub9ac", url: "#_20" },
              {title: "\uc624\ub958 \uba54\uc2dc\uc9c0 \ucd9c\ub825", url: "#_21" },
          ]},
          {title: "FFmpeg API\uc758 \uba54\ubaa8\ub9ac \uad00\ub9ac", url: "#ffmpeg-api_3", children: [
              {title: "AVFrame\uc758 \uba54\ubaa8\ub9ac \ud560\ub2f9 \ubc0f \ud574\uc81c", url: "#avframe" },
              {title: "AVPacket\uc758 \uba54\ubaa8\ub9ac \ud560\ub2f9 \ubc0f \ud574\uc81c", url: "#avpacket" },
              {title: "\ucee8\ud14d\uc2a4\ud2b8 \uba54\ubaa8\ub9ac \uad00\ub9ac", url: "#_22" },
              {title: "AVFilterContext\uc758 \uba54\ubaa8\ub9ac \ud574\uc81c", url: "#avfiltercontext" },
          ]},
          {title: "FFmpeg\uc758 \ud30c\uc77c \ucd9c\ub825", url: "#ffmpeg_5", children: [
              {title: "\ucd9c\ub825 \ud30c\uc77c \uc0dd\uc131", url: "#_23" },
              {title: "\ucd9c\ub825 \uc2a4\ud2b8\ub9bc \uc0dd\uc131", url: "#_24" },
              {title: "\ud30c\uc77c\ub85c \ud328\ud0b7 \uae30\ub85d", url: "#_25" },
              {title: "\ucd9c\ub825 \ud30c\uc77c \ub2eb\uae30", url: "#_26" },
          ]},
          {title: "FFmpeg\uc758 \ub514\ubc84\uae45\uacfc \ub85c\uae45", url: "#ffmpeg_6", children: [
              {title: "\ub85c\uae45 \ub808\ubca8 \uc124\uc815", url: "#_27" },
              {title: "\uc0ac\uc6a9\uc790 \uc815\uc758 \ub85c\uae45 \ud568\uc218", url: "#_28" },
          ]},
          {title: "FFmpeg\uc758 \uc2a4\ud2b8\ub9ac\ubc0d \ucc98\ub9ac", url: "#ffmpeg_7", children: [
              {title: "\uc785\ub825 \uc2a4\ud2b8\ub9bc \ucc98\ub9ac", url: "#_29" },
              {title: "\ucd9c\ub825 \uc2a4\ud2b8\ub9bc \uc124\uc815", url: "#_30" },
              {title: "\uc2a4\ud2b8\ub9bc \ucd08\uae30\ud654 \ubc0f \uc5f0\uacb0", url: "#_31" },
              {title: "\uc2a4\ud2b8\ub9ac\ubc0d \uc885\ub8cc", url: "#_32" },
          ]},
          {title: "\ub77c\uc774\ube0c \uc2a4\ud2b8\ub9ac\ubc0d \ubcc0\ud658", url: "#_33", children: [
              {title: "\uc785\ub825 \uc2a4\ud2b8\ub9bc \uc124\uc815", url: "#_34" },
              {title: "\ucd9c\ub825 \uc2a4\ud2b8\ub9bc \uc124\uc815", url: "#_35" },
              {title: "\uc2a4\ud2b8\ub9bc \ubcf5\uc0ac \ubc0f \uc804\uc1a1", url: "#_36" },
          ]},
          {title: "\uc2a4\ud2b8\ub9ac\ubc0d \uc911 \ube44\ud2b8\ub808\uc774\ud2b8 \uc870\uc815", url: "#_37", children: [
          ]},
          {title: "\uc2a4\ud2b8\ub9ac\ubc0d \uc911 \ub2e4\uc911 \ud2b8\ub799 \uc9c0\uc6d0", url: "#_38", children: [
              {title: "\ub2e4\uc911 \ud2b8\ub799 \uc2a4\ud2b8\ub9bc \uc0dd\uc131", url: "#_39" },
              {title: "\ud2b8\ub799\ubcc4 \ub370\uc774\ud130 \ucc98\ub9ac", url: "#_40" },
              {title: "\ub2e4\uc911 \ud2b8\ub799 \ub3d9\uae30\ud654", url: "#_41" },
          ]},
          {title: "FFmpeg\uc758 \ube44\ub514\uc624 \ubc0f \uc624\ub514\uc624 \ud544\ud130\ub9c1", url: "#ffmpeg_8", children: [
              {title: "\ube44\ub514\uc624 \ud544\ud130 \uc801\uc6a9", url: "#_42" },
              {title: "\uc624\ub514\uc624 \ud544\ud130 \uc801\uc6a9", url: "#_43" },
          ]},
          {title: "FFmpeg\uc758 GPU \uac00\uc18d", url: "#ffmpeg-gpu", children: [
              {title: "GPU \uc778\ucf54\ub529 \ud65c\uc131\ud654", url: "#gpu" },
              {title: "GPU \ub514\ucf54\ub529 \ud65c\uc131\ud654", url: "#gpu_1" },
          ]},
          {title: "FFmpeg\uc758 \ub2e4\uc911 \ud30c\uc77c \ucc98\ub9ac", url: "#ffmpeg_9", children: [
              {title: "\uc5ec\ub7ec \uc785\ub825 \ud30c\uc77c \ubcd1\ud569", url: "#_44" },
              {title: "\ud30c\uc77c \ubd84\ud560", url: "#_45" },
          ]},
          {title: "\ud30c\uc77c \uae38\uc774 \uc870\uc815", url: "#_46", children: [
          ]},
          {title: "\ud30c\uc77c \ubcc0\ud658 \uc911 \uc624\ub958 \ucc98\ub9ac", url: "#_47", children: [
              {title: "\uc624\ub958 \ucf54\ub4dc \ud655\uc778", url: "#_48" },
              {title: "\uc624\ub958 \uba54\uc2dc\uc9c0 \ucd9c\ub825", url: "#_49" },
          ]},
        ];

    </script>
    <script src="../../../js/base.js"></script>
      <script src="../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../A054/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../A054/" class="btn btn-xs btn-link">
        FFMPEG 라이브러리 컴파일
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../A052/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../A052/" class="btn btn-xs btn-link">
        FFMPEG 스크립트 작성
      </a>
    </div>
    
  </div>

    

    <h3 id="ffmpeg-api">FFmpeg API 개요</h3>
<p>FFmpeg는 멀티미디어 데이터를 다루기 위한 강력한 라이브러리를 제공하며, 이를 통해 비디오, 오디오 인코딩 및 디코딩, 포맷 변환, 스트리밍 등을 제어할 수 있다. FFmpeg의 API는 다양한 기능을 C 언어 기반으로 제공하며, 개발자는 이 API를 통해 복잡한 멀티미디어 작업을 프로그래밍적으로 처리할 수 있다.</p>
<p>FFmpeg API는 크게 네 가지 주요 라이브러리로 구성된다:</p>
<ol>
<li><strong>libavcodec</strong>: 다양한 비디오 및 오디오 코덱을 위한 라이브러리이다.</li>
<li><strong>libavformat</strong>: 멀티미디어 컨테이너 포맷을 처리하는 라이브러리이다.</li>
<li><strong>libavutil</strong>: 유틸리티 함수 및 공통 자료 구조를 제공한다.</li>
<li><strong>libswscale</strong>: 이미지 크기 변경, 포맷 변환 등의 작업을 처리한다.</li>
</ol>
<p>이외에도 하드웨어 가속을 위한 <strong>libavdevice</strong>, <strong>libswresample</strong> (오디오 리샘플링), <strong>libpostproc</strong> (포스트 프로세싱) 등의 라이브러리도 존재한다.</p>
<h3 id="ffmpeg">FFmpeg 라이브러리 초기화</h3>
<p>FFmpeg API를 사용하려면 먼저 각 라이브러리를 초기화해야 한다. 이를 위해 <code>av_register_all()</code>을 호출한다. 이 함수는 모든 코덱, 파일 포맷, 네트워크 지원을 초기화하는 데 사용된다.</p>
<pre><code class="language-c">av_register_all();
</code></pre>
<p>FFmpeg 라이브러리는 입력 또는 출력 컨텍스트를 기반으로 데이터를 처리한다. 파일을 열기 위해서는 먼저 <code>AVFormatContext</code> 구조체를 사용해야 하며, 이를 통해 파일 포맷, 코덱, 스트림 등에 대한 정보를 읽어들일 수 있다.</p>
<h3 id="_1">입력 파일 열기</h3>
<p>파일을 열 때 <code>avformat_open_input()</code>을 사용하여 파일을 연결한다. 이 함수는 입력 파일의 경로와 컨텍스트를 매개변수로 받아들이다.</p>
<pre><code class="language-c">AVFormatContext *fmt_ctx = NULL;
avformat_open_input(&amp;fmt_ctx, &quot;input.mp4&quot;, NULL, NULL);
</code></pre>
<p>이후 파일의 스트림 정보를 얻기 위해 <code>avformat_find_stream_info()</code> 함수를 호출하여 파일에 포함된 오디오 및 비디오 스트림에 대한 정보를 추출한다.</p>
<pre><code class="language-c">avformat_find_stream_info(fmt_ctx, NULL);
</code></pre>
<h3 id="_2">코덱 정보 검색</h3>
<p>각 스트림은 서로 다른 코덱을 사용할 수 있으며, 스트림에 연결된 코덱 정보를 얻기 위해서는 스트림의 코덱 파라미터를 탐색해야 한다. FFmpeg에서는 <code>AVCodecParameters</code> 구조체를 사용하여 코덱 정보를 가져올 수 있다. 스트림의 코덱을 검색하려면 먼저 해당 스트림을 선택하고, <code>avcodec_parameters_to_context()</code> 함수를 사용해 파라미터를 설정한다.</p>
<pre><code class="language-c">AVCodec *codec = avcodec_find_decoder(fmt_ctx-&gt;streams[stream_idx]-&gt;codecpar-&gt;codec_id);
</code></pre>
<p>코덱을 찾았다면, <code>AVCodecContext</code>를 할당하고 코덱에 대한 정보를 해당 컨텍스트에 채운다.</p>
<pre><code class="language-c">AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
avcodec_parameters_to_context(codec_ctx, fmt_ctx-&gt;streams[stream_idx]-&gt;codecpar);
</code></pre>
<p>이후 코덱을 열기 위해 <code>avcodec_open2()</code> 함수를 사용하여 코덱을 초기화한다.</p>
<pre><code class="language-c">avcodec_open2(codec_ctx, codec, NULL);
</code></pre>
<h3 id="_3">패킷 디코딩</h3>
<p>FFmpeg의 디코딩 과정은 기본적으로 패킷을 읽어들인 후 이를 프레임으로 변환하는 방식으로 진행된다. 파일로부터 패킷을 읽기 위해서는 <code>av_read_frame()</code>을 사용한다.</p>
<pre><code class="language-c">AVPacket pkt;
av_read_frame(fmt_ctx, &amp;pkt);
</code></pre>
<p>그 다음 패킷을 코덱에 전달하여 디코딩을 진행한다.</p>
<pre><code class="language-c">avcodec_send_packet(codec_ctx, &amp;pkt);
</code></pre>
<p>이후 디코딩된 프레임을 수신하기 위해 <code>avcodec_receive_frame()</code>을 사용한다.</p>
<pre><code class="language-c">AVFrame *frame = av_frame_alloc();
avcodec_receive_frame(codec_ctx, frame);
</code></pre>
<h3 id="_4">메모리 관리</h3>
<p>FFmpeg에서는 할당된 메모리를 명시적으로 해제해야 한다. 예를 들어, 디코딩에 사용한 프레임과 패킷은 각각 <code>av_frame_free()</code>와 <code>av_packet_unref()</code>을 사용하여 해제한다.</p>
<pre><code class="language-c">av_frame_free(&amp;frame);
av_packet_unref(&amp;pkt);
</code></pre>
<h3 id="ffmpeg_1">FFmpeg의 시간 축 변환</h3>
<p>멀티미디어 데이터를 처리할 때, 서로 다른 시간 축을 가진 여러 스트림을 다루게 된다. FFmpeg에서는 <code>AVStream</code> 구조체에 포함된 <code>time_base</code>를 사용하여 시간 축을 변환할 수 있다. 시간 변환은 아래와 같은 방식으로 이루어진다:</p>
<ul>
<li>입력 시간 값을 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{T_{in}}</span><script type="math/tex">\mathbf{T_{in}}</script></span>, 입력 스트림의 시간 단위를 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{t_{in}}</span><script type="math/tex">\mathbf{t_{in}}</script></span>, 출력 스트림의 시간 단위를 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{t_{out}}</span><script type="math/tex">\mathbf{t_{out}}</script></span>라고 할 때, 출력 시간 값 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{T_{out}}</span><script type="math/tex">\mathbf{T_{out}}</script></span>는 다음과 같이 계산된다:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{T_{out}} = \frac{\mathbf{T_{in}} \times \mathbf{t_{out}}}{\mathbf{t_{in}}}
</div>
<script type="math/tex; mode=display">
\mathbf{T_{out}} = \frac{\mathbf{T_{in}} \times \mathbf{t_{out}}}{\mathbf{t_{in}}}
</script>
</div>
<p>예를 들어, 입력 스트림의 시간 단위가 1/1000초이고, 출력 스트림의 시간 단위가 1/90000초인 경우, 시간 변환은 아래와 같이 수행된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{T_{out}} = \frac{\mathbf{T_{in}} \times 90000}{1000}
</div>
<script type="math/tex; mode=display">
\mathbf{T_{out}} = \frac{\mathbf{T_{in}} \times 90000}{1000}
</script>
</div>
<p>이 계산을 통해 서로 다른 시간 단위를 가진 스트림 간의 동기화를 처리할 수 있다.</p>
<h3 id="_5">비디오 및 오디오 스트림 처리</h3>
<p>FFmpeg API를 사용하여 비디오와 오디오 스트림을 처리하는 과정은 기본적으로 비슷하지만, 비디오 스트림은 주로 프레임을 기반으로 처리되고, 오디오 스트림은 샘플 단위로 처리된다. 각각의 스트림에 맞게 프레임 또는 샘플을 추출한 후 코덱을 통해 디코딩한다.</p>
<h4 id="_6">비디오 스트림 처리</h4>
<p>비디오 스트림을 처리할 때, 각 프레임의 시간 정보를 기반으로 프레임을 동기화하고, 필요한 경우 프레임을 리샘플링하거나 변환한다. 비디오 스트림에서 프레임을 추출하려면 <code>av_read_frame()</code>으로 패킷을 읽고, 이를 디코딩하여 <code>AVFrame</code> 구조체에 저장한다.</p>
<p>비디오 스트림에서 추출된 프레임은 픽셀 포맷이 서로 다를 수 있기 때문에, <strong>sws_scale</strong> 함수를 사용하여 픽셀 포맷을 변환한다. 이를 위해 먼저 <code>SwsContext</code>를 초기화해야 한다.</p>
<pre><code class="language-c">struct SwsContext *sws_ctx = sws_getContext(
    codec_ctx-&gt;width, codec_ctx-&gt;height, codec_ctx-&gt;pix_fmt,  // 입력 해상도 및 픽셀 포맷
    codec_ctx-&gt;width, codec_ctx-&gt;height, AV_PIX_FMT_RGB24,    // 출력 해상도 및 픽셀 포맷
    SWS_BILINEAR, NULL, NULL, NULL);                         // 스케일링 방법 및 필터 설정
</code></pre>
<p><code>avcodec_receive_frame()</code>으로 디코딩된 프레임을 받은 후, <code>sws_scale()</code> 함수를 사용하여 변환된 프레임을 처리한다.</p>
<pre><code class="language-c">sws_scale(sws_ctx, (const uint8_t *const *)frame-&gt;data, frame-&gt;linesize, 0,
          codec_ctx-&gt;height, rgb_frame-&gt;data, rgb_frame-&gt;linesize);
</code></pre>
<h4 id="_7">오디오 스트림 처리</h4>
<p>오디오 스트림의 경우, 각 패킷은 오디오 샘플을 포함하며, 이를 디코딩하여 PCM 형식의 샘플로 변환한다. 오디오 데이터는 샘플 포맷, 샘플링 레이트, 채널 수 등에 따라 다르기 때문에 FFmpeg의 <strong>libswresample</strong> 라이브러리를 사용하여 오디오 샘플을 변환할 수 있다.</p>
<p><code>SwrContext</code>를 초기화하여 오디오 샘플링 속도, 채널 레이아웃, 샘플 포맷 등을 변환할 수 있다.</p>
<pre><code class="language-c">struct SwrContext *swr_ctx = swr_alloc_set_opts(
    NULL, AV_CH_LAYOUT_STEREO, AV_SAMPLE_FMT_S16, 44100,      // 출력 포맷
    codec_ctx-&gt;channel_layout, codec_ctx-&gt;sample_fmt, codec_ctx-&gt;sample_rate, 0, NULL);
swr_init(swr_ctx);
</code></pre>
<p>오디오 데이터를 변환하려면 먼저 디코딩된 프레임을 <code>swr_convert()</code> 함수를 통해 변환한다.</p>
<pre><code class="language-c">swr_convert(swr_ctx, &amp;out_samples, output_samples, frame-&gt;data, frame-&gt;nb_samples);
</code></pre>
<h4 id="_8">비디오 및 오디오 스트림의 동기화</h4>
<p>FFmpeg에서 비디오와 오디오 스트림을 동기화하는 방법 중 하나는 <strong>PTS</strong>(Presentation Time Stamp)를 사용하는 것이다. 각 프레임이나 오디오 샘플의 PTS는 그 프레임이나 샘플이 재생될 시간 정보를 제공한다.</p>
<p>비디오의 경우, 각 프레임의 PTS는 다음과 같은 방식으로 계산된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{PTS_{frame}} = \mathbf{PTS_{start}} + \left( \frac{\mathbf{frame\_index}}{\mathbf{frame\_rate}} \right)
</div>
<script type="math/tex; mode=display">
\mathbf{PTS_{frame}} = \mathbf{PTS_{start}} + \left( \frac{\mathbf{frame\_index}}{\mathbf{frame\_rate}} \right)
</script>
</div>
<p>오디오 샘플의 경우, PTS는 각 샘플의 수에 따라 증가하며, 오디오 샘플 레이트를 사용하여 계산된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{PTS_{audio}} = \mathbf{PTS_{start}} + \left( \frac{\mathbf{sample\_index}}{\mathbf{sample\_rate}} \right)
</div>
<script type="math/tex; mode=display">
\mathbf{PTS_{audio}} = \mathbf{PTS_{start}} + \left( \frac{\mathbf{sample\_index}}{\mathbf{sample\_rate}} \right)
</script>
</div>
<p>비디오와 오디오의 동기화는 두 스트림의 PTS 값을 비교하여 결정되며, 일반적으로 오디오의 PTS를 기준으로 동기화를 맞춘다.</p>
<h3 id="ffmpeg-api_1">FFmpeg API의 확장 및 사용자 정의 필터</h3>
<p>FFmpeg API는 다양한 필터링 기능을 제공하며, 이를 활용하여 비디오와 오디오 스트림을 처리할 수 있다. <strong>AVFilterGraph</strong> 구조체는 필터 그래프를 생성하고 관리하는 데 사용된다. 필터 그래프를 구성하려면 먼저 <code>avfilter_graph_alloc()</code>을 호출하여 필터 그래프를 초기화한다.</p>
<pre><code class="language-c">AVFilterGraph *filter_graph = avfilter_graph_alloc();
</code></pre>
<p>필터를 그래프에 추가하려면 <code>avfilter_graph_create_filter()</code> 함수를 사용한다. 예를 들어, 비디오 스트림에 대해 <strong>scale</strong> 필터를 적용하여 해상도를 변경할 수 있다.</p>
<pre><code class="language-c">AVFilterContext *buffersrc_ctx = NULL;
AVFilterContext *buffersink_ctx = NULL;
avfilter_graph_create_filter(&amp;buffersrc_ctx, avfilter_get_by_name(&quot;buffer&quot;), &quot;in&quot;, args, NULL, filter_graph);
avfilter_graph_create_filter(&amp;buffersink_ctx, avfilter_get_by_name(&quot;buffersink&quot;), &quot;out&quot;, NULL, NULL, filter_graph);
</code></pre>
<p>그 후, <code>avfilter_graph_parse_ptr()</code> 함수를 통해 필터 명령어를 적용하고, 그래프를 링크한다.</p>
<pre><code class="language-c">avfilter_graph_parse_ptr(filter_graph, &quot;scale=1280:720&quot;, NULL, NULL, NULL);
avfilter_graph_config(filter_graph, NULL);
</code></pre>
<p>필터링된 프레임은 필터 그래프에서 처리된 후 새로운 <code>AVFrame</code> 구조체로 반환된다.</p>
<h3 id="_9">비디오 및 오디오 프레임의 처리 흐름</h3>
<p>FFmpeg API에서 비디오와 오디오 데이터를 처리하는 흐름은 크게 세 단계로 나뉜다: <strong>디코딩</strong>, <strong>필터링</strong>, <strong>인코딩</strong>. 디코딩된 데이터를 필터링한 후 다시 인코딩하여 파일로 출력하는 방식이다.</p>
<h4 id="_10">디코딩</h4>
<p>디코딩은 입력 파일로부터 데이터를 추출하여 FFmpeg에서 이해할 수 있는 형식으로 변환하는 과정이다. 비디오 및 오디오 패킷을 읽고 코덱을 통해 디코딩된 후, 처리할 수 있는 프레임 형태로 변환된다.</p>
<p>예를 들어, 비디오 프레임을 디코딩하는 과정은 다음과 같다:</p>
<pre><code class="language-c">while (av_read_frame(fmt_ctx, &amp;pkt) &gt;= 0) {
    if (pkt.stream_index == video_stream_index) {
        avcodec_send_packet(codec_ctx, &amp;pkt);
        avcodec_receive_frame(codec_ctx, frame);

        // 디코딩된 프레임을 필터링이나 후속 처리
    }
    av_packet_unref(&amp;pkt);
}
</code></pre>
<h4 id="_11">필터링</h4>
<p>디코딩된 프레임을 필터링할 때, 사용자가 설정한 필터 그래프를 통해 처리된다. 필터링 과정에서는 비디오 크기 조정, 색상 조정, 오디오 리샘플링 등 다양한 처리를 할 수 있다.</p>
<p>예를 들어, <strong>비디오 스케일링</strong> 필터를 사용하여 해상도를 1280x720으로 조정할 수 있다. 필터링된 프레임은 새로운 프레임으로 반환되며, 이를 인코딩하거나 후속 처리를 진행할 수 있다.</p>
<pre><code class="language-c">av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF);
av_buffersink_get_frame(buffersink_ctx, filt_frame);
</code></pre>
<h4 id="_12">인코딩</h4>
<p>필터링된 프레임은 다시 인코딩되어 출력 파일에 기록된다. 인코딩은 <strong>libavcodec</strong> 라이브러리의 <code>avcodec_send_frame()</code>과 <code>avcodec_receive_packet()</code> 함수를 사용하여 이루어진다.</p>
<pre><code class="language-c">avcodec_send_frame(out_codec_ctx, filt_frame);
avcodec_receive_packet(out_codec_ctx, &amp;out_pkt);
</code></pre>
<p>인코딩된 패킷은 파일로 쓰기 위해 <code>av_write_frame()</code>을 사용하여 출력 스트림에 전달된다.</p>
<pre><code class="language-c">av_write_frame(out_fmt_ctx, &amp;out_pkt);
</code></pre>
<p>인코딩 과정을 거치면 파일 포맷과 코덱에 맞게 압축된 형태로 저장된다.</p>
<h3 id="_13">시간 조정과 동기화</h3>
<p>비디오와 오디오 데이터를 처리하는 동안 <strong>시간 동기화</strong>는 매우 중요하다. 비디오와 오디오가 동시에 재생될 수 있도록 하기 위해 <strong>PTS</strong>(Presentation Time Stamp)와 <strong>DTS</strong>(Decoding Time Stamp)를 적절히 관리해야 한다.</p>
<p>비디오와 오디오의 시간 정보는 다음 수식을 통해 동기화된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{PTS_{sync}} = \frac{\mathbf{PTS_{video}} - \mathbf{PTS_{audio}}}{\mathbf{time\_base}}
</div>
<script type="math/tex; mode=display">
\mathbf{PTS_{sync}} = \frac{\mathbf{PTS_{video}} - \mathbf{PTS_{audio}}}{\mathbf{time\_base}}
</script>
</div>
<p>이 수식을 통해 현재 프레임이 재생되어야 하는 시간을 조정하고, 오디오와 비디오가 동기화된 상태로 유지된다.</p>
<h3 id="ffmpeg_2">FFmpeg의 리샘플링 및 스케일링</h3>
<p>비디오와 오디오 데이터는 다양한 샘플링 레이트와 해상도를 가질 수 있으므로, 리샘플링과 스케일링 과정이 필요하다. <strong>libswscale</strong>와 <strong>libswresample</strong> 라이브러리는 각각 비디오 프레임의 스케일링과 오디오 샘플의 리샘플링을 처리한다.</p>
<h4 id="_14">오디오 리샘플링</h4>
<p>오디오 데이터를 다른 샘플링 레이트나 포맷으로 변환할 때 <code>swr_convert()</code> 함수를 사용한다. 예를 들어, 44.1kHz의 오디오 데이터를 48kHz로 변환할 때 아래와 같이 코드를 작성한다:</p>
<pre><code class="language-c">swr_convert(swr_ctx, out_samples, output_samples, in_samples, input_samples);
</code></pre>
<h4 id="_15">비디오 스케일링</h4>
<p>비디오 프레임의 해상도를 변환하려면 <code>sws_scale()</code> 함수를 사용한다. 아래는 비디오 프레임을 1920x1080 해상도로 변환하는 예이다:</p>
<pre><code class="language-c">sws_scale(sws_ctx, (const uint8_t *const *)frame-&gt;data, frame-&gt;linesize, 0,
          codec_ctx-&gt;height, out_frame-&gt;data, out_frame-&gt;linesize);
</code></pre>
<p>이 과정을 통해 다양한 해상도와 샘플링 레이트를 가진 데이터를 효율적으로 처리할 수 있다.</p>
<h3 id="ffmpeg_3">FFmpeg의 멀티스레드 처리</h3>
<p>FFmpeg는 성능 향상을 위해 멀티스레드 처리를 지원한다. 멀티스레드 처리는 여러 코어를 활용하여 인코딩, 디코딩 또는 필터링 작업을 병렬로 수행할 수 있게 해준다. FFmpeg API에서 멀티스레드 처리를 활성화하기 위해서는 <code>AVCodecContext</code>의 스레드 관련 파라미터를 설정해야 한다.</p>
<h4 id="_16">스레드 개수 설정</h4>
<p>스레드 개수는 <code>AVCodecContext</code> 구조체의 <code>thread_count</code> 필드를 설정하여 제어할 수 있다. 스레드 개수를 설정하면 FFmpeg가 내부적으로 여러 스레드를 생성하여 작업을 분할 처리한다.</p>
<pre><code class="language-c">codec_ctx-&gt;thread_count = 4;  // 4개의 스레드로 병렬 처리
</code></pre>
<h4 id="_17">스레드 처리 모드</h4>
<p>FFmpeg는 두 가지 멀티스레드 처리 모드를 지원한다: <strong>slice 기반</strong>과 <strong>frame 기반</strong>. 각 모드는 <code>thread_type</code> 필드를 설정하여 활성화된다.</p>
<ul>
<li><strong>slice 기반 모드</strong>: 프레임을 여러 개의 슬라이스로 나누어 처리하는 방식. 이 모드는 하드웨어 지원이 필요할 수 있다.</li>
<li><strong>frame 기반 모드</strong>: 여러 프레임을 각각의 스레드에서 처리하는 방식으로, 대부분의 코덱에서 지원된다.</li>
</ul>
<pre><code class="language-c">codec_ctx-&gt;thread_type = FF_THREAD_FRAME;  // frame 기반 멀티스레드 처리
</code></pre>
<h4 id="_18">멀티스레드 디코딩 예시</h4>
<p>멀티스레드 디코딩을 활성화한 후 비디오를 디코딩하는 코드는 아래와 같다.</p>
<pre><code class="language-c">AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
codec_ctx-&gt;thread_count = 4;  // 4개의 스레드를 사용
codec_ctx-&gt;thread_type = FF_THREAD_FRAME;  // frame 기반 스레드 처리

// 코덱을 열고 디코딩
avcodec_open2(codec_ctx, codec, NULL);

while (av_read_frame(fmt_ctx, &amp;pkt) &gt;= 0) {
    avcodec_send_packet(codec_ctx, &amp;pkt);
    avcodec_receive_frame(codec_ctx, frame);
    // 디코딩된 프레임 처리
}
</code></pre>
<p>이 코드에서는 4개의 스레드를 활용하여 비디오 프레임을 병렬로 처리한다. 멀티스레드 처리는 특히 고해상도 비디오나 다중 스트림 데이터를 처리할 때 성능을 크게 향상시킨다.</p>
<h3 id="ffmpeg_4">FFmpeg의 버퍼 관리</h3>
<p>FFmpeg에서 멀티미디어 데이터를 처리할 때, 입력과 출력 스트림 사이에서 데이터를 효율적으로 관리하기 위해 <strong>버퍼</strong>가 사용된다. 특히, 실시간 스트리밍이나 대용량 파일을 처리할 때는 버퍼 관리가 중요하다. FFmpeg의 버퍼는 <strong>AVBufferRef</strong>와 <strong>AVFrame</strong> 구조체를 통해 관리된다.</p>
<h4 id="avbufferref">AVBufferRef 구조체</h4>
<p><strong>AVBufferRef</strong>는 데이터를 참조하는 버퍼이다. 이 구조체는 메모리를 효율적으로 관리할 수 있도록 참조 카운트를 지원하며, 메모리 누수를 방지할 수 있다. 버퍼는 직접 할당하여 사용할 수 있다.</p>
<pre><code class="language-c">AVBufferRef *buffer = av_buffer_alloc(size);
</code></pre>
<p>할당된 버퍼는 사용 후 반드시 해제해야 한다.</p>
<pre><code class="language-c">av_buffer_unref(&amp;buffer);
</code></pre>
<h4 id="avframe-avbufferref">AVFrame과 AVBufferRef</h4>
<p><strong>AVFrame</strong>은 비디오와 오디오 프레임 데이터를 저장하는 구조체로, 내부적으로 <strong>AVBufferRef</strong>를 사용하여 데이터를 참조한다. AVFrame을 사용하여 프레임 데이터를 저장하고 처리할 수 있다.</p>
<pre><code class="language-c">AVFrame *frame = av_frame_alloc();
frame-&gt;buf[0] = av_buffer_alloc(size);  // 버퍼를 할당하여 프레임에 연결
</code></pre>
<p>프레임의 데이터를 처리한 후에는 프레임과 버퍼를 해제한다.</p>
<pre><code class="language-c">av_frame_unref(frame);
</code></pre>
<h4 id="_19">버퍼 복사 및 공유</h4>
<p>FFmpeg에서 데이터 버퍼를 복사하거나 공유해야 할 때 <strong>av_buffer_ref()</strong> 함수와 <strong>av_buffer_unref()</strong> 함수를 사용하여 참조를 관리할 수 있다. 예를 들어, 동일한 데이터를 여러 개의 스레드에서 참조할 때 버퍼 복사를 최소화하고, 참조 카운트를 활용하여 메모리를 효율적으로 관리한다.</p>
<pre><code class="language-c">AVBufferRef *new_buffer = av_buffer_ref(buffer);  // 버퍼 참조 추가
av_buffer_unref(&amp;new_buffer);  // 참조 해제
</code></pre>
<h3 id="ffmpeg-api_2">FFmpeg API에서 오류 처리</h3>
<p>FFmpeg API를 사용할 때는 다양한 오류가 발생할 수 있다. API 호출이 성공하면 0을 반환하고, 실패하면 음수 값을 반환한다. 이러한 오류 코드를 적절히 처리하는 것이 중요하다.</p>
<h4 id="_20">오류 코드의 처리</h4>
<p>오류가 발생하면 FFmpeg는 음수 값을 반환하며, 이 값은 오류의 종류를 나타낸다. 오류 코드는 <code>AVERROR_*</code> 매크로로 정의되어 있다. 예를 들어, 파일을 여는 도중 오류가 발생하면 <code>AVERROR(ENOENT)</code>가 반환된다.</p>
<pre><code class="language-c">int ret = avformat_open_input(&amp;fmt_ctx, &quot;input.mp4&quot;, NULL, NULL);
if (ret &lt; 0) {
    fprintf(stderr, &quot;파일을 열 수 없다: %s\n&quot;, av_err2str(ret));
}
</code></pre>
<h4 id="_21">오류 메시지 출력</h4>
<p>FFmpeg는 오류 메시지를 출력할 때 <code>av_err2str()</code> 함수를 사용하여 오류 코드를 문자열로 변환할 수 있다.</p>
<pre><code class="language-c">char err_buf[AV_ERROR_MAX_STRING_SIZE];
av_strerror(ret, err_buf, sizeof(err_buf));
fprintf(stderr, &quot;오류 발생: %s\n&quot;, err_buf);
</code></pre>
<p>이 코드를 통해 FFmpeg API 호출에서 발생한 오류를 쉽게 디버깅할 수 있다.</p>
<h3 id="ffmpeg-api_3">FFmpeg API의 메모리 관리</h3>
<p>FFmpeg API에서 메모리를 효율적으로 관리하는 것은 매우 중요하다. 특히, 다량의 멀티미디어 데이터를 처리하는 경우 메모리 누수를 방지하고 적절한 자원 해제가 필수적이다.</p>
<h4 id="avframe">AVFrame의 메모리 할당 및 해제</h4>
<p>비디오나 오디오 프레임을 처리할 때, <strong>AVFrame</strong> 구조체를 사용하여 각 프레임의 데이터를 저장한다. <strong>AVFrame</strong>은 동적 메모리를 할당받기 때문에, 사용 후 반드시 해제해야 한다. 프레임은 <code>av_frame_alloc()</code>으로 생성되고, 작업이 완료되면 <code>av_frame_free()</code>를 사용하여 해제된다.</p>
<pre><code class="language-c">AVFrame *frame = av_frame_alloc();
if (!frame) {
    fprintf(stderr, &quot;프레임 메모리 할당 실패\n&quot;);
}

// 작업 후 프레임 메모리 해제
av_frame_free(&amp;frame);
</code></pre>
<h4 id="avpacket">AVPacket의 메모리 할당 및 해제</h4>
<p><strong>AVPacket</strong> 구조체는 비디오 또는 오디오의 압축된 데이터를 담고 있다. 패킷은 <code>av_packet_alloc()</code> 함수를 통해 할당되며, 사용 후 <code>av_packet_unref()</code> 또는 <code>av_packet_free()</code>를 통해 메모리를 해제한다.</p>
<pre><code class="language-c">AVPacket *pkt = av_packet_alloc();
if (!pkt) {
    fprintf(stderr, &quot;패킷 메모리 할당 실패\n&quot;);
}

// 패킷 데이터 처리 후 해제
av_packet_unref(pkt);  // 패킷 내의 데이터 해제
av_packet_free(&amp;pkt);  // 패킷 구조체 자체 해제
</code></pre>
<h4 id="_22">컨텍스트 메모리 관리</h4>
<p>FFmpeg에서 사용하는 주요 컨텍스트는 <strong>AVFormatContext</strong>, <strong>AVCodecContext</strong>, <strong>AVFilterContext</strong> 등이다. 이 컨텍스트들도 동적으로 할당되며, 사용이 끝나면 반드시 해제해야 한다.</p>
<p>예를 들어, <strong>AVFormatContext</strong>는 <code>avformat_open_input()</code>으로 할당되며, <code>avformat_close_input()</code>으로 해제된다.</p>
<pre><code class="language-c">AVFormatContext *fmt_ctx = NULL;
avformat_open_input(&amp;fmt_ctx, &quot;input.mp4&quot;, NULL, NULL);

// 작업 후 컨텍스트 해제
avformat_close_input(&amp;fmt_ctx);
</code></pre>
<p><strong>AVCodecContext</strong>는 코덱을 할당하고 열 때 사용되며, 작업 후에는 <code>avcodec_free_context()</code>를 통해 메모리를 해제한다.</p>
<pre><code class="language-c">AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
if (!codec_ctx) {
    fprintf(stderr, &quot;코덱 컨텍스트 할당 실패\n&quot;);
}

// 코덱 해제
avcodec_free_context(&amp;codec_ctx);
</code></pre>
<h4 id="avfiltercontext">AVFilterContext의 메모리 해제</h4>
<p>필터 처리에 사용하는 <strong>AVFilterGraph</strong>와 <strong>AVFilterContext</strong>도 작업이 끝난 후에는 반드시 해제해야 한다. 필터 그래프는 <code>avfilter_graph_alloc()</code>으로 생성되며, 작업 후에는 <code>avfilter_graph_free()</code>로 해제된다.</p>
<pre><code class="language-c">AVFilterGraph *filter_graph = avfilter_graph_alloc();
// 필터 처리 후 해제
avfilter_graph_free(&amp;filter_graph);
</code></pre>
<h3 id="ffmpeg_5">FFmpeg의 파일 출력</h3>
<p>FFmpeg는 다양한 파일 포맷을 지원하며, 출력 파일을 생성하고 스트림을 기록하는 기능을 제공한다. 출력 파일을 처리하려면 먼저 <strong>AVFormatContext</strong>를 생성하고, 파일 포맷에 맞게 설정해야 한다.</p>
<h4 id="_23">출력 파일 생성</h4>
<p>출력 파일을 생성하기 위해서는 <code>avformat_alloc_output_context2()</code>를 사용하여 파일 컨텍스트를 할당하고, 원하는 출력 포맷을 지정한다.</p>
<pre><code class="language-c">AVFormatContext *out_fmt_ctx = NULL;
avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, NULL, &quot;output.mp4&quot;);
if (!out_fmt_ctx) {
    fprintf(stderr, &quot;출력 포맷 컨텍스트 할당 실패\n&quot;);
}
</code></pre>
<h4 id="_24">출력 스트림 생성</h4>
<p>비디오 또는 오디오 스트림을 출력 파일에 기록하기 위해서는 스트림을 생성하고, 각 스트림에 맞는 코덱을 설정해야 한다. <strong>AVStream</strong> 구조체를 사용하여 스트림을 생성한다.</p>
<pre><code class="language-c">AVStream *out_stream = avformat_new_stream(out_fmt_ctx, NULL);
if (!out_stream) {
    fprintf(stderr, &quot;출력 스트림 생성 실패\n&quot;);
}
</code></pre>
<p>각 스트림에 대해 코덱 설정을 지정하고, 스트림의 파라미터를 설정한 후 인코딩된 데이터를 파일로 기록한다.</p>
<h4 id="_25">파일로 패킷 기록</h4>
<p>인코딩된 패킷을 출력 파일에 기록하려면, <code>av_write_frame()</code>을 사용하여 패킷을 출력 스트림으로 보낸다. 이때 패킷의 타임스탬프를 올바르게 설정하여 동기화된 파일이 생성되도록 해야 한다.</p>
<pre><code class="language-c">int ret = av_write_frame(out_fmt_ctx, &amp;pkt);
if (ret &lt; 0) {
    fprintf(stderr, &quot;패킷 기록 실패: %s\n&quot;, av_err2str(ret));
}
</code></pre>
<h4 id="_26">출력 파일 닫기</h4>
<p>출력 파일을 다 기록한 후에는 반드시 <code>av_write_trailer()</code>를 호출하여 파일의 트레일러를 기록한 뒤, 파일을 닫아야 한다. 이렇게 하면 모든 스트림이 올바르게 종료되고, 파일 포맷에 맞는 종결 정보가 저장된다.</p>
<pre><code class="language-c">av_write_trailer(out_fmt_ctx);
</code></pre>
<p>파일이 닫힌 후에는 출력 컨텍스트를 해제하여 메모리를 반환해야 한다.</p>
<pre><code class="language-c">avformat_free_context(out_fmt_ctx);
</code></pre>
<h3 id="ffmpeg_6">FFmpeg의 디버깅과 로깅</h3>
<p>FFmpeg는 API 사용 중 발생하는 문제를 해결하기 위해 로깅 및 디버깅 기능을 제공한다. FFmpeg의 로깅 레벨을 설정하여 프로그램 실행 중 발생하는 오류나 경고를 출력할 수 있다.</p>
<h4 id="_27">로깅 레벨 설정</h4>
<p>FFmpeg의 로깅 레벨은 <code>av_log_set_level()</code> 함수를 통해 설정할 수 있으며, 다양한 레벨의 로그를 출력할 수 있다. 예를 들어, <strong>AV_LOG_ERROR</strong> 수준의 로그만 출력하도록 설정하려면 아래와 같이 작성한다.</p>
<pre><code class="language-c">av_log_set_level(AV_LOG_ERROR);
</code></pre>
<p>로깅 레벨은 아래와 같은 옵션을 제공한다:</p>
<ul>
<li><strong>AV_LOG_QUIET</strong>: 로그를 출력하지 않음</li>
<li><strong>AV_LOG_ERROR</strong>: 오류만 출력</li>
<li><strong>AV_LOG_WARNING</strong>: 경고와 오류 출력</li>
<li><strong>AV_LOG_INFO</strong>: 정보, 경고, 오류 출력</li>
<li><strong>AV_LOG_VERBOSE</strong>: 자세한 로그 출력</li>
<li><strong>AV_LOG_DEBUG</strong>: 디버그 메시지 출력</li>
</ul>
<h4 id="_28">사용자 정의 로깅 함수</h4>
<p>FFmpeg는 사용자 정의 로깅 함수를 설정할 수 있는 기능도 제공한다. <code>av_log_set_callback()</code> 함수를 통해 사용자 정의 로깅 함수를 설정할 수 있으며, 이를 통해 FFmpeg의 로그 출력을 제어할 수 있다.</p>
<pre><code class="language-c">void custom_log(void *ptr, int level, const char *fmt, va_list args) {
    vfprintf(stderr, fmt, args);  // 사용자 정의 로그 출력
}

av_log_set_callback(custom_log);
</code></pre>
<p>사용자 정의 함수는 FFmpeg에서 발생한 로그 메시지를 특정 방식으로 출력하거나 저장하는 데 유용하다.</p>
<h3 id="ffmpeg_7">FFmpeg의 스트리밍 처리</h3>
<p>FFmpeg는 네트워크를 통한 멀티미디어 데이터의 스트리밍을 지원하며, 다양한 프로토콜을 사용하여 실시간 스트리밍 데이터를 처리할 수 있다. RTMP, HTTP, HLS 등의 프로토콜을 사용하여 비디오와 오디오 데이터를 실시간으로 전송하거나 받을 수 있다.</p>
<h4 id="_29">입력 스트림 처리</h4>
<p>입력 스트림을 처리하기 위해 FFmpeg는 <code>avformat_open_input()</code>을 사용하여 네트워크 스트림을 입력으로 연결할 수 있다. 예를 들어, RTMP 스트림을 처리하려면 스트림의 URL을 입력 파일 경로로 지정한다.</p>
<pre><code class="language-c">AVFormatContext *fmt_ctx = NULL;
avformat_open_input(&amp;fmt_ctx, &quot;rtmp://example.com/live/stream&quot;, NULL, NULL);
</code></pre>
<p>스트림 정보를 추출하기 위해 <code>avformat_find_stream_info()</code> 함수를 호출하여 해당 스트림의 비디오 및 오디오 정보를 읽어온다.</p>
<pre><code class="language-c">avformat_find_stream_info(fmt_ctx, NULL);
</code></pre>
<p>이후 일반 파일 처리와 동일하게 패킷을 읽고, 디코딩을 통해 비디오 및 오디오 데이터를 처리할 수 있다.</p>
<h4 id="_30">출력 스트림 설정</h4>
<p>출력 스트림을 설정하기 위해서는 <code>avformat_alloc_output_context2()</code>을 사용하여 출력 컨텍스트를 생성하고, 스트리밍 프로토콜에 맞는 URL을 지정한다. 예를 들어, RTMP 스트림으로 데이터를 전송하려면 RTMP URL을 설정한다.</p>
<pre><code class="language-c">AVFormatContext *out_fmt_ctx = NULL;
avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, &quot;flv&quot;, &quot;rtmp://example.com/live/stream&quot;);
</code></pre>
<h4 id="_31">스트림 초기화 및 연결</h4>
<p>출력 스트림을 생성한 후, 스트림을 초기화하고 <code>avio_open()</code>을 사용하여 출력 URL을 FFmpeg의 IO 컨텍스트에 연결한다. 이렇게 하면 FFmpeg가 네트워크 스트림으로 데이터를 전송할 수 있게 된다.</p>
<pre><code class="language-c">avio_open(&amp;out_fmt_ctx-&gt;pb, &quot;rtmp://example.com/live/stream&quot;, AVIO_FLAG_WRITE);
</code></pre>
<p>스트림이 연결되면 비디오와 오디오 스트림을 인코딩하여 출력 스트림으로 전송할 수 있다. 인코딩된 패킷은 <code>av_write_frame()</code>을 통해 스트림으로 전송된다.</p>
<pre><code class="language-c">av_write_frame(out_fmt_ctx, &amp;pkt);
</code></pre>
<h4 id="_32">스트리밍 종료</h4>
<p>스트리밍 작업이 완료되면 <code>av_write_trailer()</code>를 호출하여 스트림을 종료하고, 모든 스트림 데이터를 전송한 후 <code>avio_closep()</code>를 사용하여 스트림을 닫습니다.</p>
<pre><code class="language-c">av_write_trailer(out_fmt_ctx);
avio_closep(&amp;out_fmt_ctx-&gt;pb);
</code></pre>
<p>이 과정을 통해 FFmpeg는 네트워크를 통한 실시간 스트리밍이 가능해진다.</p>
<h3 id="_33">라이브 스트리밍 변환</h3>
<p>FFmpeg는 라이브 스트리밍 데이터를 인코딩 및 변환하여 실시간으로 전송할 수 있는 기능을 제공한다. 예를 들어, 입력 스트림을 특정 포맷으로 변환하여 실시간으로 RTMP 서버에 전송할 수 있다.</p>
<h4 id="_34">입력 스트림 설정</h4>
<p>라이브 스트리밍을 처리하려면 입력으로 네트워크 스트림을 받아들여야 한다. 이를 위해 <code>avformat_open_input()</code>을 사용하여 입력 스트림을 열고, 스트림 정보를 파싱한다.</p>
<pre><code class="language-c">AVFormatContext *in_fmt_ctx = NULL;
avformat_open_input(&amp;in_fmt_ctx, &quot;rtmp://example.com/live/stream&quot;, NULL, NULL);
avformat_find_stream_info(in_fmt_ctx, NULL);
</code></pre>
<h4 id="_35">출력 스트림 설정</h4>
<p>출력 스트림은 <code>avformat_alloc_output_context2()</code>로 생성하고, RTMP URL을 설정하여 스트림을 전송할 서버를 지정한다.</p>
<pre><code class="language-c">AVFormatContext *out_fmt_ctx = NULL;
avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, &quot;flv&quot;, &quot;rtmp://example.com/live/output&quot;);
avio_open(&amp;out_fmt_ctx-&gt;pb, &quot;rtmp://example.com/live/output&quot;, AVIO_FLAG_WRITE);
</code></pre>
<h4 id="_36">스트림 복사 및 전송</h4>
<p>라이브 스트리밍을 변환할 때는 입력 스트림에서 읽어들인 패킷을 변환한 후, 출력 스트림으로 전송한다. 패킷을 읽고, 필요에 따라 인코딩을 거쳐 <code>av_write_frame()</code>으로 출력한다.</p>
<pre><code class="language-c">while (av_read_frame(in_fmt_ctx, &amp;pkt) &gt;= 0) {
    av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
    av_packet_unref(&amp;pkt);
}
</code></pre>
<p>라이브 스트리밍 작업이 완료되면 <code>av_write_trailer()</code>로 스트림을 종료하고, 스트림을 닫습니다.</p>
<pre><code class="language-c">av_write_trailer(out_fmt_ctx);
avio_closep(&amp;out_fmt_ctx-&gt;pb);
</code></pre>
<h3 id="_37">스트리밍 중 비트레이트 조정</h3>
<p>FFmpeg는 스트리밍 중에 동적으로 비트레이트를 조정할 수 있는 기능을 제공한다. 이는 네트워크 대역폭이나 사용자 환경에 따라 적절한 품질로 스트리밍을 제공하기 위한 중요한 기능이다. 비트레이트 조정은 <strong>AVCodecContext</strong>에서 설정할 수 있다.</p>
<pre><code class="language-c">codec_ctx-&gt;bit_rate = 1000000;  // 비트레이트를 1Mbps로 설정
</code></pre>
<p>비트레이트를 변경함으로써 스트리밍 중에 데이터 전송량과 품질 간의 균형을 조정할 수 있다.</p>
<h3 id="_38">스트리밍 중 다중 트랙 지원</h3>
<p>FFmpeg는 하나의 스트림에서 여러 개의 트랙을 지원할 수 있다. 예를 들어, 비디오, 오디오, 자막 트랙을 동시에 포함하는 스트림을 생성하거나 전송할 수 있다. 다중 트랙을 지원하기 위해서는 각 트랙에 대해 별도의 <strong>AVStream</strong>을 생성하고, 각 트랙의 코덱 설정을 개별적으로 관리해야 한다.</p>
<h4 id="_39">다중 트랙 스트림 생성</h4>
<p>먼저 출력 컨텍스트를 생성하고, 비디오와 오디오 트랙을 추가한다. 각 트랙은 <code>avformat_new_stream()</code> 함수를 사용하여 생성한다.</p>
<pre><code class="language-c">AVStream *video_stream = avformat_new_stream(out_fmt_ctx, NULL);
AVStream *audio_stream = avformat_new_stream(out_fmt_ctx, NULL);
if (!video_stream || !audio_stream) {
    fprintf(stderr, &quot;트랙 생성 실패\n&quot;);
}
</code></pre>
<p>각 스트림에 맞는 코덱을 설정한 후, 스트림을 초기화한다.</p>
<pre><code class="language-c">AVCodecContext *video_codec_ctx = avcodec_alloc_context3(video_codec);
AVCodecContext *audio_codec_ctx = avcodec_alloc_context3(audio_codec);

// 비디오 및 오디오 코덱 설정
video_codec_ctx-&gt;bit_rate = 400000;
audio_codec_ctx-&gt;bit_rate = 128000;
</code></pre>
<h4 id="_40">트랙별 데이터 처리</h4>
<p>다중 트랙 스트림에서 각 트랙은 별도로 패킷을 읽고 처리한다. 비디오와 오디오 패킷은 각각의 코덱 컨텍스트에서 디코딩되고, 변환된 후 출력 스트림에 기록된다. 패킷을 처리할 때는 트랙의 <strong>stream_index</strong>를 확인하여 비디오 패킷인지 오디오 패킷인지를 구분한다.</p>
<pre><code class="language-c">AVPacket pkt;
while (av_read_frame(in_fmt_ctx, &amp;pkt) &gt;= 0) {
    if (pkt.stream_index == video_stream_idx) {
        // 비디오 패킷 처리
    } else if (pkt.stream_index == audio_stream_idx) {
        // 오디오 패킷 처리
    }
    av_packet_unref(&amp;pkt);
}
</code></pre>
<h4 id="_41">다중 트랙 동기화</h4>
<p>다중 트랙 스트림에서 비디오와 오디오의 동기화는 매우 중요하다. 각 트랙의 <strong>PTS</strong>(Presentation Time Stamp)를 기반으로 동기화가 이루어진다. 이를 위해 각 트랙의 PTS를 적절히 관리하여 시간 차이가 발생하지 않도록 한다.</p>
<p>비디오와 오디오의 PTS를 비교하여 적절한 동기화를 유지할 수 있으며, FFmpeg는 내부적으로 <strong>av_interleaved_write_frame()</strong> 함수를 사용하여 두 트랙의 동기화를 자동으로 처리한다.</p>
<pre><code class="language-c">av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
</code></pre>
<p>이 함수는 비디오와 오디오의 PTS를 자동으로 관리하여 시간에 맞춰 데이터를 출력한다.</p>
<h3 id="ffmpeg_8">FFmpeg의 비디오 및 오디오 필터링</h3>
<p>FFmpeg는 다양한 필터를 제공하여 비디오와 오디오 데이터를 처리할 수 있다. 필터는 영상이나 음성을 변환, 편집, 조정하는 데 사용되며, <strong>libavfilter</strong> 라이브러리를 통해 접근할 수 있다. 비디오 필터는 해상도 변경, 색상 조정, 자막 추가 등이 있으며, 오디오 필터는 볼륨 조정, 리샘플링 등이 있다.</p>
<h4 id="_42">비디오 필터 적용</h4>
<p>비디오 필터를 적용하려면 먼저 <strong>AVFilterGraph</strong>를 생성하고 필터 그래프를 구성해야 한다. 예를 들어, 비디오 해상도를 1280x720으로 변경하는 필터를 적용하려면 아래와 같은 단계를 따른다.</p>
<ol>
<li>필터 그래프와 컨텍스트를 초기화한다.</li>
</ol>
<pre><code class="language-c">AVFilterGraph *filter_graph = avfilter_graph_alloc();
AVFilterContext *buffersrc_ctx = NULL;
AVFilterContext *buffersink_ctx = NULL;
</code></pre>
<ol>
<li><strong>buffersrc</strong>와 <strong>buffersink</strong> 필터를 추가하고, 스케일 필터를 설정한다.</li>
</ol>
<pre><code class="language-c">avfilter_graph_create_filter(&amp;buffersrc_ctx, avfilter_get_by_name(&quot;buffer&quot;), &quot;in&quot;, NULL, NULL, filter_graph);
avfilter_graph_create_filter(&amp;buffersink_ctx, avfilter_get_by_name(&quot;buffersink&quot;), &quot;out&quot;, NULL, NULL, filter_graph);

avfilter_graph_parse_ptr(filter_graph, &quot;scale=1280:720&quot;, NULL, NULL, NULL);
avfilter_graph_config(filter_graph, NULL);
</code></pre>
<ol>
<li>필터링된 프레임을 <strong>sws_scale()</strong> 함수를 통해 처리한다.</li>
</ol>
<pre><code class="language-c">av_buffersrc_add_frame_flags(buffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF);
av_buffersink_get_frame(buffersink_ctx, filt_frame);
</code></pre>
<h4 id="_43">오디오 필터 적용</h4>
<p>오디오 필터는 오디오 샘플의 볼륨 조정, 리샘플링, 채널 레이아웃 변경 등을 지원한다. 예를 들어, 오디오 볼륨을 1.5배로 증가시키는 필터를 적용하려면 아래와 같이 설정한다.</p>
<ol>
<li>필터 그래프와 컨텍스트를 초기화한다.</li>
</ol>
<pre><code class="language-c">AVFilterGraph *audio_filter_graph = avfilter_graph_alloc();
AVFilterContext *abuffersrc_ctx = NULL;
AVFilterContext *abuffersink_ctx = NULL;
</code></pre>
<ol>
<li><strong>abuffersrc</strong>와 <strong>abuffersink</strong> 필터를 추가하고, 볼륨 필터를 설정한다.</li>
</ol>
<pre><code class="language-c">avfilter_graph_create_filter(&amp;abuffersrc_ctx, avfilter_get_by_name(&quot;abuffer&quot;), &quot;in&quot;, NULL, NULL, audio_filter_graph);
avfilter_graph_create_filter(&amp;abuffersink_ctx, avfilter_get_by_name(&quot;abuffersink&quot;), &quot;out&quot;, NULL, NULL, audio_filter_graph);

avfilter_graph_parse_ptr(audio_filter_graph, &quot;volume=1.5&quot;, NULL, NULL, NULL);
avfilter_graph_config(audio_filter_graph, NULL);
</code></pre>
<ol>
<li>필터링된 오디오 샘플을 처리한다.</li>
</ol>
<pre><code class="language-c">av_buffersrc_add_frame_flags(abuffersrc_ctx, frame, AV_BUFFERSRC_FLAG_KEEP_REF);
av_buffersink_get_frame(abuffersink_ctx, filt_frame);
</code></pre>
<p>이 과정을 통해 FFmpeg는 비디오와 오디오 스트림을 실시간으로 필터링하고, 변환할 수 있다.</p>
<h3 id="ffmpeg-gpu">FFmpeg의 GPU 가속</h3>
<p>FFmpeg는 GPU를 활용한 인코딩 및 디코딩을 지원하여 성능을 향상시킬 수 있다. 특히, 대용량 비디오 데이터를 처리할 때 GPU 가속을 통해 더 빠른 속도로 작업을 처리할 수 있다. FFmpeg는 NVIDIA의 NVENC, AMD의 AMF, Intel의 QSV 등의 하드웨어 인코딩을 지원한다.</p>
<h4 id="gpu">GPU 인코딩 활성화</h4>
<p>NVIDIA NVENC를 사용하는 GPU 인코딩을 활성화하려면 <strong>AVCodec</strong>을 설정할 때 <code>h264_nvenc</code> 코덱을 사용하여 GPU에서 비디오 인코딩을 수행할 수 있다.</p>
<pre><code class="language-c">AVCodec *codec = avcodec_find_encoder_by_name(&quot;h264_nvenc&quot;);
AVCodecContext *codec_ctx = avcodec_alloc_context3(codec);
</code></pre>
<p>인코딩 설정을 완료한 후, GPU에서 인코딩 작업이 이루어지도록 패킷을 처리한다.</p>
<h4 id="gpu_1">GPU 디코딩 활성화</h4>
<p>GPU 디코딩은 비디오를 GPU에서 해석하여 CPU의 부하를 줄일 수 있다. NVIDIA의 경우, <strong>cuvid</strong>를 사용하여 GPU에서 비디오를 디코딩할 수 있다.</p>
<pre><code class="language-c">AVCodec *decoder = avcodec_find_decoder_by_name(&quot;h264_cuvid&quot;);
AVCodecContext *dec_ctx = avcodec_alloc_context3(decoder);
</code></pre>
<p>이후 디코딩된 데이터를 처리하는 것은 일반적인 FFmpeg의 디코딩 흐름과 동일하게 진행된다.</p>
<h3 id="ffmpeg_9">FFmpeg의 다중 파일 처리</h3>
<p>FFmpeg는 여러 개의 파일을 동시에 처리할 수 있는 기능을 제공한다. 이를 통해 여러 입력 파일을 병합하거나, 여러 출력 파일로 변환 작업을 병렬로 처리할 수 있다. 여러 파일을 처리할 때는 <strong>AVFormatContext</strong>를 각각의 파일에 대해 생성하고, 각 파일의 스트림을 관리해야 한다.</p>
<h4 id="_44">여러 입력 파일 병합</h4>
<p>여러 개의 입력 파일을 하나의 출력 파일로 병합할 때는 각 입력 파일에 대해 <strong>AVFormatContext</strong>를 열어야 한다. 이후 각 파일의 스트림을 하나의 출력 파일로 전송하여 병합 작업을 수행한다.</p>
<ol>
<li><strong>여러 입력 파일 열기</strong></li>
</ol>
<p>먼저 각 입력 파일을 <strong>avformat_open_input()</strong>을 통해 열고, 스트림 정보를 추출한다.</p>
<pre><code class="language-c">AVFormatContext *fmt_ctx1 = NULL;
AVFormatContext *fmt_ctx2 = NULL;
avformat_open_input(&amp;fmt_ctx1, &quot;input1.mp4&quot;, NULL, NULL);
avformat_open_input(&amp;fmt_ctx2, &quot;input2.mp4&quot;, NULL, NULL);

avformat_find_stream_info(fmt_ctx1, NULL);
avformat_find_stream_info(fmt_ctx2, NULL);
</code></pre>
<ol>
<li><strong>출력 파일 설정</strong></li>
</ol>
<p>출력 파일을 생성하고, 각 입력 파일의 스트림을 출력 스트림에 추가한다.</p>
<pre><code class="language-c">AVFormatContext *out_fmt_ctx = NULL;
avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, NULL, &quot;output.mp4&quot;);

AVStream *out_stream1 = avformat_new_stream(out_fmt_ctx, NULL);
AVStream *out_stream2 = avformat_new_stream(out_fmt_ctx, NULL);
</code></pre>
<ol>
<li><strong>스트림 복사 및 병합</strong></li>
</ol>
<p>각 입력 파일에서 읽어온 패킷을 출력 스트림에 기록하여 파일을 병합한다.</p>
<pre><code class="language-c">AVPacket pkt;
while (av_read_frame(fmt_ctx1, &amp;pkt) &gt;= 0) {
    av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
    av_packet_unref(&amp;pkt);
}

while (av_read_frame(fmt_ctx2, &amp;pkt) &gt;= 0) {
    av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
    av_packet_unref(&amp;pkt);
}
</code></pre>
<ol>
<li><strong>파일 종료 및 정리</strong></li>
</ol>
<p>모든 파일이 병합된 후, 출력 파일의 트레일러를 기록하고 파일을 닫습니다.</p>
<pre><code class="language-c">av_write_trailer(out_fmt_ctx);
avio_closep(&amp;out_fmt_ctx-&gt;pb);
avformat_close_input(&amp;fmt_ctx1);
avformat_close_input(&amp;fmt_ctx2);
</code></pre>
<h4 id="_45">파일 분할</h4>
<p>FFmpeg는 큰 파일을 여러 개의 작은 파일로 분할하는 기능도 제공한다. 이 과정에서 지정한 크기 또는 시간 단위로 파일을 분할할 수 있다. 예를 들어, 비디오 파일을 10초 단위로 분할하려면 <strong>segment</strong> 옵션을 사용할 수 있다.</p>
<ol>
<li><strong>출력 파일 세그먼트 설정</strong></li>
</ol>
<p>분할된 출력 파일을 기록할 때는 FFmpeg의 세그먼트 옵션을 설정하여 일정한 시간 간격으로 파일을 나눈다.</p>
<pre><code class="language-c">AVFormatContext *out_fmt_ctx = NULL;
avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, NULL, &quot;output_segment_%03d.mp4&quot;);
</code></pre>
<p>여기서 <code>%03d</code>는 분할된 파일의 이름에 순차적인 숫자를 붙이기 위한 형식 지정자이다.</p>
<ol>
<li><strong>시간 간격에 따른 분할</strong></li>
</ol>
<p>출력 파일을 시간 간격으로 분할할 때는 <strong>segment_time</strong> 옵션을 설정하여 각 파일의 길이를 제어할 수 있다.</p>
<pre><code class="language-c">int segment_time = 10;  // 10초 단위로 파일 분할
</code></pre>
<ol>
<li><strong>패킷 처리 및 파일 분할</strong></li>
</ol>
<p>스트림을 읽으면서 지정된 시간 간격에 따라 파일을 분할하고 기록한다.</p>
<pre><code class="language-c">int64_t next_segment_time = segment_time * AV_TIME_BASE;
while (av_read_frame(fmt_ctx, &amp;pkt) &gt;= 0) {
    if (pkt.pts &gt;= next_segment_time) {
        av_write_trailer(out_fmt_ctx);
        avformat_alloc_output_context2(&amp;out_fmt_ctx, NULL, NULL, &quot;output_segment_%03d.mp4&quot;);
        next_segment_time += segment_time * AV_TIME_BASE;
    }
    av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
    av_packet_unref(&amp;pkt);
}
</code></pre>
<ol>
<li><strong>파일 종료</strong></li>
</ol>
<p>분할 작업이 완료되면 각 분할된 파일에 대해 트레일러를 기록하고, 모든 파일을 닫습니다.</p>
<pre><code class="language-c">av_write_trailer(out_fmt_ctx);
avio_closep(&amp;out_fmt_ctx-&gt;pb);
avformat_close_input(&amp;fmt_ctx);
</code></pre>
<h3 id="_46">파일 길이 조정</h3>
<p>FFmpeg는 비디오나 오디오 파일의 길이를 조정하여 특정 구간만 추출하거나, 원하는 시간에 맞게 파일을 자를 수 있다. 이 작업은 파일을 읽고, 시작 시간과 종료 시간을 설정하여 해당 구간만 처리하는 방식으로 이루어진다.</p>
<ol>
<li><strong>파일 구간 추출</strong></li>
</ol>
<p>파일에서 특정 시간대의 데이터를 추출하기 위해 <strong>AVFormatContext</strong>에서 시작 시간과 종료 시간을 설정할 수 있다.</p>
<pre><code class="language-c">int64_t start_time = 30 * AV_TIME_BASE;  // 30초에서 시작
int64_t duration = 10 * AV_TIME_BASE;    // 10초 구간만 추출

av_seek_frame(fmt_ctx, -1, start_time, AVSEEK_FLAG_ANY);
</code></pre>
<ol>
<li><strong>구간 내 패킷 처리</strong></li>
</ol>
<p>지정된 구간 내에서 패킷을 읽어와 출력 파일에 기록한다.</p>
<pre><code class="language-c">while (av_read_frame(fmt_ctx, &amp;pkt) &gt;= 0) {
    if (pkt.pts &gt; start_time + duration) {
        break;
    }
    av_interleaved_write_frame(out_fmt_ctx, &amp;pkt);
    av_packet_unref(&amp;pkt);
}
</code></pre>
<h3 id="_47">파일 변환 중 오류 처리</h3>
<p>FFmpeg API를 사용할 때, 파일 변환 과정에서 다양한 오류가 발생할 수 있다. 이러한 오류를 처리하기 위해서는 적절한 오류 메시지를 출력하고, 오류가 발생한 상황에 맞는 예외 처리가 필요하다.</p>
<h4 id="_48">오류 코드 확인</h4>
<p>FFmpeg의 각 함수는 성공 시 0을 반환하고, 실패 시 음수 값을 반환한다. 반환된 오류 코드를 확인하여 문제가 발생했을 때 적절한 조치를 취할 수 있다.</p>
<pre><code class="language-c">int ret = avformat_open_input(&amp;fmt_ctx, &quot;input.mp4&quot;, NULL, NULL);
if (ret &lt; 0) {
    fprintf(stderr, &quot;파일을 열 수 없다: %s\n&quot;, av_err2str(ret));
}
</code></pre>
<h4 id="_49">오류 메시지 출력</h4>
<p>FFmpeg는 오류 코드를 사람이 읽을 수 있는 오류 메시지로 변환하기 위한 <code>av_strerror()</code> 함수를 제공한다. 이 함수를 사용하여 오류 메시지를 출력할 수 있다.</p>
<pre><code class="language-c">char err_buf[AV_ERROR_MAX_STRING_SIZE];
av_strerror(ret, err_buf, sizeof(err_buf));
fprintf(stderr, &quot;오류 발생: %s\n&quot;, err_buf);
</code></pre>
<p>이 방식으로 각 함수의 호출 결과를 확인하고, 오류가 발생했을 때 즉시 대응할 수 있다.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../A054/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../A054/" class="btn btn-xs btn-link">
        FFMPEG 라이브러리 컴파일
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../A052/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../A052/" class="btn btn-xs btn-link">
        FFMPEG 스크립트 작성
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>