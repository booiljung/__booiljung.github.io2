<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_02/0206/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>NumPy와의 상호작용 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "NumPy\uc640\uc758 \uc0c1\ud638\uc791\uc6a9", url: "#_top", children: [
              {title: "Tensor\uc640 ndarray \uac04\uc758 \ubcc0\ud658", url: "#tensor-ndarray" },
              {title: "Tensor\uc640 ndarray\uc758 \uc5f0\uc0b0 \ud638\ud658\uc131", url: "#tensor-ndarray_1" },
              {title: "NumPy\uc640 PyTorch \uac04\uc758 \uc5f0\uc0b0 \ucc28\uc774\uc810", url: "#numpy-pytorch" },
              {title: "NumPy\uc640 PyTorch\ub97c \ud63c\ud569\ud558\uc5ec \uc0ac\uc6a9\ud558\uae30", url: "#numpy-pytorch_1" },
              {title: "PyTorch\uc640 NumPy\uc758 \uba54\ubaa8\ub9ac \uad00\ub9ac \ucc28\uc774", url: "#pytorch-numpy" },
              {title: "\ub2e4\ucc28\uc6d0 \ubc30\uc5f4\uacfc \ud150\uc11c\uc758 \uc0c1\ud638\uc791\uc6a9", url: "#_1" },
              {title: "\uc815\ub9ac\ub418\uc9c0 \uc54a\uc740 \ubc30\uc5f4\uacfc Tensor\uc758 \ube44\uad50", url: "#tensor" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    

    <h1 id="numpy">NumPy와의 상호작용</h1>
<p>PyTorch는 다양한 과학적 연산을 효율적으로 수행할 수 있는 강력한 라이브러리이며, NumPy와의 긴밀한 상호작용을 제공합니다. NumPy는 과학 컴퓨팅에서 널리 사용되는 라이브러리로, 특히 벡터와 행렬 연산을 간편하게 수행할 수 있습니다. PyTorch의 <code>Tensor</code>는 NumPy의 <code>ndarray</code>와 유사한 구조를 가지며, 두 데이터 구조 간의 변환은 쉽고 빠릅니다. 이는 PyTorch를 사용할 때, 기존의 NumPy 기반 코드를 쉽게 PyTorch 코드로 전환할 수 있음을 의미합니다. 다음으로 PyTorch의 Tensor와 NumPy의 ndarray 간 상호작용을 설명합니다.</p>
<h3 id="tensor-ndarray">Tensor와 ndarray 간의 변환</h3>
<p>PyTorch에서는 Tensor와 ndarray를 상호 변환하는 것이 매우 직관적입니다. 다음과 같은 방법으로 두 데이터 구조를 서로 변환할 수 있습니다:</p>
<h4 id="1-numpy-tensor">1. NumPy 배열을 Tensor로 변환하기</h4>
<p>NumPy 배열을 PyTorch의 Tensor로 변환하려면 <code>torch.from_numpy()</code> 함수를 사용합니다. 이 함수는 NumPy 배열을 입력으로 받아 동일한 데이터 값을 갖는 Tensor를 반환합니다. 이때, 변환된 Tensor는 NumPy 배열과 메모리를 공유하므로, 하나의 값을 변경하면 다른 쪽에도 영향을 미칩니다.</p>
<pre><code class="language-python">import numpy as np
import torch

# NumPy 배열 생성
np_array = np.array([1, 2, 3, 4, 5])

# NumPy 배열을 Tensor로 변환
tensor = torch.from_numpy(np_array)

print(tensor)  # tensor([1, 2, 3, 4, 5])
</code></pre>
<p>위 코드에서 <code>np_array</code>는 NumPy 배열이고, 이를 <code>torch.from_numpy()</code>로 변환하여 Tensor로 바꾸었습니다. 중요한 점은 이 변환은 <strong>메모리 공유</strong>가 이루어진다는 것입니다. 따라서 <code>np_array</code>와 <code>tensor</code> 중 하나를 수정하면, 다른 하나에도 동일한 수정이 적용됩니다.</p>
<h4 id="2-tensor-numpy">2. Tensor를 NumPy 배열로 변환하기</h4>
<p>반대로, Tensor를 NumPy 배열로 변환하려면 <code>tensor.numpy()</code> 메서드를 사용하면 됩니다. 이 메서드는 Tensor의 데이터를 NumPy 배열로 변환하여 반환합니다. 마찬가지로, 변환된 NumPy 배열은 원본 Tensor와 메모리를 공유합니다.</p>
<pre><code class="language-python"># Tensor를 NumPy 배열로 변환
np_array_2 = tensor.numpy()

print(np_array_2)  # [1 2 3 4 5]
</code></pre>
<p>위 코드에서는 <code>tensor</code>를 <code>numpy()</code> 메서드를 사용하여 NumPy 배열로 변환하였습니다. 이 경우에도 <code>tensor</code>와 <code>np_array_2</code>는 동일한 메모리 주소를 공유하기 때문에, 하나를 수정하면 다른 하나에도 영향을 미칩니다.</p>
<h4 id="3">3. 메모리 공유의 장단점</h4>
<p>Tensor와 NumPy 배열 간의 변환이 메모리를 공유하는 것은 효율적인 메모리 사용 측면에서 유리합니다. 예를 들어, 데이터를 복사하지 않고도 서로 다른 연산 환경에서 동일한 데이터를 사용할 수 있습니다. 하지만, 이는 메모리 공유로 인해 한쪽의 값이 변하면 다른 쪽에도 영향을 미칠 수 있다는 것을 의미하므로, 의도하지 않은 변경에 주의해야 합니다. 다음 예제를 보겠습니다:</p>
<pre><code class="language-python"># NumPy 배열의 값을 변경
np_array[0] = 100

print(tensor)  # tensor([100,   2,   3,   4,   5])
</code></pre>
<p>위 코드에서 <code>np_array</code>의 첫 번째 요소를 <code>100</code>으로 변경하면, 이를 메모리로 공유하는 <code>tensor</code>의 첫 번째 값도 자동으로 변경되는 것을 확인할 수 있습니다.</p>
<h3 id="tensor-ndarray_1">Tensor와 ndarray의 연산 호환성</h3>
<p>Tensor와 NumPy 배열은 데이터 구조와 연산 방식에서 유사한 점이 많습니다. 이는 수학적 연산을 수행할 때, NumPy를 이용하던 사용자들이 PyTorch로 쉽게 전환할 수 있는 이유가 됩니다. 벡터나 행렬 연산을 예로 들어보겠습니다.</p>
<h4 id="1">1. 벡터 덧셈</h4>
<p>벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{a} \in \mathbb{R}^n</span><script type="math/tex">\mathbf{a} \in \mathbb{R}^n</script></span>과 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{b} \in \mathbb{R}^n</span><script type="math/tex">\mathbf{b} \in \mathbb{R}^n</script></span>이 있다고 가정합니다. 두 벡터의 덧셈은 다음과 같이 정의됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{c} = \mathbf{a} + \mathbf{b}, \quad \mathbf{c} \in \mathbb{R}^n
</div>
<script type="math/tex; mode=display">
\mathbf{c} = \mathbf{a} + \mathbf{b}, \quad \mathbf{c} \in \mathbb{R}^n
</script>
</div>
<p>NumPy와 PyTorch를 이용한 벡터 덧셈은 다음과 같습니다:</p>
<pre><code class="language-python"># NumPy 벡터 연산
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c_np = a + b

# PyTorch 벡터 연산
a_torch = torch.tensor([1, 2, 3])
b_torch = torch.tensor([4, 5, 6])
c_torch = a_torch + b_torch

print(c_np)     # [5 7 9]
print(c_torch)  # tensor([5, 7, 9])
</code></pre>
<p>이 예제에서 NumPy와 PyTorch는 모두 벡터 간의 연산을 쉽게 수행할 수 있도록 동일한 연산자를 사용하고 있습니다.</p>
<h4 id="2">2. 행렬 곱셈</h4>
<p>행렬 곱셈은 선형 대수에서 매우 중요한 연산으로, PyTorch와 NumPy 모두에서 동일하게 수행할 수 있습니다. 두 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A} \in \mathbb{R}^{m \times n}</span><script type="math/tex">\mathbf{A} \in \mathbb{R}^{m \times n}</script></span>과 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B} \in \mathbb{R}^{n \times p}</span><script type="math/tex">\mathbf{B} \in \mathbb{R}^{n \times p}</script></span>가 주어졌을 때, 이들의 곱 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{C} \in \mathbb{R}^{m \times p}</span><script type="math/tex">\mathbf{C} \in \mathbb{R}^{m \times p}</script></span>는 다음과 같이 정의됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{C} = \mathbf{A} \mathbf{B}
</div>
<script type="math/tex; mode=display">
\mathbf{C} = \mathbf{A} \mathbf{B}
</script>
</div>
<p>이를 NumPy와 PyTorch에서 구현하는 방법은 다음과 같습니다:</p>
<pre><code class="language-python"># NumPy를 사용한 행렬 곱셈
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C_np = np.dot(A, B)

# PyTorch를 사용한 행렬 곱셈
A_torch = torch.tensor([[1, 2], [3, 4]])
B_torch = torch.tensor([[5, 6], [7, 8]])
C_torch = torch.matmul(A_torch, B_torch)

print(C_np)
# [[19 22]
#  [43 50]]

print(C_torch)
# tensor([[19, 22],
#         [43, 50]])
</code></pre>
<p>위 예제에서 <code>np.dot</code>과 <code>torch.matmul</code>은 동일한 역할을 하며, NumPy와 PyTorch 모두 행렬 곱셈을 직관적으로 수행할 수 있도록 설계되었습니다.</p>
<h4 id="3-broadcasting">3. 브로드캐스팅(Broadcasting)</h4>
<p>NumPy의 핵심 기능 중 하나인 브로드캐스팅은 PyTorch에도 동일하게 적용됩니다. 브로드캐스팅은 서로 다른 차원을 가진 배열 간의 연산을 수행할 수 있도록 해주는 기능입니다. 예를 들어, 스칼라 값을 행렬에 더하거나 작은 차원의 벡터를 더 큰 행렬에 더할 때 브로드캐스팅이 사용됩니다. 이를 통해 효율적인 연산을 수행할 수 있습니다.</p>
<p>스칼라 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>와 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{a}</span><script type="math/tex">\mathbf{a}</script></span>가 주어졌을 때, 다음과 같은 연산이 가능합니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{b} = \mathbf{a} + \alpha, \quad \alpha \in \mathbb{R}, \mathbf{a} \in \mathbb{R}^n
</div>
<script type="math/tex; mode=display">
\mathbf{b} = \mathbf{a} + \alpha, \quad \alpha \in \mathbb{R}, \mathbf{a} \in \mathbb{R}^n
</script>
</div>
<p>NumPy와 PyTorch에서의 브로드캐스팅 예제는 다음과 같습니다:</p>
<pre><code class="language-python"># NumPy 브로드캐스팅
a = np.array([1, 2, 3])
b_np = a + 10

# PyTorch 브로드캐스팅
a_torch = torch.tensor([1, 2, 3])
b_torch = a_torch + 10

print(b_np)     # [11 12 13]
print(b_torch)  # tensor([11, 12, 13])
</code></pre>
<p>위 코드에서는 <code>10</code>이라는 스칼라 값을 각각의 배열 요소에 더하여 브로드캐스팅이 이루어집니다. NumPy와 PyTorch는 동일한 방식으로 작동하므로 기존의 NumPy 기반 코드에서 PyTorch로 전환할 때 추가적인 코드 수정 없이 브로드캐스팅을 이용할 수 있습니다.</p>
<h3 id="numpy-pytorch">NumPy와 PyTorch 간의 연산 차이점</h3>
<p>NumPy와 PyTorch의 연산 방식은 유사하지만, 몇 가지 주의할 점이 있습니다. 특히, PyTorch는 GPU 가속을 지원하므로, 대규모 데이터 연산 시 PyTorch를 사용하는 것이 더욱 효율적입니다. 이를 위해서는 Tensor를 GPU로 전송할 필요가 있으며, 다음과 같은 코드로 쉽게 처리할 수 있습니다:</p>
<pre><code class="language-python"># Tensor를 GPU로 전송
tensor_gpu = tensor.to('cuda')
</code></pre>
<p>이처럼 GPU를 활용함으로써 PyTorch는 대규모 행렬 연산을 훨씬 빠르게 수행할 수 있습니다. 그러나 GPU에서 수행되는 연산은 NumPy와의 직접적인 메모리 공유가 불가능하므로, 이 경우 <code>tensor.cpu().numpy()</code>와 같은 방법으로 GPU Tensor를 CPU의 NumPy 배열로 변환해야 합니다.</p>
<h3 id="numpy-pytorch_1">NumPy와 PyTorch를 혼합하여 사용하기</h3>
<p>기존의 코드베이스가 NumPy를 사용하고 있다면, 이를 PyTorch로 전환하면서도 일부 NumPy 함수들을 계속 사용할 수 있습니다. 특히, NumPy에서만 제공하는 특수한 기능을 사용할 때는 PyTorch Tensor를 NumPy로 변환하여 연산하고, 결과를 다시 PyTorch로 변환하는 방식으로 작업을 수행할 수 있습니다. 이를 통해 기존 코드의 수정 없이도 새로운 기능을 쉽게 추가할 수 있습니다.</p>
<p>다음은 PyTorch Tensor를 NumPy로 변환하여 NumPy의 <code>mean</code> 함수를 사용한 후, 결과를 다시 PyTorch Tensor로 변환하는 예제입니다:</p>
<pre><code class="language-python"># PyTorch Tensor 생성
tensor = torch.tensor([1.0, 2.0, 3.0, 4.0])

# NumPy 함수 사용을 위해 Tensor를 NumPy 배열로 변환
np_array = tensor.numpy()
mean_value = np.mean(np_array)

# 결과를 다시 PyTorch Tensor로 변환
mean_tensor = torch.tensor(mean_value)

print(mean_tensor)  # tensor(2.5)
</code></pre>
<p>이 예제에서는 NumPy의 <code>mean</code> 함수가 필요하여 PyTorch Tensor를 NumPy 배열로 변환한 후, 최종 결과를 다시 Tensor로 변환하였습니다. 이처럼 두 라이브러리 간의 긴밀한 상호작용을 통해 PyTorch를 사용할 때 NumPy의 기능을 계속 활용할 수 있습니다.</p>
<h3 id="pytorch-numpy">PyTorch와 NumPy의 메모리 관리 차이</h3>
<p>Tensor와 NumPy 배열 간의 상호작용에서 주의할 점 중 하나는 메모리 관리 방식의 차이입니다. PyTorch는 메모리 효율성을 극대화하기 위해 메모리 공유를 기본적으로 지원하지만, 특정 상황에서는 이를 이해하고 관리할 필요가 있습니다.</p>
<h4 id="1_1">1. 메모리 공유와 잠재적 문제점</h4>
<p>메모리 공유는 메모리 사용량을 줄이고 데이터 변환 속도를 높이는 장점이 있지만, 예상치 못한 동작을 일으킬 수도 있습니다. 예를 들어, PyTorch에서 <code>torch.from_numpy()</code>를 사용해 생성된 Tensor는 NumPy 배열과 동일한 메모리를 사용하기 때문에, 하나의 값을 변경하면 다른 쪽에도 동일하게 반영됩니다. 이는 메모리 측면에서 매우 효율적이지만, 코드가 복잡해질 경우 예기치 않은 결과를 초래할 수 있습니다.</p>
<p>예를 들어, 다음과 같은 코드가 있습니다:</p>
<pre><code class="language-python"># NumPy 배열 생성
np_array = np.array([1, 2, 3, 4, 5])

# NumPy 배열을 Tensor로 변환 (메모리 공유)
tensor = torch.from_numpy(np_array)

# Tensor의 값을 수정
tensor[0] = 10

print(np_array)  # [10  2  3  4  5]
</code></pre>
<p>위 코드에서 <code>tensor</code>의 첫 번째 요소를 수정하면, <code>np_array</code>의 첫 번째 요소도 자동으로 변경됩니다. 이처럼 의도하지 않은 메모리 공유 문제를 방지하기 위해, PyTorch에서는 필요 시 데이터를 복사하여 메모리를 분리할 수 있는 기능을 제공합니다.</p>
<h4 id="2_1">2. 메모리 분리 방법</h4>
<p>의도하지 않은 메모리 공유를 방지하려면, <code>clone()</code> 또는 <code>copy()</code> 메서드를 사용하여 데이터를 복사하여 분리할 수 있습니다. 이를 통해 두 개의 독립적인 데이터 구조를 만들 수 있습니다.</p>
<pre><code class="language-python"># NumPy 배열을 Tensor로 변환 (복사하여 메모리 분리)
tensor_copy = torch.tensor(np_array)

# Tensor의 값을 수정
tensor_copy[0] = 100

print(np_array)      # [10  2  3  4  5]
print(tensor_copy)   # tensor([100,   2,   3,   4,   5])
</code></pre>
<p>위 코드에서 <code>torch.tensor(np_array)</code>는 <code>np_array</code>와 메모리를 공유하지 않는 새로운 Tensor를 생성하므로, <code>tensor_copy</code>를 수정해도 원래의 <code>np_array</code>는 영향을 받지 않습니다.</p>
<h3 id="_1">다차원 배열과 텐서의 상호작용</h3>
<p>NumPy와 PyTorch 모두 다차원 배열(행렬)을 지원합니다. 예를 들어, 3차원 배열을 사용하는 딥러닝 모델에서 입력 데이터는 종종 <span class="arithmatex"><span class="MathJax_Preview">(N, C, H, W)</span><script type="math/tex">(N, C, H, W)</script></span> 형태로 나타납니다. 여기서 <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>은 배치 크기, <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span>는 채널 수, <span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span>는 높이, <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span>는 너비를 나타냅니다. NumPy와 PyTorch의 데이터 구조는 이러한 다차원 데이터를 효과적으로 다룰 수 있도록 설계되어 있으며, 다음과 같은 방식으로 변환할 수 있습니다.</p>
<h4 id="1-3">1. 3차원 데이터의 예제</h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{T} \in \mathbb{R}^{2 \times 2 \times 3}
</div>
<script type="math/tex; mode=display">
\mathbf{T} \in \mathbb{R}^{2 \times 2 \times 3}
</script>
</div>
<p>를 나타내는 3차원 배열이 있다고 가정할 때, 이를 NumPy와 PyTorch에서 생성하고 변환하는 과정은 다음과 같습니다:</p>
<pre><code class="language-python"># NumPy 3차원 배열 생성
np_3d_array = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])

# NumPy 배열을 PyTorch Tensor로 변환
torch_3d_tensor = torch.from_numpy(np_3d_array)

print(torch_3d_tensor)
# tensor([[[ 1,  2,  3],
#          [ 4,  5,  6]],
#
#         [[ 7,  8,  9],
#          [10, 11, 12]]])
</code></pre>
<p>위 예제에서 NumPy 배열과 PyTorch Tensor는 동일한 구조로 다차원 데이터를 표현할 수 있으며, 이를 통해 다양한 데이터 형식에 대한 연산을 지원합니다.</p>
<h4 id="2-tensor">2. Tensor 차원 변경</h4>
<p>딥러닝 작업에서는 입력 데이터의 차원을 변경하거나 재배열하는 일이 자주 발생합니다. 예를 들어, 데이터의 형식을 변환할 때 <code>reshape</code> 또는 <code>permute</code>와 같은 기능이 사용됩니다. NumPy와 PyTorch 모두 이러한 기능을 제공합니다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{T} \in \mathbb{R}^{2 \times 3}
</div>
<script type="math/tex; mode=display">
\mathbf{T} \in \mathbb{R}^{2 \times 3}
</script>
</div>
<p>의 차원을 <span class="arithmatex"><span class="MathJax_Preview">3 \times 2</span><script type="math/tex">3 \times 2</script></span>로 변경하는 예제를 살펴봅시다:</p>
<pre><code class="language-python"># NumPy를 사용한 차원 변경
np_array = np.array([[1, 2, 3], [4, 5, 6]])
np_reshaped = np_array.reshape(3, 2)

# PyTorch를 사용한 차원 변경
torch_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])
torch_reshaped = torch_tensor.view(3, 2)

print(np_reshaped)
# [[1 2]
#  [3 4]
#  [5 6]]

print(torch_reshaped)
# tensor([[1, 2],
#         [3, 4],
#         [5, 6]])
</code></pre>
<p>위 코드에서 NumPy의 <code>reshape</code>과 PyTorch의 <code>view</code>는 동일한 기능을 수행합니다. PyTorch에서는 <code>view</code> 외에도 <code>reshape</code>, <code>permute</code> 등의 다양한 차원 변경 함수가 제공됩니다.</p>
<h3 id="tensor">정리되지 않은 배열과 Tensor의 비교</h3>
<p>NumPy의 배열과 PyTorch의 Tensor는 구조적으로 유사하지만, 몇 가지 근본적인 차이가 있습니다. 특히, PyTorch는 GPU 가속을 지원하고, 자동 미분 기능(Autograd)을 통해 딥러닝에 적합하도록 설계되었습니다. 반면, NumPy는 CPU에서의 일반적인 과학적 연산에 중점을 두고 있습니다. 이러한 차이점은 두 라이브러리 간의 연산을 이해하고 활용할 때 중요한 요소로 작용합니다.</p>
<p>다음은 PyTorch Tensor와 NumPy 배열의 주요 차이점입니다:</p>
<table>
<thead>
<tr>
<th>특징</th>
<th>NumPy 배열</th>
<th>PyTorch Tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>기본 연산</td>
<td>CPU에서 수행</td>
<td>CPU 및 GPU에서 수행 가능</td>
</tr>
<tr>
<td>메모리 공유</td>
<td>메모리 공유 없이 별도로 할당</td>
<td>NumPy 배열과의 메모리 공유 가능</td>
</tr>
<tr>
<td>자동 미분</td>
<td>지원하지 않음</td>
<td>자동 미분(Autograd) 지원</td>
</tr>
<tr>
<td>차원 변경</td>
<td><code>reshape</code>, <code>transpose</code> 사용 가능</td>
<td><code>view</code>, <code>permute</code> 사용 가능</td>
</tr>
<tr>
<td>데이터 변환</td>
<td><code>.astype()</code> 등 다양한 형 변환 지원</td>
<td><code>.to()</code>, <code>.type()</code> 사용 가능</td>
</tr>
</tbody>
</table>

  <br>
    

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>