<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_02/0203/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Tensor 연산: 기본 사칙연산 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Tensor \uc5f0\uc0b0: \uae30\ubcf8 \uc0ac\uce59\uc5f0\uc0b0", url: "#_top", children: [
              {title: "Tensor\uc758 \ub367\uc148(Addition)", url: "#tensor-addition" },
              {title: "Tensor\uc758 \ube84\uc148(Subtraction)", url: "#tensor-subtraction" },
              {title: "Tensor\uc758 \uc2a4\uce7c\ub77c \uacf1(Scalar Multiplication)", url: "#tensor-scalar-multiplication" },
              {title: "Tensor\uc758 \uc6d0\uc18c\ubcc4 \uacf1(Element-wise Multiplication)", url: "#tensor-element-wise-multiplication" },
              {title: "Tensor\uc758 \ud589\ub82c \uacf1(Matrix Multiplication)", url: "#tensor-matrix-multiplication" },
              {title: "Tensor\uc758 \ub098\ub217\uc148(Element-wise Division)", url: "#tensor-element-wise-division" },
              {title: "Tensor\uc758 \uba71\uc2b9(Power)", url: "#tensor-power" },
              {title: "Tensor\uc758 \uc804\uce58(Transpose)", url: "#tensor-transpose" },
              {title: "Tensor\uc758 \ub0b4\uc801(Dot Product)", url: "#tensor-dot-product" },
              {title: "Tensor\uc758 \uc678\uc801(Outer Product)", url: "#tensor-outer-product" },
              {title: "Tensor\uc758 \ud589\ub82c-\ubca1\ud130 \uacf1(Matrix-Vector Multiplication)", url: "#tensor-matrix-vector-multiplication" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    

    <h1 id="tensor">Tensor 연산: 기본 사칙연산</h1>
<h3 id="tensor-addition">Tensor의 덧셈(Addition)</h3>
<p>두 Tensor 간의 덧셈은 같은 크기를 가져야 한다. 두 Tensor가 같은 크기일 때, 각각의 요소를 대응하는 위치끼리 더하는 방식으로 이루어진다. 두 Tensor, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>가 각각 <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 크기라면, 덧셈 연산은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{C} = \mathbf{A} + \mathbf{B} \quad \text{where} \quad C_{ij} = A_{ij} + B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{C} = \mathbf{A} + \mathbf{B} \quad \text{where} \quad C_{ij} = A_{ij} + B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<h4 id="_1">예제 코드:</h4>
<pre><code class="language-python">import torch

A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[5, 6], [7, 8]])
C = A + B
print(C)
</code></pre>
<h3 id="tensor-subtraction">Tensor의 뺄셈(Subtraction)</h3>
<p>Tensor의 뺄셈 역시 덧셈과 마찬가지로 동일한 크기의 두 Tensor 사이에서 수행된다. <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 크기를 가질 때, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{D} = \mathbf{A} - \mathbf{B}</span><script type="math/tex">\mathbf{D} = \mathbf{A} - \mathbf{B}</script></span>는 각 요소의 차로 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{D} = \mathbf{A} - \mathbf{B} \quad \text{where} \quad D_{ij} = A_{ij} - B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{D} = \mathbf{A} - \mathbf{B} \quad \text{where} \quad D_{ij} = A_{ij} - B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<h4 id="_2">예제 코드:</h4>
<pre><code class="language-python">D = A - B
print(D)
</code></pre>
<h3 id="tensor-scalar-multiplication">Tensor의 스칼라 곱(Scalar Multiplication)</h3>
<p>Tensor와 스칼라 간의 곱셈은 각 요소에 스칼라 값을 곱하는 방식으로 이루어진다. 스칼라 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>와 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>에 대해 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{E} = k \cdot \mathbf{A} \quad \text{where} \quad E_{ij} = k \cdot A_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{E} = k \cdot \mathbf{A} \quad \text{where} \quad E_{ij} = k \cdot A_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<h4 id="_3">예제 코드:</h4>
<pre><code class="language-python">k = 2
E = k * A
print(E)
</code></pre>
<h3 id="tensor-element-wise-multiplication">Tensor의 원소별 곱(Element-wise Multiplication)</h3>
<p>두 Tensor 간의 원소별 곱은 대응하는 위치의 요소끼리 곱하는 연산이다. 두 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>가 같은 크기를 가질 때, 원소별 곱은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{F} = \mathbf{A} \odot \mathbf{B} \quad \text{where} \quad F_{ij} = A_{ij} \cdot B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{F} = \mathbf{A} \odot \mathbf{B} \quad \text{where} \quad F_{ij} = A_{ij} \cdot B_{ij}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<p>이 연산은 종종 Hadamard 곱이라고도 불리며, 두 Tensor가 동일한 차원을 가질 때 수행할 수 있다.</p>
<h4 id="_4">예제 코드:</h4>
<pre><code class="language-python">F = A * B
print(F)
</code></pre>
<h3 id="tensor-matrix-multiplication">Tensor의 행렬 곱(Matrix Multiplication)</h3>
<p>행렬 곱은 원소별 곱과 달리 두 Tensor가 행렬로서 곱해질 때 사용되는 연산이다. 두 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>가 각각 <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span>과 <span class="arithmatex"><span class="MathJax_Preview">n \times p</span><script type="math/tex">n \times p</script></span>의 크기를 가질 때, 행렬 곱은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{G} = \mathbf{A} \cdot \mathbf{B} \quad \text{where} \quad G_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, p\}
</div>
<script type="math/tex; mode=display">
\mathbf{G} = \mathbf{A} \cdot \mathbf{B} \quad \text{where} \quad G_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, p\}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>의 열 수와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>의 행 수가 같아야만 곱셈이 가능하다. 결과로 생성된 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{G}</span><script type="math/tex">\mathbf{G}</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">m \times p</span><script type="math/tex">m \times p</script></span> 크기를 가진다.</p>
<h4 id="_5">예제 코드:</h4>
<pre><code class="language-python">G = torch.matmul(A, B)
print(G)
</code></pre>
<h3 id="tensor-element-wise-division">Tensor의 나눗셈(Element-wise Division)</h3>
<p>Tensor의 나눗셈은 대응하는 위치의 요소끼리 나누는 방식으로 이루어진다. 두 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>가 같은 크기를 가질 때, 원소별 나눗셈은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{H} = \mathbf{A} \oslash \mathbf{B} \quad \text{where} \quad H_{ij} = \frac{A_{ij}}{B_{ij}}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{H} = \mathbf{A} \oslash \mathbf{B} \quad \text{where} \quad H_{ij} = \frac{A_{ij}}{B_{ij}}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<p>단, 이 연산에서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{B}</span><script type="math/tex">\mathbf{B}</script></span>의 요소 중 <span class="arithmatex"><span class="MathJax_Preview">0</span><script type="math/tex">0</script></span>인 값이 있을 경우, 계산 과정에서 에러가 발생하거나 무한대 값이 나올 수 있으므로 주의해야 한다.</p>
<h4 id="_6">예제 코드:</h4>
<pre><code class="language-python">H = A / B
print(H)
</code></pre>
<h3 id="tensor-power">Tensor의 멱승(Power)</h3>
<p>Tensor의 각 요소에 대해 스칼라 멱승 연산을 수행할 수 있다. Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 스칼라 <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>에 대해, 멱승 연산은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{I} = \mathbf{A}^{p} \quad \text{where} \quad I_{ij} = (A_{ij})^{p}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{I} = \mathbf{A}^{p} \quad \text{where} \quad I_{ij} = (A_{ij})^{p}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<h4 id="_7">예제 코드:</h4>
<pre><code class="language-python">p = 2
I = A ** p
print(I)
</code></pre>
<h3 id="tensor-transpose">Tensor의 전치(Transpose)</h3>
<p>Tensor의 전치 연산은 행과 열을 교환하는 작업이다. 이 연산은 특히 2차원 Tensor, 즉 행렬에 대해 자주 사용되며, <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 크기의 Tensor <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>를 <span class="arithmatex"><span class="MathJax_Preview">n \times m</span><script type="math/tex">n \times m</script></span> 크기의 Tensor로 변환한다. 수학적으로는 다음과 같이 표현된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{A}^{T} \quad \text{where} \quad (A^{T})_{ij} = A_{ji}, \quad i \in \{1, \ldots, n\}, \; j \in \{1, \ldots, m\}
</div>
<script type="math/tex; mode=display">
\mathbf{A}^{T} \quad \text{where} \quad (A^{T})_{ij} = A_{ji}, \quad i \in \{1, \ldots, n\}, \; j \in \{1, \ldots, m\}
</script>
</div>
<h4 id="_8">예제 코드:</h4>
<pre><code class="language-python">A_T = A.t()
print(A_T)
</code></pre>
<h3 id="tensor-dot-product">Tensor의 내적(Dot Product)</h3>
<p>두 벡터의 내적은 두 벡터가 동일한 차원을 가질 때 수행할 수 있으며, 벡터의 각 요소를 대응하는 위치끼리 곱한 뒤, 그 결과들을 모두 더하는 방식으로 계산된다. 두 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{u}</span><script type="math/tex">\mathbf{u}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}</span><script type="math/tex">\mathbf{v}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>차원일 때, 내적은 다음과 같이 정의된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_{i} v_{i}
</div>
<script type="math/tex; mode=display">
\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_{i} v_{i}
</script>
</div>
<h4 id="_9">예제 코드:</h4>
<pre><code class="language-python">u = torch.tensor([1, 2, 3])
v = torch.tensor([4, 5, 6])
dot_product = torch.dot(u, v)
print(dot_product)
</code></pre>
<h3 id="tensor-outer-product">Tensor의 외적(Outer Product)</h3>
<p>외적은 두 벡터가 주어졌을 때, 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{u}</span><script type="math/tex">\mathbf{u}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}</span><script type="math/tex">\mathbf{v}</script></span>의 요소별 곱으로 구성된 행렬을 생성한다. <span class="arithmatex"><span class="MathJax_Preview">\mathbf{u}</span><script type="math/tex">\mathbf{u}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>차원, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{v}</span><script type="math/tex">\mathbf{v}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>차원 벡터일 때, 외적은 <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 크기의 행렬로 나타난다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{P} = \mathbf{u} \otimes \mathbf{v} \quad \text{where} \quad P_{ij} = u_{i} v_{j}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</div>
<script type="math/tex; mode=display">
\mathbf{P} = \mathbf{u} \otimes \mathbf{v} \quad \text{where} \quad P_{ij} = u_{i} v_{j}, \quad i \in \{1, \ldots, m\}, \; j \in \{1, \ldots, n\}
</script>
</div>
<h4 id="_10">예제 코드:</h4>
<pre><code class="language-python">outer_product = torch.ger(u, v)
print(outer_product)
</code></pre>
<h3 id="tensor-matrix-vector-multiplication">Tensor의 행렬-벡터 곱(Matrix-Vector Multiplication)</h3>
<p>Tensor 간의 행렬-벡터 곱은 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>와 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{u}</span><script type="math/tex">\mathbf{u}</script></span>가 주어졌을 때 수행된다. <span class="arithmatex"><span class="MathJax_Preview">\mathbf{A}</span><script type="math/tex">\mathbf{A}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> 크기의 행렬이고, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{u}</span><script type="math/tex">\mathbf{u}</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>차원 벡터라면, 결과는 <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>차원 벡터가 된다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{w} = \mathbf{A} \mathbf{u} \quad \text{where} \quad w_{i} = \sum_{j=1}^{n} A_{ij} u_{j}, \quad i \in \{1, \ldots, m\}
</div>
<script type="math/tex; mode=display">
\mathbf{w} = \mathbf{A} \mathbf{u} \quad \text{where} \quad w_{i} = \sum_{j=1}^{n} A_{ij} u_{j}, \quad i \in \{1, \ldots, m\}
</script>
</div>
<h4 id="_11">예제 코드:</h4>
<pre><code class="language-python">A = torch.tensor([[1, 2], [3, 4]])
u = torch.tensor([5, 6])
w = torch.mv(A, u)
print(w)
</code></pre>

  <br>
    

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>