<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_01/0101/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>PyTorch란 무엇인가? - 소프트웨어 융합</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "PyTorch\ub780 \ubb34\uc5c7\uc778\uac00?", url: "#_top", children: [
              {title: "\ub3d9\uc801 \uacc4\uc0b0 \uadf8\ub798\ud504 (Dynamic Computational Graph)", url: "#dynamic-computational-graph" },
              {title: "\ud150\uc11c(Tensor)", url: "#tensor" },
              {title: "\uc790\ub3d9 \ubbf8\ubd84 (Automatic Differentiation)", url: "#automatic-differentiation" },
              {title: "\uc2e0\uacbd\ub9dd (Neural Network)", url: "#neural-network" },
              {title: "\ucd5c\uc801\ud654 (Optimization)", url: "#optimization" },
              {title: "GPU \uac00\uc18d (GPU Acceleration)", url: "#gpu-gpu-acceleration" },
              {title: "\ubaa8\ub4c8\ud654\uc640 \uc7ac\uc0ac\uc6a9\uc131 (Modularity and Reusability)", url: "#modularity-and-reusability" },
              {title: "\ud655\uc7a5\uc131 (Extensibility)", url: "#extensibility" },
              {title: "PyTorch\uc758 \uc0dd\ud0dc\uacc4 (PyTorch Ecosystem)", url: "#pytorch-pytorch-ecosystem" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    

    <h1 id="pytorch">PyTorch란 무엇인가?</h1>
<p>PyTorch는 Facebook AI Research(FAIR)에서 개발한 오픈 소스 딥러닝 프레임워크로, Python을 기반으로 하여 사용자가 직관적으로 딥러닝 모델을 구현하고 학습할 수 있도록 설계되었습니다. PyTorch는 특히 연구자와 엔지니어가 빠르고 유연하게 프로토타입을 개발하고 실험할 수 있도록 돕는다는 점에서 큰 인기를 얻고 있습니다. 주요 특징으로는 동적 계산 그래프, 쉬운 디버깅, 강력한 자동 미분 기능 등이 있습니다. 이러한 특성들은 PyTorch를 다른 딥러닝 프레임워크와 차별화하며, 사용자로 하여금 직관적이고 유연하게 코드를 작성할 수 있게 해줍니다.</p>
<h3 id="dynamic-computational-graph">동적 계산 그래프 (Dynamic Computational Graph)</h3>
<p>PyTorch는 동적 계산 그래프를 사용합니다. 동적 계산 그래프는 네트워크 구조를 런타임에 생성하는 방식으로, 코드 작성 시점에서 그래프를 고정하지 않습니다. 이는 <strong>텐서플로(TensorFlow)</strong>와 같은 다른 프레임워크의 정적 계산 그래프(static computational graph)와 대비되는 중요한 차이점입니다. 동적 계산 그래프의 장점은 다음과 같습니다.</p>
<ol>
<li><strong>유연한 네트워크 설계</strong>: 네트워크 구조를 코드로 바로 표현할 수 있어, 조건문이나 반복문을 통해 더 복잡한 구조를 쉽게 설계할 수 있습니다.</li>
<li><strong>쉬운 디버깅</strong>: 런타임에서 즉시 오류를 확인할 수 있기 때문에, 디버깅이 훨씬 쉽습니다. </li>
</ol>
<p>이러한 특성 덕분에 연구자와 개발자는 빠르게 새로운 아이디어를 테스트하고, 실험할 수 있습니다.</p>
<p>예를 들어, PyTorch에서는 다음과 같은 방식으로 간단한 연산 그래프를 작성할 수 있습니다.</p>
<pre><code class="language-python">import torch

# 두 텐서의 합을 계산
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = torch.tensor([4.0, 5.0], requires_grad=True)
z = x + y
print(z)
</code></pre>
<p>위 코드에서 <code>z</code>는 <code>x</code>와 <code>y</code>의 합으로, 계산이 수행되면 동적으로 그래프가 생성됩니다. 런타임 시점에서 그래프가 만들어지므로, 매 연산마다 계산 그래프를 새로 생성할 수 있습니다.</p>
<h3 id="tensor">텐서(Tensor)</h3>
<p>텐서는 딥러닝 모델의 기본 구성 요소로, 수학적으로는 다차원 배열(예: 스칼라, 벡터, 행렬)입니다. PyTorch에서 텐서는 <code>torch.Tensor</code> 클래스로 구현되며, 이를 통해 수치 계산을 효율적으로 수행할 수 있습니다. 텐서의 개념을 이해하기 위해 몇 가지 수학적 정의를 살펴보겠습니다.</p>
<ul>
<li><strong>스칼라 (Scalar)</strong>: 0차원 텐서로 단일 숫자를 의미합니다. 예를 들어, <span class="arithmatex"><span class="MathJax_Preview"> a \in \mathbb{R} </span><script type="math/tex"> a \in \mathbb{R} </script></span>는 스칼라입니다.</li>
<li><strong>벡터 (Vector)</strong>: 1차원 텐서로, 여러 스칼라 값을 일렬로 나열한 것입니다. 예를 들어, <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{v} = [v_1, v_2, \ldots, v_n] </span><script type="math/tex"> \mathbf{v} = [v_1, v_2, \ldots, v_n] </script></span>에서 <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{v} \in \mathbb{R}^n </span><script type="math/tex"> \mathbf{v} \in \mathbb{R}^n </script></span>입니다.</li>
<li><strong>행렬 (Matrix)</strong>: 2차원 텐서로, 스칼라들이 행과 열로 이루어진 것입니다. 예를 들어, <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{M} \in \mathbb{R}^{m \times n} </span><script type="math/tex"> \mathbf{M} \in \mathbb{R}^{m \times n} </script></span>는 행렬입니다.</li>
<li><strong>다차원 텐서 (Higher-dimensional Tensor)</strong>: 더 높은 차원의 텐서로, 벡터와 행렬을 일반화한 형태입니다. 예를 들어, 3차원 텐서는 여러 개의 행렬로 이루어진 배열입니다.</li>
</ul>
<p>PyTorch에서 텐서는 다음과 같이 생성할 수 있습니다:</p>
<pre><code class="language-python"># 스칼라 텐서
scalar = torch.tensor(3.14)

# 벡터 텐서
vector = torch.tensor([1.0, 2.0, 3.0])

# 행렬 텐서
matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]])

# 다차원 텐서
tensor_3d = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])
</code></pre>
<h3 id="automatic-differentiation">자동 미분 (Automatic Differentiation)</h3>
<p>PyTorch는 <strong>autograd</strong>라는 모듈을 통해 자동 미분 기능을 제공합니다. 자동 미분은 텐서의 연산을 추적하여 손실 함수에 대해 미분(gradient)을 계산할 수 있게 해줍니다. 이 과정은 딥러닝에서 매우 중요한데, 신경망을 학습시키기 위해서는 손실 함수를 최소화하는 방향으로 파라미터를 조정해야 하기 때문입니다.</p>
<p>PyTorch의 자동 미분은 연산 그래프를 기반으로 합니다. 이 연산 그래프는 각 연산이 노드로, 텐서가 간선으로 연결된 그래프 구조로, <strong>미분 연산의 체인 룰</strong>을 활용하여 그래디언트를 계산합니다.</p>
<p>예를 들어, 함수 <span class="arithmatex"><span class="MathJax_Preview"> f(x) = x^2 </span><script type="math/tex"> f(x) = x^2 </script></span>의 도함수를 계산한다고 가정합시다. PyTorch를 이용하면 다음과 같이 구현할 수 있습니다.</p>
<pre><code class="language-python">x = torch.tensor(2.0, requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)  # 4.0 출력
</code></pre>
<p>여기서 <code>requires_grad=True</code>로 설정된 텐서는 모든 연산을 추적하고, <code>backward()</code> 호출을 통해 그래디언트를 자동으로 계산합니다. 이 예제에서 도함수 <span class="arithmatex"><span class="MathJax_Preview"> \frac{d}{dx}(x^2) = 2x </span><script type="math/tex"> \frac{d}{dx}(x^2) = 2x </script></span>이므로, <span class="arithmatex"><span class="MathJax_Preview"> x = 2 </span><script type="math/tex"> x = 2 </script></span>일 때의 그래디언트는 4가 됩니다.</p>
<h3 id="neural-network">신경망 (Neural Network)</h3>
<p>PyTorch에서 신경망은 <code>torch.nn</code> 모듈을 통해 쉽게 정의할 수 있습니다. 신경망은 여러 층(layer)으로 구성된 함수이며, 각 층은 입력을 받아 가중치와 편향을 적용한 후 활성화 함수를 통해 비선형 변환을 수행합니다. 이러한 구조 덕분에 신경망은 매우 복잡한 패턴을 학습할 수 있습니다.</p>
<h4 id="_1">기본적인 신경망 구조</h4>
<p>신경망은 입력층, 은닉층(hidden layer), 출력층으로 구성됩니다. 입력층은 데이터가 모델에 들어오는 시작 지점이며, 은닉층은 입력 데이터에서 중요한 특성을 추출하는 역할을 합니다. 출력층은 최종 예측 결과를 생성합니다.</p>
<p>PyTorch에서 신경망을 정의하려면 <code>torch.nn.Module</code>을 상속받아 새로운 클래스를 만들고, <code>__init__</code> 메서드에서 모델의 각 층을 정의합니다. 그런 다음 <code>forward</code> 메서드를 구현하여 입력 데이터를 어떻게 처리할지 정의합니다.</p>
<p>다음은 간단한 피드포워드 신경망 예제입니다:</p>
<pre><code class="language-python">import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(2, 4)  # 입력 크기: 2, 출력 크기: 4
        self.fc2 = nn.Linear(4, 1)  # 입력 크기: 4, 출력 크기: 1

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# 모델 인스턴스 생성
model = SimpleNN()
</code></pre>
<p>위 코드에서 <code>SimpleNN</code> 클래스는 2개의 입력을 받아 1개의 출력을 생성하는 신경망입니다. 첫 번째 층(<code>fc1</code>)은 2개의 입력을 받아 4개의 출력 노드를 생성하고, 두 번째 층(<code>fc2</code>)는 4개의 입력을 받아 1개의 출력을 생성합니다. <code>torch.relu</code>는 활성화 함수로, 은닉층에서 비선형성을 추가하는 역할을 합니다.</p>
<h3 id="optimization">최적화 (Optimization)</h3>
<p>PyTorch는 신경망 학습을 위해 다양한 최적화 알고리즘을 제공합니다. 최적화 알고리즘은 손실 함수의 값을 최소화하기 위해 모델의 가중치를 조정하는 역할을 합니다. 가장 널리 사용되는 최적화 알고리즘은 <strong>확률적 경사 하강법(Stochastic Gradient Descent, SGD)</strong>입니다. PyTorch에서는 <code>torch.optim</code> 모듈을 통해 다양한 최적화 기법을 사용할 수 있습니다.</p>
<p>예를 들어, 아래와 같은 방식으로 모델의 파라미터를 최적화할 수 있습니다:</p>
<pre><code class="language-python">import torch.optim as optim

# 모델, 손실 함수, 최적화기 정의
model = SimpleNN()
criterion = nn.MSELoss()  # 평균 제곱 오차 손실 함수
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 예제 입력과 목표값
input_data = torch.tensor([[1.0, 2.0]])
target = torch.tensor([[0.5]])

# 순전파(forward pass)
output = model(input_data)
loss = criterion(output, target)

# 역전파(backward pass)와 최적화 단계
optimizer.zero_grad()  # 기울기 초기화
loss.backward()        # 기울기 계산
optimizer.step()       # 파라미터 업데이트
</code></pre>
<p>위 예제에서 <code>optimizer.step()</code>은 기울기를 사용해 모델의 파라미터를 조정하는 역할을 합니다. 이 과정은 다음 학습 배치에서 손실을 줄이기 위한 방향으로 파라미터를 업데이트하게 됩니다.</p>
<h3 id="gpu-gpu-acceleration">GPU 가속 (GPU Acceleration)</h3>
<p>PyTorch는 <strong>CUDA</strong>를 통해 GPU 가속을 지원합니다. GPU를 사용하면 대규모 데이터셋과 복잡한 신경망을 훨씬 빠르게 학습시킬 수 있습니다. PyTorch에서 GPU를 활용하려면 <code>.to()</code> 메서드를 사용하여 텐서를 GPU로 옮기기만 하면 됩니다. 예를 들어, 다음과 같이 GPU에서 연산을 수행할 수 있습니다:</p>
<pre><code class="language-python">device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
x = torch.tensor([1.0, 2.0], device=device)
y = torch.tensor([3.0, 4.0], device=device)
z = x + y
print(z)
</code></pre>
<p>위 코드에서 <code>torch.device</code>를 사용해 GPU가 사용 가능한지 확인하고, 텐서를 해당 장치로 옮깁니다. 이후 모든 연산은 GPU에서 빠르게 수행됩니다. GPU 사용 여부를 동적으로 선택할 수 있기 때문에, 코드가 다양한 환경에서도 유연하게 작동하도록 만들 수 있습니다.</p>
<h3 id="modularity-and-reusability">모듈화와 재사용성 (Modularity and Reusability)</h3>
<p>PyTorch의 또 다른 강점은 모듈화된 설계입니다. 신경망을 구축할 때 여러 층(layer)이나 블록(block)을 쉽게 재사용할 수 있으며, 이를 통해 코드의 유지보수와 확장성이 매우 용이합니다. <code>torch.nn.Module</code>을 기반으로 신경망을 정의하는 방식 덕분에, 신경망의 일부를 별도의 모듈로 나누어 더 큰 네트워크의 구성 요소로 사용할 수 있습니다.</p>
<p>예를 들어, 복잡한 네트워크를 구축할 때 자주 사용하는 컨볼루션 블록이나 레지듀얼 블록을 별도의 클래스로 정의하여 손쉽게 재사용할 수 있습니다.</p>
<pre><code class="language-python">import torch.nn as nn

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ConvBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()

    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))

# ConvBlock을 사용하여 더 큰 네트워크를 정의
class CustomNetwork(nn.Module):
    def __init__(self):
        super(CustomNetwork, self).__init__()
        self.layer1 = ConvBlock(3, 16)
        self.layer2 = ConvBlock(16, 32)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        return x
</code></pre>
<p>위 코드에서 <code>ConvBlock</code> 클래스는 컨볼루션 층, 배치 정규화(batch normalization), ReLU 활성화 함수를 포함한 블록입니다. <code>CustomNetwork</code>는 이 블록을 사용하여 더 큰 신경망을 정의하며, 여러 층을 쉽게 결합할 수 있습니다. 이처럼 PyTorch의 모듈화된 설계는 복잡한 모델을 구축할 때 유연성과 재사용성을 제공합니다.</p>
<h3 id="extensibility">확장성 (Extensibility)</h3>
<p>PyTorch는 사용자 정의 연산(custom operations)과 연산자(operator)를 구현하는 데도 뛰어난 확장성을 제공합니다. 특히 연구 단계에서 기존의 모델 구조로는 해결할 수 없는 문제나 새로운 방법론을 실험할 때 매우 유용합니다. PyTorch의 모든 연산은 사용자 정의가 가능하며, 새로운 연산을 구현하려면 <code>torch.autograd.Function</code>을 상속받아 <code>forward</code>와 <code>backward</code> 메서드를 구현할 수 있습니다.</p>
<p>다음은 사용자 정의 연산을 정의하는 예시입니다:</p>
<pre><code class="language-python">class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input &lt; 0] = 0
        return grad_input
</code></pre>
<p>위 예제에서 <code>MyReLU</code>는 PyTorch의 기본 ReLU 연산을 대체하는 사용자 정의 함수입니다. <code>forward</code> 메서드는 정방향 연산을 수행하고, <code>backward</code> 메서드는 역방향 연산에서 그래디언트를 계산합니다. 이러한 확장성을 통해 연구자와 개발자는 기존의 기능에 얽매이지 않고 자유롭게 새로운 기능을 실험할 수 있습니다.</p>
<h3 id="pytorch-pytorch-ecosystem">PyTorch의 생태계 (PyTorch Ecosystem)</h3>
<p>PyTorch는 본체 외에도 풍부한 생태계를 갖추고 있습니다. 대표적인 PyTorch 생태계 구성 요소는 다음과 같습니다:</p>
<ol>
<li><strong>TorchVision</strong>: 컴퓨터 비전용 데이터셋, 모델, 이미지 변환 기능을 포함한 라이브러리로, 이미지 분류, 객체 탐지 등 다양한 컴퓨터 비전 작업에 활용됩니다.</li>
<li><strong>TorchText</strong>: 자연어 처리(NLP) 작업을 위한 라이브러리로, 텍스트 전처리와 모델 정의에 필요한 다양한 도구를 제공합니다.</li>
<li><strong>TorchAudio</strong>: 오디오 데이터 처리 및 분석을 위한 라이브러리로, 음성 인식과 같은 음성 관련 작업에 유용합니다.</li>
<li><strong>TorchServe</strong>: PyTorch 모델을 배포할 수 있는 고성능 서빙 프레임워크로, 학습된 모델을 실시간으로 사용할 수 있게 해줍니다.</li>
</ol>
<p>각 라이브러리는 특정 도메인에서 PyTorch를 더 쉽게 사용할 수 있도록 설계되었으며, 이를 통해 데이터 로드, 전처리, 모델 배포와 같은 반복적인 작업을 간소화할 수 있습니다.</p>

  <br>
    

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>