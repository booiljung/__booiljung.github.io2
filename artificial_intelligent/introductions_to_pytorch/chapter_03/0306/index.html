<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_03/0306/" rel="canonical"/>
<link href="../../../../img/favicon.ico" rel="shortcut icon"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
<title>Autograd를 활용한 간단한 예제 - 실험 도서관</title>
<link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet"/>
<link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet"/>
<link href="../../../../css/base.css" rel="stylesheet"/>
<link href="../../../../css/highlight.css" rel="stylesheet"/>
<link href="../../../../css/custom.css" rel="stylesheet"/>
<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
<script src="../../../../js/jquery-3.2.1.min.js"></script>
<script src="../../../../js/bootstrap-3.3.7.min.js"></script>
<script src="../../../../js/highlight.pack.js"></script>
<base target="_top"/>
<script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Autograd\ub97c \ud65c\uc6a9\ud55c \uac04\ub2e8\ud55c \uc608\uc81c", url: "#_top", children: [
              {title: "Autograd \uac1c\uc694", url: "#autograd_1" },
              {title: "\uac04\ub2e8\ud55c \uc120\ud615 \ud68c\uadc0 \ubaa8\ub378 \uc608\uc81c", url: "#_1" },
              {title: "PyTorch \uad6c\ud604", url: "#pytorch" },
              {title: "\uadf8\ub798\ud504\uc758 \uc5ed\ud560\uacfc \uacc4\uc0b0 \uadf8\ub798\ud504", url: "#_3" },
              {title: "\uae30\uc6b8\uae30 \uacc4\uc0b0", url: "#_4" },
              {title: "\uae30\uc6b8\uae30 \ucd94\uc801\uacfc \ud150\uc11c\uc758 requires_grad \uc18d\uc131", url: "#requires_grad" },
              {title: "\uae30\uc6b8\uae30 \ucd94\uc801 \uba48\ucd94\uae30: torch.no_grad()", url: "#torchno_grad" },
              {title: "retain_graph \uc635\uc158", url: "#retain_graph" },
              {title: "\ub2e4\uc911 \uc785\ub825\uacfc \ub2e4\uc911 \ucd9c\ub825\uc5d0\uc11c\uc758 \uae30\uc6b8\uae30 \uacc4\uc0b0", url: "#_6" },
          ]},
        ];

    </script>
<script src="../../../../js/base.js"></script>
<script src="../../../../js/google_analytics.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script>
</meta></head>
<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>
<div class="container-fluid wm-page-content">
<a name="_top"></a>
<h1 id="autograd">Autograd를 활용한 간단한 예제</h1>
<h3 id="autograd_1">Autograd 개요</h3>
<p>PyTorch의 <code>autograd</code>는 텐서 연산에 대해 자동으로 미분을 계산할 수 있도록 해주는 기능입니다. 이 기능은 텐서의 연산 기록을 추적하여, 역전파(Backpropagation)를 통해 기울기(Gradient)를 계산할 수 있습니다. 모든 텐서는 기본적으로 연산 그래프의 노드로 작동하며, <code>requires_grad</code> 속성을 통해 기울기 계산이 필요한 텐서인지 여부를 설정할 수 있습니다. </p>
<h3 id="_1">간단한 선형 회귀 모델 예제</h3>
<p>선형 회귀 모델은 가장 기본적인 기계 학습 모델 중 하나로, 독립 변수 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>와 종속 변수 <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 사이의 관계를 직선 형태로 모델링합니다. 여기서는 <span class="arithmatex"><span class="MathJax_Preview">y = wx + b</span><script type="math/tex">y = wx + b</script></span> 형태의 간단한 선형 모델을 설정하고, PyTorch의 <code>autograd</code>를 활용해 기울기를 계산하는 과정을 살펴보겠습니다.</p>
<h4 id="_2">수학적 배경</h4>
<p>먼저 선형 회귀 모델에서 예측 값 <span class="arithmatex"><span class="MathJax_Preview">\hat{y}</span><script type="math/tex">\hat{y}</script></span>는 다음과 같이 정의됩니다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y} = w x + b
</div>
<script type="math/tex; mode=display">
\hat{y} = w x + b
</script>
</div>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>: 가중치 (Weight)
- <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span>: 편향 (Bias)
- <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>: 입력 값 (Input)</p>
<p>손실 함수로는 평균 제곱 오차(MSE)를 사용할 것입니다. 손실 함수 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>은 다음과 같이 정의됩니다.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w x_i + b))^2
</div>
<script type="math/tex; mode=display">
L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 = \frac{1}{N} \sum_{i=1}^{N} (y_i - (w x_i + b))^2
</script>
</div>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>: 데이터의 총 개수
- <span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span>: 실제 출력 값 (Actual Output)
- <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_i</span><script type="math/tex">\hat{y}_i</script></span>: 예측된 출력 값 (Predicted Output)</p>
<h3 id="pytorch">PyTorch 구현</h3>
<p>다음으로, 이를 코드로 구현해 보겠습니다. PyTorch에서는 손실 함수를 직접 계산하고 <code>autograd</code>를 통해 기울기를 자동으로 계산할 수 있습니다.</p>
<pre><code class="language-python">import torch

# 데이터 초기화
x = torch.tensor([1.0, 2.0, 3.0, 4.0])
y = torch.tensor([2.0, 4.0, 6.0, 8.0])

# 가중치와 편향 초기화
w = torch.tensor(0.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)

# 학습률 설정
learning_rate = 0.01

# 예측 함수 정의
def forward(x):
    return w * x + b

# 손실 함수 정의
def loss(y_pred, y):
    return ((y_pred - y) ** 2).mean()

# 10번 반복 학습
for epoch in range(10):
    # 예측값 계산
    y_pred = forward(x)

    # 손실 계산
    l = loss(y_pred, y)

    # 역전파를 통해 기울기 계산
    l.backward()

    # 경사 하강법으로 파라미터 업데이트
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad

    # 기울기 초기화
    w.grad.zero_()
    b.grad.zero_()

    # 손실 출력
    print(f'Epoch {epoch+1}: w = {w.item():.3f}, b = {b.item():.3f}, loss = {l.item():.3f}')
</code></pre>
<h3 id="_3">그래프의 역할과 계산 그래프</h3>
<p>위 코드에서 <code>autograd</code>는 텐서들의 연산을 기록하여, 이들을 통해 생성된 그래프를 구성합니다. 이를 통해 역전파 시, 그래프를 탐색하며 각 파라미터의 기울기를 계산할 수 있습니다. <code>l.backward()</code> 명령어를 실행하면, <code>w</code>와 <code>b</code>에 대한 기울기가 자동으로 계산됩니다.</p>
<p>아래는 <code>l.backward()</code> 실행 시, <code>autograd</code>가 구축한 계산 그래프의 개념을 보여줍니다.</p>
<div class="mermaid">graph LR
A[x] --&gt; B[wx + b]
B --&gt; C["(y - (wx + b))^2"]
C --&gt; D[L]
</div>
<h3 id="_4">기울기 계산</h3>
<p>역전파를 통해 손실 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>의 각 파라미터에 대한 기울기를 구하는 과정은 다음과 같습니다. </p>
<p>먼저 <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>을 <span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span>에 대해 편미분합니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial L}{\partial w} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (w x_i + b)) (-x_i)
</div>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial w} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (w x_i + b)) (-x_i)
</script>
</div>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial L}{\partial b} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (w x_i + b)) (-1)
</div>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial b} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (w x_i + b)) (-1)
</script>
</div>
<p>계산 그래프에서 각 연산의 노드는 역전파 시, 상류에서 내려온 기울기를 곱하여 전달하게 됩니다. 이 기울기 전달 과정이 PyTorch의 <code>autograd</code> 기능을 통해 자동으로 수행되므로, 사용자는 수식의 복잡성을 신경 쓰지 않아도 됩니다.</p>
<h3 id="requires_grad">기울기 추적과 텐서의 <code>requires_grad</code> 속성</h3>
<p>위 예제에서 <code>w</code>와 <code>b</code>는 <code>requires_grad=True</code>로 설정되었습니다. 이는 PyTorch에게 이 변수들이 학습 중에 기울기를 추적해야 한다고 알리는 역할을 합니다. 만약 <code>requires_grad=False</code>로 설정하면, 역전파 시 해당 변수의 기울기가 계산되지 않습니다. </p>
<p>다음은 예제의 텐서 초기화 부분에서 <code>requires_grad</code>의 사용 예입니다:</p>
<pre><code class="language-python">w = torch.tensor(0.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)
</code></pre>
<p>이제, 기울기 추적을 위해 어떤 방식으로 작동하는지 알아보겠습니다. 예를 들어, 다음과 같이 <span class="arithmatex"><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>에 대한 함수 <span class="arithmatex"><span class="MathJax_Preview">y = w^2 + 3w + 2</span><script type="math/tex">y = w^2 + 3w + 2</script></span>가 있다고 가정합시다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y = w^2 + 3w + 2
</div>
<script type="math/tex; mode=display">
y = w^2 + 3w + 2
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">w=1.0</span><script type="math/tex">w=1.0</script></span>일 때 <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>의 기울기를 계산해보겠습니다.</p>
<pre><code class="language-python">w = torch.tensor(1.0, requires_grad=True)
y = w**2 + 3*w + 2
y.backward()
print(w.grad)  # 출력: 5.0
</code></pre>
<h4 id="_5">수학적 설명</h4>
<p>위의 예에서 기울기를 수학적으로 계산해 보면:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial y}{\partial w} = 2w + 3
</div>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial w} = 2w + 3
</script>
</div>
<p>따라서, <span class="arithmatex"><span class="MathJax_Preview">w=1.0</span><script type="math/tex">w=1.0</script></span>일 때:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial y}{\partial w} = 2(1.0) + 3 = 5.0
</div>
<script type="math/tex; mode=display">
\frac{\partial y}{\partial w} = 2(1.0) + 3 = 5.0
</script>
</div>
<p>위의 코드와 일치하는 결과를 얻을 수 있습니다. PyTorch의 <code>autograd</code>는 연산을 추적하여 그래프를 구성하고, 이 그래프를 역전파 시 탐색하여 자동으로 기울기를 계산합니다.</p>
<h3 id="torchno_grad">기울기 추적 멈추기: <code>torch.no_grad()</code></h3>
<p>때로는 기울기 추적이 필요하지 않을 때가 있습니다. 예를 들어, 학습이 완료된 후에 모델을 평가할 때, 파라미터를 변경하지 않기 때문에 기울기를 계산할 필요가 없습니다. 이때는 <code>torch.no_grad()</code> 블록을 사용하여 불필요한 메모리 사용과 계산을 방지할 수 있습니다:</p>
<pre><code class="language-python">with torch.no_grad():
    y_pred = forward(x)
    print(y_pred)
</code></pre>
<p>이 블록 안에서는 <code>w</code>와 <code>b</code>의 기울기가 추적되지 않으며, 이를 통해 모델 평가 시 성능을 최적화할 수 있습니다.</p>
<h3 id="retain_graph"><code>retain_graph</code> 옵션</h3>
<p><code>autograd</code>를 통해 역전파를 수행하면 기본적으로 그래프는 삭제됩니다. 동일한 그래프를 여러 번 사용하여 역전파를 실행하고 싶다면, <code>retain_graph=True</code> 옵션을 설정해야 합니다. 다음은 그 사용 예입니다:</p>
<pre><code class="language-python">y = forward(x)
l = loss(y, y_true)
l.backward(retain_graph=True)  # 그래프를 유지하여 이후의 backward()에서도 사용 가능
</code></pre>
<p>이 옵션을 사용하는 상황은 드물지만, 복잡한 그래프에서 여러 번의 역전파를 해야 하는 경우에 유용합니다.</p>
<h3 id="_6">다중 입력과 다중 출력에서의 기울기 계산</h3>
<p><code>autograd</code>는 다중 입력과 다중 출력에 대해서도 유연하게 동작합니다. 예를 들어, 입력 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x} \in \mathbb{R}^n</span><script type="math/tex">\mathbf{x} \in \mathbb{R}^n</script></span>와 출력 스칼라 <span class="arithmatex"><span class="MathJax_Preview">y \in \mathbb{R}</span><script type="math/tex">y \in \mathbb{R}</script></span>의 경우, <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span>를 각 입력 요소 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span>에 대해 미분하면 다음과 같은 벡터 기울기를 얻게 됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{g} = \nabla_{\mathbf{x}} y = \left[ \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_n} \right]
</div>
<script type="math/tex; mode=display">
\mathbf{g} = \nabla_{\mathbf{x}} y = \left[ \frac{\partial y}{\partial x_1}, \frac{\partial y}{\partial x_2}, \ldots, \frac{\partial y}{\partial x_n} \right]
</script>
</div>
<p>이는 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>가 주어졌을 때 <code>autograd</code>가 자동으로 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{g}</span><script type="math/tex">\mathbf{g}</script></span>를 계산할 수 있음을 의미합니다. </p>
<p>또한, 다중 출력 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y} \in \mathbb{R}^m</span><script type="math/tex">\mathbf{y} \in \mathbb{R}^m</script></span>일 경우, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span> 사이의 야코비안(Jacobian) 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{J}</span><script type="math/tex">\mathbf{J}</script></span>를 생성할 수 있으며, 이는 다음과 같이 정의됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_m}{\partial x_1} &amp; \frac{\partial y_m}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
</div>
<script type="math/tex; mode=display">
\mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
</script>
</div>
<p>PyTorch의 <code>autograd</code>는 이와 같은 복잡한 구조도 자동으로 처리할 수 있습니다.</p>
<br/>
<br/>
</div>
<footer class="container-fluid wm-page-content">
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>