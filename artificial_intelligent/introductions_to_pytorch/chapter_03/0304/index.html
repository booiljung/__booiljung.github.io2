<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_03/0304/" rel="canonical"/>
<link href="../../../../img/favicon.ico" rel="shortcut icon"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
<title>requires_grad 속성과 중간 연산 추적 - 소프트웨어 융합</title>
<link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet"/>
<link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet"/>
<link href="../../../../css/base.css" rel="stylesheet"/>
<link href="../../../../css/highlight.css" rel="stylesheet"/>
<link href="../../../../css/custom.css" rel="stylesheet"/>
<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
<script src="../../../../js/jquery-3.2.1.min.js"></script>
<script src="../../../../js/bootstrap-3.3.7.min.js"></script>
<script src="../../../../js/highlight.pack.js"></script>
<base target="_top"/>
<script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "requires_grad \uc18d\uc131\uacfc \uc911\uac04 \uc5f0\uc0b0 \ucd94\uc801", url: "#_top", children: [
              {title: "requires_grad \uc18d\uc131\uc758 \uc5ed\ud560", url: "#requires_grad_1" },
              {title: "\uc911\uac04 \uc5f0\uc0b0\uacfc \uadf8\ub798\ud504 \ucd94\uc801", url: "#_1" },
              {title: "\uc5f0\uc0b0 \uadf8\ub798\ud504\uc758 \ub3d9\uc801 \uc0dd\uc131", url: "#_2" },
              {title: "\uc5f0\uc0b0 \uacb0\uacfc\uc640 requires_grad \uc18d\uc131", url: "#requires_grad_2" },
              {title: "\uc5f0\uc0b0 \ucd94\uc801\uacfc \uba54\ubaa8\ub9ac \uad00\ub9ac", url: "#_3" },
              {title: "\uc608\uc81c: \uc5f0\uc0b0 \uadf8\ub798\ud504 \ud574\uc81c", url: "#_4" },
              {title: "\uadf8\ub798\ub514\uc5b8\ud2b8 \ud750\ub984 \ucc28\ub2e8\uacfc detach()\uc758 \uc0ac\uc6a9", url: "#detach" },
              {title: "\uc5f0\uc0b0 \ucd94\uc801\uacfc requires_grad\uc758 \uc911\uc694\uc131", url: "#requires_grad_3" },
              {title: "\uc911\uac04 \uc5f0\uc0b0\uc758 \uadf8\ub798\ub514\uc5b8\ud2b8 \ud750\ub984", url: "#_5" },
              {title: "\uc5ed\uc804\ud30c \uacfc\uc815\uc5d0\uc11c\uc758 \ubbf8\ubd84 \uacc4\uc0b0", url: "#_6" },
              {title: "retain_graph\uc640 detach \uc0ac\uc6a9\uc758 \uc608", url: "#retain_graph-detach" },
              {title: "\uc5f0\uc0b0 \uc911\ub2e8\uacfc \uc785\ub825-\ucd9c\ub825 \uac04\uc758 \uad00\uacc4", url: "#-" },
          ]},
        ];

    </script>
<script src="../../../../js/base.js"></script>
<script src="../../../../js/google_analytics.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script>
</meta></head>
<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>
<div class="container-fluid wm-page-content">
<a name="_top"></a>
<h1 id="requires_grad">requires_grad 속성과 중간 연산 추적</h1>
<p>PyTorch에서 텐서 연산을 다룰 때, <code>requires_grad</code> 속성은 자동 미분 기능을 활성화하는 중요한 역할을 한다. <code>requires_grad=True</code>로 설정된 텐서는 모든 연산 과정을 추적하며, 이를 통해 손쉽게 그레이디언트를 계산할 수 있다. 이 속성을 이해하고 적절하게 사용하는 것은 PyTorch에서 신경망을 구현하거나 학습을 최적화하는 데 필수적이다.</p>
<h3 id="requires_grad_1">requires_grad 속성의 역할</h3>
<p>일반적으로 <code>requires_grad</code> 속성은 텐서가 학습 가능한 파라미터인지 여부를 결정한다. 예를 들어, 신경망의 가중치나 편향과 같은 파라미터들은 학습 과정에서 그레이디언트가 필요하므로 <code>requires_grad=True</code>로 설정된다. 반대로 학습과 무관한 값들은 <code>requires_grad=False</code>로 설정하여 연산 추적 및 메모리 사용을 방지할 수 있다.</p>
<p>다음은 간단한 예이다:</p>
<pre><code class="language-python">import torch

# requires_grad=True로 설정된 텐서
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2  # 연산이 추적된다.
</code></pre>
<p>위 코드에서 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>는 <code>requires_grad=True</code>로 설정되어 있으므로, 이후 연산인 <span class="arithmatex"><span class="MathJax_Preview"> y = x^2 </span><script type="math/tex"> y = x^2 </script></span>도 그 연산의 그래프가 자동으로 생성되고 추적된다.</p>
<h3 id="_1">중간 연산과 그래프 추적</h3>
<p>PyTorch에서 <code>requires_grad=True</code>로 설정된 텐서로 연산이 이루어지면, 각 연산은 그래프에 기록되고, 이 그래프는 연산의 흐름을 표현한다. 이를 통해 역방향 그래디언트 계산 시에 각 연산의 영향을 자동으로 추적하여 미분을 수행할 수 있다. </p>
<p>아래의 그림은 간단한 중간 연산 추적을 나타낸다:</p>
<div class="mermaid">graph TD;
    A[x (requires_grad=True)] --&gt; B[y = x^2];
    B --&gt; C[z = 2 * y];
    C --&gt; D[backward()]
</div>
<p>위의 그래프에서 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>가 <code>requires_grad=True</code>로 설정되어 있고, 이를 이용한 연산 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>, <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>는 모두 추적되어 역방향 연산에 사용된다. 이러한 그래프는 연산을 수행할 때 동적으로 생성되며, 필요 시 해제되어 메모리를 절약할 수 있다.</p>
<h3 id="_2">연산 그래프의 동적 생성</h3>
<p>연산 그래프는 동적으로 생성되며, 한 번 연산이 완료된 이후에도 새로운 텐서 연산이 발생할 때마다 새롭게 생성된다. 예를 들어:</p>
<pre><code class="language-python">x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x ** 2
z = 2 * y + 3
</code></pre>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>는 각각 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>의 중간 연산의 결과이며, PyTorch는 이 과정에서 자동으로 연산 그래프를 생성하여 모든 중간 연산을 추적한다. </p>
<p>수식으로 나타내면 다음과 같다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{x} = \begin{bmatrix} 2.0 \\ 3.0 \end{bmatrix}, \quad \mathbf{y} = \mathbf{x}^2, \quad \mathbf{z} = 2\mathbf{y} + 3
</div>
<script type="math/tex; mode=display">
\mathbf{x} = \begin{bmatrix} 2.0 \\ 3.0 \end{bmatrix}, \quad \mathbf{y} = \mathbf{x}^2, \quad \mathbf{z} = 2\mathbf{y} + 3
</script>
</div>
<p>이 연산의 결과로 생성된 그래프는 역방향으로 미분할 때 자동으로 그래디언트를 계산한다.</p>
<h3 id="requires_grad_2">연산 결과와 requires_grad 속성</h3>
<p>연산의 결과로 나온 새로운 텐서는 기존의 <code>requires_grad</code> 속성을 상속받는다. 만약 원래의 입력 텐서가 <code>requires_grad=True</code>였다면, 모든 중간 연산 결과들도 자동으로 <code>requires_grad=True</code>를 가지게 된다. 이러한 점은 학습 중에 불필요한 연산을 피하는 데 유용하다. </p>
<p>예를 들어, 학습 과정에서 입력 데이터를 전처리할 때 <code>requires_grad</code>를 꺼두면, PyTorch는 해당 연산을 추적하지 않으며 메모리와 계산 비용을 절약할 수 있다.</p>
<pre><code class="language-python">x = torch.tensor([1.0, 2.0], requires_grad=True)
with torch.no_grad():
    y = x * 2  # 추적하지 않음
</code></pre>
<p>위의 코드에서 <code>torch.no_grad()</code> 컨텍스트 내부의 연산은 <code>requires_grad</code>와 관계없이 추적되지 않는다. 이는 파라미터를 업데이트하지 않는 테스트 과정 등에서 유용하게 사용된다.</p>
<h3 id="_3">연산 추적과 메모리 관리</h3>
<p>PyTorch에서 연산 그래프는 역전파를 위해 생성되며, 그 과정에서 각 연산의 중간 결과와 연결 정보를 메모리에 저장한다. 그러나 이러한 연산 추적은 메모리를 많이 사용하기 때문에 필요하지 않은 연산에 대해서는 추적을 피하는 것이 좋다.</p>
<p>연산 추적을 중단하거나 메모리를 절약하는 방법은 다음과 같다:</p>
<ol>
<li>
<p><strong><code>torch.no_grad()</code></strong>:
   이 컨텍스트 매니저는 블록 내의 모든 연산이 추적되지 않도록 설정한다. 학습 과정이 아닌 추론(inference) 단계에서 주로 사용된다.</p>
</li>
<li>
<p><strong><code>.detach()</code></strong>:
   특정 텐서에서 연산 그래프를 분리하여 추적을 방지한다. 새로운 텐서를 반환하지만, 원래 텐서와 동일한 값을 가지면서도 연산 그래프에서는 분리된다.</p>
</li>
<li>
<p><strong><code>retain_graph=True</code> 옵션</strong>:
   <code>backward()</code>를 호출할 때, 기본적으로 사용된 그래프는 해제된다. 그래프를 유지하고 싶다면 <code>retain_graph=True</code>를 명시해야 한다. 이 옵션은 연속해서 역방향 계산이 필요한 경우 유용하지만, 메모리를 더 사용하게 된다.</p>
</li>
</ol>
<h3 id="_4">예제: 연산 그래프 해제</h3>
<p>일반적으로 <code>backward()</code> 함수가 호출되면 연산 그래프는 해제되어 메모리를 확보한다. 그러나 특정 시점에서 동일한 그래프를 반복적으로 사용하여 여러 번의 그레이디언트를 계산해야 할 경우 <code>retain_graph=True</code>를 사용하여 그래프를 유지할 수 있다.</p>
<pre><code class="language-python">x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2
y.backward(torch.tensor([1.0, 1.0, 1.0]), retain_graph=True)  # 그래프 유지
y.backward(torch.tensor([1.0, 1.0, 1.0]))  # 두 번째 호출 시에도 동작
</code></pre>
<p>위의 코드에서 <code>retain_graph=True</code>가 설정되어 있어, <code>y.backward()</code>를 여러 번 호출해도 그래프가 유지된다. 하지만 이 옵션은 메모리 사용량을 증가시킬 수 있으므로 필요할 때만 사용해야 한다.</p>
<h3 id="detach">그래디언트 흐름 차단과 detach()의 사용</h3>
<p>텐서의 연산 중 특정 연산이 그래디언트를 필요로 하지 않는 경우 <code>detach()</code> 메서드를 사용하여 그래디언트 흐름을 차단할 수 있다. 예를 들어:</p>
<pre><code class="language-python">x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x ** 2
z = y.detach()  # z는 requires_grad=False로 설정됨
</code></pre>
<p>위 코드에서 <code>z</code>는 <code>y</code>와 동일한 값을 가지지만, 연산 그래프에서 분리되어 그래디언트 추적이 이루어지지 않는다. 이 방식은 데이터 변형 중에 불필요한 연산을 제거할 때 유용하게 사용할 수 있다.</p>
<p>수식으로 나타내면:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{y} = \mathbf{x}^2, \quad \mathbf{z} = \mathbf{y}_{\text{detach}}
</div>
<script type="math/tex; mode=display">
\mathbf{y} = \mathbf{x}^2, \quad \mathbf{z} = \mathbf{y}_{\text{detach}}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{z}</span><script type="math/tex">\mathbf{z}</script></span>는 더 이상 역방향 미분을 위한 그래프를 추적하지 않는다.</p>
<h3 id="requires_grad_3">연산 추적과 requires_grad의 중요성</h3>
<p>모델 학습 과정에서 주의해야 할 점은 연산 추적이 자동으로 일어나지만, 모든 텐서가 필요하지 않다는 것이다. 예를 들어, 입력 데이터 또는 단순 상수 텐서는 <code>requires_grad=False</code>로 설정하여 불필요한 메모리 사용을 줄일 수 있다.</p>
<p>특히 파이토치의 자동 미분은 다음과 같은 상황에서 연산 그래프를 활용한다:</p>
<ul>
<li><strong>역전파 (Backpropagation)</strong>:
  <code>backward()</code> 함수를 호출할 때, <code>requires_grad=True</code>로 설정된 텐서의 연산 그래프를 따라가며 모든 연산에 대한 미분을 자동으로 계산한다.</li>
<li><strong>그레이디언트 업데이트</strong>:
  미분 값을 이용해 파라미터를 업데이트하며, 이 과정에서 사용된 연산의 추적을 통해 그레이디언트가 계산된다.</li>
</ul>
<p>코드로 보자면:</p>
<pre><code class="language-python">w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)
output = w * 3 + b
output.sum().backward()
</code></pre>
<p>이 코드에서 <code>w</code>와 <code>b</code>는 <code>requires_grad=True</code>로 설정되었고, 연산의 결과는 <code>output</code>을 통해 최종 손실 함수로 이어진다. <code>backward()</code> 호출 시, 파라미터 <code>w</code>와 <code>b</code>의 그레이디언트가 자동으로 계산된다.</p>
<h3 id="_5">중간 연산의 그래디언트 흐름</h3>
<p>PyTorch에서 <code>requires_grad=True</code>인 텐서가 연산에 참여하면, 그 텐서를 이용한 중간 연산들은 자동으로 연산 그래프의 일부가 된다. 이로 인해 연산이 여러 단계로 이루어졌을 때, 각 단계마다의 그래디언트 흐름을 자동으로 추적할 수 있게 된다. 이는 신경망의 역전파 과정에서 필수적인 기능이다.</p>
<p>예를 들어, 다음과 같은 계산 과정을 살펴보자:</p>
<pre><code class="language-python">x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
z = 3 * y + 4
w = z ** 2
w.backward()
</code></pre>
<p>이 코드에서 <code>x</code>로부터 시작된 연산은 여러 단계로 이어지며, 모든 중간 연산이 그래프에 기록된다. 연산 과정을 수식으로 나타내면:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y = \mathbf{x}^2, \quad z = 3y + 4, \quad w = z^2
</div>
<script type="math/tex; mode=display">
y = \mathbf{x}^2, \quad z = 3y + 4, \quad w = z^2
</script>
</div>
<p>연산 그래프는 다음과 같다:</p>
<div class="mermaid">graph TD;
    A[x (requires_grad=True)] --&gt; B[y = x^2];
    B --&gt; C[z = 3y + 4];
    C --&gt; D[w = z^2];
    D --&gt; E[backward()]
</div>
<h3 id="_6">역전파 과정에서의 미분 계산</h3>
<p><code>backward()</code> 함수가 호출되면, 연산 그래프의 끝에서부터 시작하여 역방향으로 각 노드의 미분이 자동으로 계산된다. 이는 체인 룰(chain rule)에 의해 이루어지며, 각 연산 단계에서 미분 값이 누적되어 최종 입력 텐서의 그래디언트를 계산하게 된다.</p>
<p>위의 예제를 바탕으로 구체적인 미분 과정을 살펴보자:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial w}{\partial z} = 2z, \quad \frac{\partial z}{\partial y} = 3, \quad \frac{\partial y}{\partial x} = 2x
</div>
<script type="math/tex; mode=display">
\frac{\partial w}{\partial z} = 2z, \quad \frac{\partial z}{\partial y} = 3, \quad \frac{\partial y}{\partial x} = 2x
</script>
</div>
<p>따라서, 연쇄 법칙을 적용하여 다음과 같이 <span class="arithmatex"><span class="MathJax_Preview"> \frac{\partial w}{\partial x} </span><script type="math/tex"> \frac{\partial w}{\partial x} </script></span>를 구할 수 있다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial w}{\partial x} = \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x} = 2z \cdot 3 \cdot 2x
</div>
<script type="math/tex; mode=display">
\frac{\partial w}{\partial x} = \frac{\partial w}{\partial z} \cdot \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x} = 2z \cdot 3 \cdot 2x
</script>
</div>
<p>이러한 미분 계산은 PyTorch 내부적으로 자동으로 수행되며, 사용자는 <code>backward()</code> 함수를 호출하는 것만으로 최종 결과를 얻을 수 있다. 코드의 마지막 줄에서 <code>w.backward()</code>를 호출하면, <code>x.grad</code>에는 <span class="arithmatex"><span class="MathJax_Preview"> \frac{\partial w}{\partial x} </span><script type="math/tex"> \frac{\partial w}{\partial x} </script></span>의 값이 저장된다.</p>
<h3 id="retain_graph-detach">retain_graph와 detach 사용의 예</h3>
<p>학습 과정에서는 주로 한 번의 역전파로 그래디언트를 계산하고, 그 그래프는 해제된다. 하지만 때로는 동일한 연산 그래프를 사용하여 여러 번의 역전파를 해야 하는 상황이 발생할 수 있다. 이때 <code>retain_graph=True</code>를 사용하거나, 중간 연산의 그래디언트를 필요로 하지 않을 경우 <code>detach()</code>를 사용하여 메모리 낭비를 방지할 수 있다.</p>
<h4 id="1-retain_graph">예시 1: retain_graph 사용</h4>
<pre><code class="language-python">x = torch.tensor([1.0], requires_grad=True)
y = x ** 2
y.backward(retain_graph=True)  # 그래프 유지
y.backward()  # 두 번째 backward 호출 가능
</code></pre>
<p>이 코드에서 <code>retain_graph=True</code>가 설정되어 있어, 첫 번째 <code>backward()</code> 이후에도 그래프가 해제되지 않는다. 따라서 동일한 그래프를 다시 사용하여 두 번째 미분을 수행할 수 있다.</p>
<h4 id="2-detach">예시 2: detach 사용</h4>
<pre><code class="language-python">a = torch.tensor([2.0, 3.0], requires_grad=True)
b = a * 2
c = b.detach()  # b의 그래프에서 분리된 텐서 생성
d = c ** 2  # c는 requires_grad=False 상태로 연산 수행
</code></pre>
<p>여기서 <code>c</code>는 <code>b.detach()</code>에 의해 그래프에서 분리되었으므로, <code>d</code>를 구할 때에는 자동 미분 추적이 수행되지 않는다. 이는 신경망의 특정 부분이 고정되거나, 역전파 계산이 필요 없는 연산을 포함할 때 유용하다.</p>
<h3 id="-">연산 중단과 입력-출력 간의 관계</h3>
<p>실제로 신경망 학습 시에는 특정 연산의 추적을 중단하거나, 입력 데이터가 신경망으로 들어올 때 연산의 흐름을 명확하게 제어하는 것이 중요하다. 예를 들어, 모델 학습 중간에 평가 단계가 들어간다면, 불필요한 연산을 막기 위해 <code>torch.no_grad()</code> 컨텍스트를 사용하여 추적을 비활성화할 수 있다.</p>
<pre><code class="language-python">with torch.no_grad():
    output = model(test_input)  # 학습 중이 아닌 상태로 추론 수행
</code></pre>
<p>이와 같이 <code>torch.no_grad()</code> 블록을 사용하면 모든 텐서의 <code>requires_grad</code> 속성은 임시로 <code>False</code>가 되어, 메모리 사용과 연산 속도가 최적화된다.</p>
<br/>
<br/>
</div>
<footer class="container-fluid wm-page-content">
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>