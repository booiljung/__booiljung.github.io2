<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/introductions_to_pytorch/chapter_03/0301/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Autograd의 개념과 작동 원리 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Autograd\uc758 \uac1c\ub150\uacfc \uc791\ub3d9 \uc6d0\ub9ac", url: "#_top", children: [
              {title: "\ud150\uc11c\uc640 \uc5f0\uc0b0 \uadf8\ub798\ud504", url: "#_1" },
              {title: "\uc21c\uc804\ud30c\uc640 \uc5ed\uc804\ud30c", url: "#_2" },
              {title: "\uccb4\uc778 \ub8f0\uacfc \uc5f0\uc0b0 \uadf8\ub798\ud504", url: "#_5" },
              {title: "\uc5f0\uc0b0 \uadf8\ub798\ud504\uc758 \ubc29\ud5a5\uc131\uacfc \ub3d9\uc801 \uc0dd\uc131", url: "#_6" },
              {title: "\uc5f0\uc0b0\uc758 \uae30\ub85d\uacfc requires_grad \uc18d\uc131", url: "#requires_grad" },
              {title: ".grad_fn\uc744 \ud1b5\ud55c \ud568\uc218 \ucd94\uc801", url: "#grad_fn" },
              {title: "\ub2e4\ucc28\uc6d0 \ud150\uc11c\uc758 \uc790\ub3d9 \ubbf8\ubd84", url: "#_7" },
              {title: "\ubca1\ud130\uc640 \ud589\ub82c\uc758 \uc790\ub3d9 \ubbf8\ubd84: \uc790\ucf54\ube44\uc548\uacfc \uccb4\uc778 \ub8f0", url: "#_8" },
              {title: "\uace0\ucc28\uc6d0 \ud150\uc11c\uc640 \ubc30\uce58 \uc5f0\uc0b0", url: "#_9" },
              {title: "\uba54\ubaa8\ub9ac \uad00\ub9ac\uc640 \ubbf8\ubd84 \uadf8\ub798\ud504\uc758 \uc18c\uba78", url: "#_10" },
              {title: "\uadf8\ub798\ub514\uc5b8\ud2b8\uc758 \uc815\ubc00\ub3c4\uc640 \uc218\uce58 \ubd88\uc548\uc815\uc131", url: "#_11" },
              {title: "\uc5f0\uc0b0 \uadf8\ub798\ud504\uc758 \ube44\ud65c\uc131\ud654: torch.no_grad()", url: "#torchno_grad" },
              {title: "\uba40\ud2f0\ud0dc\uc2a4\ud0b9\uc744 \uc704\ud55c autograd: \ubcd1\ub82c \uadf8\ub798\ub514\uc5b8\ud2b8 \uacc4\uc0b0", url: "#autograd_1" },
              {title: "\ubbf8\ubd84\uc758 \uace0\ucc28 \ub3c4\ud568\uc218 \uacc4\uc0b0", url: "#_12" },
              {title: "\uae30\uc6b8\uae30 \uc18c\uc2e4\uacfc \uae30\uc6b8\uae30 \ud3ed\ubc1c \ubc29\uc9c0: \ud074\ub9ac\ud551(Clipping)\uacfc \uc815\uaddc\ud654", url: "#clipping" },
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    

    <h1 id="autograd">Autograd의 개념과 작동 원리</h1>
<p>PyTorch의 핵심 기능 중 하나는 <code>autograd</code>라는 자동 미분(Automatic Differentiation) 기능입니다. <code>autograd</code>는 텐서 연산을 기록하고, 이를 바탕으로 연산의 그래디언트를 자동으로 계산할 수 있게 해줍니다. 이 기능은 딥러닝의 학습 과정에서 필수적인 역전파 알고리즘을 쉽게 구현할 수 있도록 해주며, 모델의 파라미터 업데이트를 위해 기울기를 계산하는 과정을 간소화합니다.</p>
<h3 id="_1">텐서와 연산 그래프</h3>
<p>딥러닝에서의 모델 학습은 대부분의 경우, 텐서 연산으로 표현됩니다. PyTorch의 <code>autograd</code>는 이러한 텐서 연산을 기반으로 연산 그래프(Computation Graph)를 구축합니다. 연산 그래프는 각 연산이 노드(Node)로 표현되고, 각 텐서가 엣지(Edge)로 연결된 형태입니다. 이를 통해 연산의 관계를 추적할 수 있으며, 역전파를 통해 기울기를 계산할 수 있습니다.</p>
<p>예를 들어, 다음과 같은 연산이 있다고 가정해보겠습니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
z = f(x, y) = x^2 + y^2
</div>
<script type="math/tex; mode=display">
z = f(x, y) = x^2 + y^2
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>는 입력 텐서이고, <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>는 출력 텐서입니다. 이때 <code>autograd</code>는 내부적으로 다음과 같은 연산 그래프를 생성합니다:</p>
<ul>
<li>입력: <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>, <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span></li>
<li>연산: <span class="arithmatex"><span class="MathJax_Preview"> x^2 </span><script type="math/tex"> x^2 </script></span>, <span class="arithmatex"><span class="MathJax_Preview"> y^2 </span><script type="math/tex"> y^2 </script></span></li>
<li>출력: <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span></li>
</ul>
<p>이 연산 그래프에서 각각의 노드는 연산을 나타내며, 엣지는 텐서를 나타냅니다.</p>
<h3 id="_2">순전파와 역전파</h3>
<p>연산 그래프를 이용해 <code>autograd</code>는 순전파(Forward Propagation)와 역전파(Backpropagation) 과정을 수행합니다. 순전파는 입력 값을 주어진 연산을 통해 계산하여 최종 출력을 얻는 과정이며, 역전파는 최종 출력에서 시작하여 입력 변수들에 대한 기울기를 계산하는 과정입니다.</p>
<h4 id="_3">순전파 과정</h4>
<p>순전파에서는 입력 값들이 주어진 연산을 통해 전달되며, 중간 값과 최종 출력 값이 계산됩니다. 예를 들어, 위의 예시에서 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>가 각각 3과 4라면:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
z = 3^2 + 4^2 = 9 + 16 = 25
</div>
<script type="math/tex; mode=display">
z = 3^2 + 4^2 = 9 + 16 = 25
</script>
</div>
<h4 id="_4">역전파 과정</h4>
<p>역전파는 순전파와 반대로, 출력 값으로부터 각 변수들에 대한 기울기를 계산하는 과정입니다. 연산 그래프에서 최종 출력에 대한 미분을 각 변수로 전파시키면서, 각각의 변수에 대한 미분 값을 계산합니다.</p>
<p>연산 그래프에서, 예를 들어 <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>가 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>에 대해 미분된 결과는 다음과 같습니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial z}{\partial x} = 2x, \quad \frac{\partial z}{\partial y} = 2y
</div>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial x} = 2x, \quad \frac{\partial z}{\partial y} = 2y
</script>
</div>
<p>위 식을 통해, 입력 값인 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>가 주어졌을 때 각각의 값에 대한 기울기를 계산할 수 있습니다. <code>autograd</code>는 이러한 미분 연산을 체인 룰(Chain Rule)에 따라 자동으로 처리합니다.</p>
<h3 id="_5">체인 룰과 연산 그래프</h3>
<p>자동 미분의 핵심 원리는 체인 룰을 이용하여 각 연산의 미분을 연쇄적으로 계산하는 것입니다. 수학적으로, 체인 룰은 다음과 같이 정의됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{dz}{dx} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}
</div>
<script type="math/tex; mode=display">
\frac{dz}{dx} = \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}
</script>
</div>
<p>이 원리를 연산 그래프에 적용하면, 각 연산의 결과를 따라 연쇄적으로 미분을 계산할 수 있습니다. 예를 들어, 다음과 같은 연산이 있다고 가정합니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
a = x^2, \quad b = y^2, \quad z = a + b
</div>
<script type="math/tex; mode=display">
a = x^2, \quad b = y^2, \quad z = a + b
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview"> a </span><script type="math/tex"> a </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> b </span><script type="math/tex"> b </script></span>는 각각의 중간 결과입니다. 역전파 과정에서, 최종 출력 <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>에 대한 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>의 미분은 다음과 같이 계산됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial a} \cdot \frac{\partial a}{\partial x}
</div>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial x} = \frac{\partial z}{\partial a} \cdot \frac{\partial a}{\partial x}
</script>
</div>
<p>각각의 미분 값은 다음과 같이 계산할 수 있습니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial z}{\partial a} = 1, \quad \frac{\partial a}{\partial x} = 2x \Rightarrow \frac{\partial z}{\partial x} = 2x
</div>
<script type="math/tex; mode=display">
\frac{\partial z}{\partial a} = 1, \quad \frac{\partial a}{\partial x} = 2x \Rightarrow \frac{\partial z}{\partial x} = 2x
</script>
</div>
<p>이와 같은 방식으로, <code>autograd</code>는 연산 그래프에서의 연산 순서에 따라 역전파를 통해 자동으로 미분 값을 계산합니다.</p>
<h3 id="_6">연산 그래프의 방향성과 동적 생성</h3>
<p><code>autograd</code>의 연산 그래프는 방향성(Directed)을 가지며, 순전파의 방향에 따라 그래프가 구성됩니다. 각 연산이 수행될 때마다 새로운 노드가 생성되고, 이 노드들은 이전 연산과 연결되어 점진적으로 연산 그래프가 구축됩니다. </p>
<p>흥미로운 점은 PyTorch의 연산 그래프가 <strong>동적(Dynamic Computation Graph)</strong> 방식으로 구현된다는 점입니다. 이는 코드가 실행되는 동안 그래프가 실시간으로 생성된다는 것을 의미합니다. 따라서, 반복문과 조건문을 포함하여 다양한 형태의 연산을 수행할 때마다 다른 연산 그래프가 동적으로 생성될 수 있습니다. 이는 정적인 계산 그래프를 사용하는 프레임워크들과 차별화되는 점이며, 보다 유연한 모델 구조를 설계하고 구현할 수 있게 해줍니다.</p>
<h3 id="requires_grad">연산의 기록과 <code>requires_grad</code> 속성</h3>
<p><code>autograd</code>는 텐서의 연산을 추적하기 위해 각 텐서에 <code>requires_grad</code> 속성을 제공합니다. 이 속성이 <code>True</code>로 설정된 텐서는 모든 연산 과정이 자동으로 기록되며, 이를 바탕으로 역전파 과정에서 기울기를 계산할 수 있습니다. </p>
<p>예를 들어, 다음과 같이 텐서를 생성할 수 있습니다:</p>
<pre><code class="language-python">import torch

x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)
z = x * y
</code></pre>
<p>위 코드에서 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>는 <code>requires_grad=True</code>로 설정되었기 때문에, 이들을 이용한 연산 <span class="arithmatex"><span class="MathJax_Preview"> z = x \cdot y </span><script type="math/tex"> z = x \cdot y </script></span>는 <code>autograd</code>에 의해 기록됩니다. 이제 역전파를 수행하여 <span class="arithmatex"><span class="MathJax_Preview"> x </span><script type="math/tex"> x </script></span>와 <span class="arithmatex"><span class="MathJax_Preview"> y </span><script type="math/tex"> y </script></span>의 기울기를 구할 수 있습니다:</p>
<pre><code class="language-python">z.backward()
print(x.grad)  # 출력: tensor(3.0)
print(y.grad)  # 출력: tensor(2.0)
</code></pre>
<p>여기서 <code>z.backward()</code>는 <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>에 대한 각 변수의 미분 값을 자동으로 계산하여, <code>x.grad</code>와 <code>y.grad</code>에 저장합니다.</p>
<h3 id="grad_fn"><code>.grad_fn</code>을 통한 함수 추적</h3>
<p>각 텐서에는 <code>.grad_fn</code> 속성이 있으며, 이는 텐서를 생성한 연산을 추적합니다. 예를 들어, 위의 <span class="arithmatex"><span class="MathJax_Preview"> z </span><script type="math/tex"> z </script></span>는 <span class="arithmatex"><span class="MathJax_Preview"> x \cdot y </span><script type="math/tex"> x \cdot y </script></span>에 의해 생성된 텐서이며, <code>z.grad_fn</code>은 이 연산의 정보를 포함합니다. 이를 통해 <code>autograd</code>는 역전파 과정에서 사용해야 할 연산과 그 순서를 결정할 수 있습니다.</p>
<p>예시로 <code>z.grad_fn</code>을 출력하면 다음과 같은 결과를 확인할 수 있습니다:</p>
<pre><code class="language-python">print(z.grad_fn)
# 출력: &lt;MulBackward0&gt;
</code></pre>
<p>여기서 <code>&lt;MulBackward0&gt;</code>은 곱셈 연산의 역전파 연산을 나타내며, 이는 <code>z</code>가 <code>x</code>와 <code>y</code>의 곱셈으로 생성되었음을 의미합니다.</p>
<h3 id="_7">다차원 텐서의 자동 미분</h3>
<p><code>autograd</code>는 다차원 텐서에도 동일하게 적용됩니다. 벡터나 행렬 연산을 포함하여 다양한 형태의 텐서 연산에서 자동 미분이 가능하며, 이 경우에도 체인 룰이 적용됩니다. 예를 들어, 다음과 같은 벡터 연산을 고려해 보겠습니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{y} = \mathbf{A}\mathbf{x}
</div>
<script type="math/tex; mode=display">
\mathbf{y} = \mathbf{A}\mathbf{x}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{A} </span><script type="math/tex"> \mathbf{A} </script></span>는 <span class="arithmatex"><span class="MathJax_Preview"> n \times m </span><script type="math/tex"> n \times m </script></span> 크기의 행렬, <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{x} </span><script type="math/tex"> \mathbf{x} </script></span>는 <span class="arithmatex"><span class="MathJax_Preview"> m \times 1 </span><script type="math/tex"> m \times 1 </script></span> 크기의 벡터입니다. 이 경우 <span class="arithmatex"><span class="MathJax_Preview"> \mathbf{y} </span><script type="math/tex"> \mathbf{y} </script></span>는 <span class="arithmatex"><span class="MathJax_Preview"> n \times 1 </span><script type="math/tex"> n \times 1 </script></span> 크기의 벡터가 됩니다. 역전파 과정에서는 각각의 원소에 대해 미분을 계산하게 되며, 이를 통해 각 변수의 기울기를 얻을 수 있습니다. </p>
<h3 id="_8">벡터와 행렬의 자동 미분: 자코비안과 체인 룰</h3>
<p>다차원 텐서의 자동 미분은 기본적으로 <strong>자코비안(Jacobian)</strong> 행렬을 사용합니다. 자코비안 행렬은 벡터 함수의 기울기를 일반화한 개념으로, 각 변수의 변화율을 행렬 형태로 표현합니다. 이를 통해, 벡터와 행렬 연산에서도 자동 미분이 가능합니다.</p>
<p>예를 들어, 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>와 벡터 함수 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y} = f(\mathbf{x})</span><script type="math/tex">\mathbf{y} = f(\mathbf{x})</script></span>가 있다고 가정하겠습니다. 여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x} \in \mathbb{R}^n</span><script type="math/tex">\mathbf{x} \in \mathbb{R}^n</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y} \in \mathbb{R}^m</span><script type="math/tex">\mathbf{y} \in \mathbb{R}^m</script></span>입니다. 이 경우 자코비안 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{J}</span><script type="math/tex">\mathbf{J}</script></span>는 다음과 같이 정의됩니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial y_1}{\partial x_1} &amp; \frac{\partial y_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_n} \\ \frac{\partial y_2}{\partial x_1} &amp; \frac{\partial y_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_2}{\partial x_n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \frac{\partial y_m}{\partial x_1} &amp; \frac{\partial y_m}{\partial x_2} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n} \end{bmatrix}
</div>
<script type="math/tex; mode=display">
\mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\ \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n} \end{bmatrix}
</script>
</div>
<p>자동 미분 과정에서 자코비안 행렬을 사용하여 각 변수에 대한 기울기를 계산할 수 있습니다. 체인 룰을 사용해 자코비안 행렬을 계산하면 다음과 같은 방식으로 표현할 수 있습니다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
</div>
<script type="math/tex; mode=display">
\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \cdot \frac{\partial \mathbf{y}}{\partial \mathbf{x}}
</script>
</div>
<p>이 과정에서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{z}</span><script type="math/tex">\mathbf{z}</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>의 함수이며, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{y}</span><script type="math/tex">\mathbf{y}</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>의 함수입니다. PyTorch의 <code>autograd</code>는 이러한 연산들을 자동으로 수행하여, 각 텐서의 기울기를 계산합니다.</p>
<h3 id="_9">고차원 텐서와 배치 연산</h3>
<p>딥러닝 모델에서는 한 번에 여러 개의 데이터를 동시에 처리하기 위해 <strong>배치(batch)</strong> 연산이 빈번하게 사용됩니다. 예를 들어, 이미지 분류 모델에서 입력 이미지를 한 장씩 처리하는 대신, 여러 이미지를 동시에 처리하는 경우가 많습니다. 이를 위해 고차원 텐서가 사용되며, <code>autograd</code>는 이러한 배치 연산에서도 자동 미분을 수행할 수 있습니다.</p>
<p>예를 들어, 입력 데이터가 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{X} \in \mathbb{R}^{B \times D}</span><script type="math/tex">\mathbf{X} \in \mathbb{R}^{B \times D}</script></span>인 경우, 여기서 <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span>는 배치 크기, <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span>는 데이터의 차원입니다. 모델은 이 입력 데이터를 처리하여 출력 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{Y} \in \mathbb{R}^{B \times K}</span><script type="math/tex">\mathbf{Y} \in \mathbb{R}^{B \times K}</script></span>를 생성합니다. 이 경우, 역전파 과정에서 각 배치에 대해 기울기가 개별적으로 계산되어야 하며, 이를 <code>autograd</code>가 자동으로 수행합니다.</p>
<h3 id="_10">메모리 관리와 미분 그래프의 소멸</h3>
<p>연산 그래프가 동적으로 생성되는 특성 때문에, 메모리 관리가 중요합니다. 기본적으로 <code>autograd</code>는 그래디언트를 계산한 후에 그래프를 유지하지 않으며, 메모리를 해제합니다. 이는 메모리 사용량을 줄이기 위해서이며, 특히 복잡한 모델을 사용할 때 중요한 기능입니다.</p>
<p>연산 그래프를 유지하려면, <code>retain_graph=True</code> 옵션을 사용할 수 있습니다. 예를 들어, 여러 번 역전파를 수행해야 하는 경우 다음과 같이 사용할 수 있습니다:</p>
<pre><code class="language-python">z.backward(retain_graph=True)
</code></pre>
<p>그러나 불필요하게 그래프를 유지하는 것은 메모리 사용량을 증가시킬 수 있으므로, 필요한 경우에만 사용해야 합니다.</p>
<h3 id="_11">그래디언트의 정밀도와 수치 불안정성</h3>
<p>미분 연산은 수치적으로 불안정한 경우가 종종 있습니다. 특히 딥러닝에서 매우 작은 값이나 큰 값이 등장할 때 수치 불안정성(numerical instability)이 발생할 수 있습니다. 이를 방지하기 위해, <code>autograd</code>는 연산 과정에서 다양한 최적화 기법을 사용하며, 수치적으로 안정적인 알고리즘을 채택합니다.</p>
<p>예를 들어, 로지스틱 회귀 모델에서는 시그모이드 함수의 출력 값이 매우 작거나 매우 큰 경우에 기울기 값이 0에 가까워지는 <strong>기울기 소실(vanishing gradient)</strong> 문제가 발생할 수 있습니다. <code>autograd</code>는 이러한 문제를 최소화하기 위해, 수치적으로 안정적인 연산 방식을 선택하여 기울기를 계산합니다.</p>
<h3 id="torchno_grad">연산 그래프의 비활성화: <code>torch.no_grad()</code></h3>
<p>모델의 학습이 아닌 추론(inference) 단계에서는 그래디언트 계산이 필요하지 않습니다. 이때에도 불필요한 연산 그래프 생성으로 인해 메모리 사용량이 늘어날 수 있습니다. 이를 방지하기 위해, PyTorch에서는 <code>torch.no_grad()</code> 컨텍스트 매니저를 사용하여 연산 그래프를 비활성화할 수 있습니다. </p>
<p>다음은 이를 활용한 예시입니다:</p>
<pre><code class="language-python">import torch

x = torch.tensor(2.0, requires_grad=True)
with torch.no_grad():
    y = x * 3
print(y.requires_grad)  # 출력: False
</code></pre>
<p>위 코드에서 <code>torch.no_grad()</code> 안에서 수행된 연산은 그래디언트를 계산하기 위한 연산 그래프를 생성하지 않으므로, 메모리 사용량을 줄일 수 있습니다. 이는 모델 평가나 예측 단계에서 매우 유용합니다.</p>
<h3 id="autograd_1">멀티태스킹을 위한 <code>autograd</code>: 병렬 그래디언트 계산</h3>
<p>PyTorch의 <code>autograd</code>는 여러 개의 스칼라 값에 대한 그래디언트를 동시에 계산할 수 있습니다. 예를 들어, 다중 출력 모델의 경우 각 출력에 대한 그래디언트를 개별적으로 구할 수 있습니다. 일반적으로 이러한 상황에서는 자코비안 행렬이 등장하지만, <code>autograd</code>는 손실 함수로부터 직접 기울기를 추적하여 효율적으로 그래디언트를 계산합니다.</p>
<p>만약 다중 스칼라 값이 있을 때 각각에 대한 그래디언트를 구하고 싶다면, 각 값에 대해 <code>backward()</code> 함수를 호출하거나, 한 번의 <code>backward()</code>로 전체 그래디언트를 구할 수 있습니다. 이는 모델의 출력 차원이 커질 때 유용하며, 메모리와 연산 자원을 효율적으로 사용할 수 있습니다.</p>
<h3 id="_12">미분의 고차 도함수 계산</h3>
<p>PyTorch의 <code>autograd</code>는 고차 도함수(Higher-order derivatives)도 지원합니다. 예를 들어, 손실 함수의 2차 도함수(헤시안 행렬)를 계산하고 싶다면, <code>autograd</code>는 이를 자동으로 처리할 수 있습니다. 이를 위해, 기존의 <code>backward()</code> 메서드를 확장한 <code>torch.autograd.grad</code> 함수를 사용할 수 있습니다. </p>
<p>고차 도함수 계산은 뉴턴 메서드(Newton’s Method)와 같은 최적화 기법에 유용할 수 있으며, 다음과 같은 예시로 설명할 수 있습니다:</p>
<pre><code class="language-python">x = torch.tensor(2.0, requires_grad=True)
y = x ** 3

# 1차 미분 (dy/dx)
grad1 = torch.autograd.grad(y, x, create_graph=True)[0]

# 2차 미분 (d²y/dx²)
grad2 = torch.autograd.grad(grad1, x)[0]

print(grad1)  # 출력: tensor(12.0000)
print(grad2)  # 출력: tensor(12.0000)
</code></pre>
<p>이 예시에서, <code>create_graph=True</code> 옵션을 설정함으로써, 1차 미분 결과를 기반으로 다시 연산 그래프를 생성하여 2차 미분도 자동으로 계산할 수 있게 합니다. 이와 같이, PyTorch의 <code>autograd</code>는 고차 도함수 계산을 지원하며, 복잡한 미분 연산을 수행할 수 있습니다.</p>
<h3 id="clipping">기울기 소실과 기울기 폭발 방지: 클리핑(Clipping)과 정규화</h3>
<p>신경망 모델에서 종종 발생하는 문제 중 하나는 기울기 소실(Vanishing Gradient)과 기울기 폭발(Exploding Gradient)입니다. 이는 학습이 진행되면서 기울기의 값이 지나치게 작아지거나 커져서, 역전파를 통한 학습이 제대로 이루어지지 않는 현상입니다. </p>
<p>이 문제를 해결하기 위해, PyTorch는 그래디언트 클리핑(Gradient Clipping)을 제공합니다. 예를 들어, 다음과 같이 그래디언트의 최대 크기를 제한할 수 있습니다:</p>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
</code></pre>
<p>여기서 <code>max_norm</code>은 그래디언트의 최대 허용 크기를 의미하며, 지정된 값을 넘는 그래디언트는 제한됩니다. 이 방법은 기울기 폭발을 방지하여 학습을 안정적으로 유지하는 데 유용합니다.</p>

  <br>
    

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>