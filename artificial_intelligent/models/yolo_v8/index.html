<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<link href="https://booiljung.github.io/artificial_intelligent/models/yolo_v8/" rel="canonical"/>
<link href="../../../img/favicon.ico" rel="shortcut icon"/>
<meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
<title>YOLOv8 - 소프트웨어 융합</title>
<link href="../../../css/bootstrap-3.3.7.min.css" rel="stylesheet"/>
<link href="../../../css/font-awesome-4.7.0.css" rel="stylesheet"/>
<link href="../../../css/base.css" rel="stylesheet"/>
<link href="../../../css/highlight.css" rel="stylesheet"/>
<link href="../../../css/custom.css" rel="stylesheet"/>
<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->
<script src="../../../js/jquery-3.2.1.min.js"></script>
<script src="../../../js/bootstrap-3.3.7.min.js"></script>
<script src="../../../js/highlight.pack.js"></script>
<base target="_top"/>
<script>
      var base_url = '../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "YOLOv8", url: "#_top", children: [
              {title: "\uad6c\uc870\uc801 \uac1c\uc120", url: "#_1" },
              {title: "YOLOv8 \uc544\ud0a4\ud14d\ucc98", url: "#yolov8_1" },
              {title: "5. YOLOv8 \uc544\ud0a4\ud14d\ucc98 \ub2e4\uc774\uc5b4\uadf8\ub7a8", url: "#5-yolov8" },
              {title: "\uc131\ub2a5", url: "#_2" },
              {title: "\uc801\uc6a9 \uac00\ub2a5 \ubd84\uc57c", url: "#_3" },
              {title: "\uae30\uc220\uc801 \ud2b9\uc9d5", url: "#_4" },
              {title: "\ud559\uc2b5 \uacfc\uc815 \ubc0f \uc0ac\uc6a9\ubc95", url: "#_5" },
              {title: "YOLOv8 \uc7a5\uc810", url: "#yolov8_2" },
              {title: "YOLOv8\uc758 \ub2e8\uc810", url: "#yolov8_3" },
              {title: "YOLOv8 \uc694\uad6c \uc790\uc6d0", url: "#yolov8_4" },
              {title: "YOLOv8 \uac1d\uccb4 \ud0d0\uc9c0 PyTorch \uc608\uc81c \ucf54\ub4dc", url: "#yolov8-pytorch" },
              {title: "YOLOv8 \ud559\uc2b5 PyTorch \uc608\uc81c", url: "#yolov8-pytorch_2" },
          ]},
        ];

    </script>
<script src="../../../js/base.js"></script>
<script src="../../../js/google_analytics.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script>
</meta></head>
<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>
<div class="container-fluid wm-page-content">
<a name="_top"></a>
<h1 id="yolov8">YOLOv8</h1>
<p><strong>YOLOv8</strong>은 YOLO(You Only Look Once) 시리즈의 최신 모델로, 객체 탐지 및 분류 작업에서 효율성과 정확도를 동시에 개선한 모델입니다. YOLO는 실시간 객체 탐지 작업에서 자주 사용되었으며, YOLOv8은 성능 향상을 위해 다양한 개선이 이루어진 버전입니다. YOLOv8에 대한 자세한 설명은 다음과 같습니다:</p>
<h3 id="_1"><strong>구조적 개선</strong></h3>
<p>YOLOv8은 기존 YOLO 모델에서 몇 가지 중요한 구조적 개선을 통해 정확도와 처리 속도를 더욱 높였습니다.
- <strong>Backbone 개선</strong>: YOLOv8은 최신 네트워크 아키텍처를 적용하여 특징 추출 성능을 개선했습니다. 이로 인해 더 적은 연산량으로 더 높은 표현 능력을 가지게 되었으며, 특히 작은 객체에 대한 탐지 성능이 향상되었습니다.
- <strong>Neck의 업그레이드</strong>: YOLOv8은 Feature Pyramid Network(FPN)와 Path Aggregation Network(PAN)의 개념을 결합하여, 다양한 크기의 객체를 보다 정확하게 탐지할 수 있도록 목 구조를 개선했습니다.
- <strong>Anchor-Free 디자인</strong>: YOLOv8은 이전 YOLO 모델에서 사용하던 앵커 기반 접근 방식을 벗어나, 앵커 없는(Anchor-Free) 탐지 방식을 채택하였습니다. 이는 학습 속도와 효율성을 높이고, 작은 객체에 대한 탐지 성능을 더욱 향상시키는 데 기여합니다.</p>
<h3 id="yolov8_1">YOLOv8 아키텍처</h3>
<p><strong>YOLOv8 아키텍처</strong>는 이전 YOLO 모델에서 성능과 효율성을 개선한 최신 구조로 설계되었습니다. YOLOv8의 아키텍처는 주요 구성 요소인 <strong>Backbone</strong>, <strong>Neck</strong>, 그리고 <strong>Head</strong>로 구성되며, 다양한 크기의 객체를 탐지할 수 있도록 설계되었습니다. 다음은 YOLOv8의 아키텍처를 자세히 설명한 내용입니다.</p>
<h4 id="1-backbone">1. <strong>Backbone (특징 추출기)</strong></h4>
<p>YOLOv8의 Backbone은 입력 이미지에서 유용한 특징을 추출하는 역할을 합니다. Backbone의 중요한 특징은 <strong>EfficientNet</strong>과 유사한 구조로 설계되어 있어, 높은 효율성과 성능을 제공합니다.</p>
<ul>
<li>
<p><strong>CSPNet (Cross Stage Partial Network)</strong>: YOLOv8의 Backbone에서 <strong>CSP 구조</strong>가 사용됩니다. 이는 YOLOv7에서도 사용되었던 구조로, 네트워크의 반복적인 부분을 줄여 메모리 사용량을 감소시키면서도 특징 추출 성능을 유지할 수 있게 합니다. 이를 통해 학습 속도는 빠르면서도 더 깊은 신경망 구조를 사용할 수 있습니다.</p>
</li>
<li>
<p><strong>Convolutional Layers (합성곱 레이어)</strong>: Backbone에서는 여러 층의 합성곱 레이어가 사용되어 점차적으로 입력 이미지의 해상도를 줄여가며 중요한 시각적 특징을 추출합니다. 이 과정에서 이미지의 세부 정보를 유지하면서 객체의 크기와 위치에 따라 다양한 스케일의 특징을 학습합니다.</p>
</li>
<li>
<p><strong>Focus Layer</strong>: YOLOv8은 이미지를 4개로 분할한 후 이들을 결합하여 중요한 특징을 추출하는 <strong>Focus Layer</strong>를 사용합니다. 이는 객체의 세부 정보를 더 효과적으로 학습하는 데 기여합니다.</p>
</li>
</ul>
<h4 id="2-neck">2. <strong>Neck (다중 스케일 특징 연결)</strong></h4>
<p>Neck는 다양한 크기의 객체를 효과적으로 탐지하기 위해 여러 레벨의 특징을 결합하는 역할을 합니다. YOLOv8의 Neck은 주로 <strong>FPN (Feature Pyramid Network)</strong>과 <strong>PAN (Path Aggregation Network)</strong>을 결합하여 구성됩니다.</p>
<ul>
<li>
<p><strong>FPN (Feature Pyramid Network)</strong>: FPN은 YOLOv8에서 서로 다른 해상도에서 추출된 특징들을 결합하여 다양한 크기의 객체를 탐지하는 데 사용됩니다. 이를 통해 작은 객체부터 큰 객체까지 폭넓은 크기의 객체를 탐지할 수 있게 됩니다.</p>
</li>
<li>
<p><strong>PAN (Path Aggregation Network)</strong>: PAN은 FPN에서 추출한 다중 스케일의 특징을 더 잘 결합하기 위해 사용됩니다. 이는 정보 흐름을 위아래로 모두 전달함으로써 더 풍부한 표현력을 제공합니다.</p>
</li>
<li>
<p><strong>SPP (Spatial Pyramid Pooling)</strong>: SPP는 공간적 특징을 결합하여 객체 탐지 성능을 높이는 데 기여합니다. 여러 크기의 커널을 사용해 다양한 크기의 특징을 결합하여 더욱 강력한 표현을 만들어 냅니다.</p>
</li>
</ul>
<h4 id="3-head">3. <strong>Head (탐지 결과 출력)</strong></h4>
<p>Head는 Backbone과 Neck에서 추출한 특징을 바탕으로 실제 객체를 탐지하는 부분입니다. YOLOv8의 Head는 이전 YOLO 모델과 마찬가지로 <strong>Bounding Box (경계 상자)</strong>와 <strong>객체 클래스</strong>를 예측합니다. YOLOv8에서는 이 과정에서 몇 가지 중요한 개선 사항이 적용되었습니다.</p>
<ul>
<li>
<p><strong>Anchor-Free Design (앵커 없는 디자인)</strong>: YOLOv8은 기존 YOLO 모델에서 사용되었던 앵커 박스를 사용하지 않습니다. 이를 통해 앵커를 설정하는 번거로움 없이 다양한 객체 크기에 대응할 수 있으며, 속도와 정확도 모두 향상됩니다.</p>
</li>
<li>
<p><strong>Objectness Score</strong>: 각 탐지된 객체에 대해 객체가 존재할 확률을 나타내는 점수를 예측하며, 이를 통해 탐지된 객체의 신뢰도를 평가합니다.</p>
</li>
<li>
<p><strong>Bounding Box Regression</strong>: YOLOv8은 객체의 위치와 크기를 경계 상자 좌표로 표현하며, 이를 통해 이미지 내에서 객체가 어디에 위치하고 있는지 예측합니다.</p>
</li>
<li>
<p><strong>Class Prediction</strong>: 탐지된 객체가 어떤 클래스에 속하는지 예측하며, 여러 객체 클래스에 대한 다중 분류가 가능합니다.</p>
</li>
</ul>
<h3 id="5-yolov8">5. <strong>YOLOv8 아키텍처 다이어그램</strong></h3>
<p>YOLOv8 아키텍처를 시각적으로 표현하면 다음과 같습니다:</p>
<div class="mermaid">graph TD;
    A[입력 이미지] --&gt; B[Focus Layer]
    B --&gt; C[CSPNet 기반 Backbone]
    C --&gt; D[FPN + PAN Neck]
    D --&gt; E[Anchor-Free Head]
    E --&gt; F[객체 경계 상자 및 클래스 출력]

</div>
<p>이 아키텍처는 YOLO의 기본 철학인 <strong>실시간 객체 탐지</strong>를 유지하면서도 최신 딥러닝 기술을 활용하여 정확도와 효율성을 극대화한 모델입니다.</p>
<h3 id="_2"><strong>성능</strong></h3>
<p>YOLOv8은 기존 모델에 비해 다음과 같은 성능적 장점을 가지고 있습니다:
- <strong>더 높은 정확도 (mAP)</strong>: YOLOv8은 정확도 측면에서 기존 YOLOv7보다 개선된 성능을 보입니다. mAP(mean Average Precision)가 더 높은 수준으로 향상되어 작은 객체 탐지나 복잡한 장면에서도 더 우수한 성능을 발휘합니다.
- <strong>더 빠른 처리 속도</strong>: YOLOv8은 기존 YOLO 모델의 장점인 속도를 그대로 유지하면서도 더 적은 연산 자원을 사용하여 더 빠르게 객체를 탐지합니다. 이를 통해 실시간 응용 프로그램에서 유용하게 사용될 수 있습니다.</p>
<h3 id="_3"><strong>적용 가능 분야</strong></h3>
<p>YOLOv8은 다양한 비전 작업에서 활용할 수 있습니다:
- <strong>실시간 객체 탐지</strong>: YOLOv8은 특히 실시간 작업에서 뛰어난 성능을 발휘하며, 드론, 자율 주행, 보안 카메라 등 실시간 모니터링과 같은 응용 분야에 적합합니다.
- <strong>이미지 및 비디오 분석</strong>: 비디오 스트림에서 다수의 객체를 동시에 탐지할 수 있으며, 추적과 분류 작업에도 유리합니다.
- <strong>의료 영상 분석</strong>: 높은 정확도로 병변이나 이상 징후를 탐지할 수 있어 의료 영상에서 유용하게 사용될 수 있습니다.</p>
<h3 id="_4"><strong>기술적 특징</strong></h3>
<ul>
<li><strong>Input Resolution</strong>: YOLOv8은 다양한 입력 해상도를 지원하며, 작은 객체 탐지에서도 강력한 성능을 보입니다.</li>
<li><strong>Lightweight</strong>: 경량화된 모델 구조 덕분에 모바일 장치나 임베디드 시스템에서도 YOLOv8을 활용할 수 있습니다.</li>
<li><strong>Dynamic Inference</strong>: YOLOv8은 모델 크기와 연산 자원에 따라 동적으로 추론을 최적화할 수 있어 다양한 하드웨어 환경에 적용 가능합니다.</li>
</ul>
<h3 id="_5"><strong>학습 과정 및 사용법</strong></h3>
<p>YOLOv8은 사용하기 간편하며, PyTorch, TensorFlow와 같은 딥러닝 프레임워크에서 쉽게 학습 및 추론을 수행할 수 있습니다. 사용자는 다음 단계를 통해 YOLOv8을 학습시키고 사용할 수 있습니다:
- <strong>데이터 준비</strong>: 원하는 객체 탐지 작업에 맞는 데이터셋을 준비합니다.
- <strong>학습 및 검증</strong>: 준비된 데이터셋으로 모델을 학습시키고, 검증 데이터로 성능을 평가합니다.
- <strong>추론(Inference)</strong>: 학습된 모델을 사용하여 실시간으로 객체를 탐지하거나 이미지, 비디오에서 결과를 확인합니다.</p>
<pre><code class="language-python">import torch
model = torch.hub.load('ultralytics/yolov8', 'yolov8s')  # YOLOv8 모델 불러오기
img = 'https://ultralytics.com/images/zidane.jpg'  # 테스트 이미지
results = model(img)  # 추론 실행
results.show()  # 결과 시각화
</code></pre>
<h3 id="yolov8_2">YOLOv8 장점</h3>
<h4 id="_6"><strong>높은 정확도</strong>:</h4>
<ul>
<li>YOLOv8은 이전 버전보다 더 높은 정확도를 제공합니다. 특히 작은 객체 탐지나 복잡한 장면에서 mAP(mean Average Precision)가 개선되어 다양한 환경에서 우수한 성능을 발휘합니다.</li>
</ul>
<h4 id="_7"><strong>빠른 처리 속도</strong>:</h4>
<ul>
<li>YOLOv8은 실시간 객체 탐지 작업에서 매우 빠른 속도를 자랑합니다. 경량화된 아키텍처 덕분에 고성능을 유지하면서도 짧은 처리 시간을 요구하며, 실시간 비디오 분석이나 드론과 같은 응용에 적합합니다.</li>
</ul>
<h4 id="_8"><strong>효율적인 자원 사용</strong>:</h4>
<ul>
<li>모델이 경량화되어 GPU 메모리 사용량이 낮고, 연산 자원이 적게 소모됩니다. 이로 인해 모바일 장치나 임베디드 시스템에서도 효율적으로 사용될 수 있습니다.</li>
</ul>
<h4 id="anchor-free"><strong>Anchor-Free 디자인</strong>:</h4>
<ul>
<li>YOLOv8은 앵커 기반 방식에서 벗어나, 앵커 없이도 더 빠르고 정확하게 객체를 탐지할 수 있습니다. 이로 인해 학습이 간소화되고 더 나은 일반화 성능을 보입니다.</li>
</ul>
<h4 id="_9"><strong>다양한 해상도 지원</strong>:</h4>
<ul>
<li>YOLOv8은 다양한 입력 해상도를 지원하며, 고해상도 이미지에서도 작은 객체를 탐지하는 데 뛰어난 성능을 보입니다.</li>
</ul>
<h4 id="_10"><strong>적용 범위의 다양성</strong>:</h4>
<ul>
<li>YOLOv8은 실시간 객체 탐지뿐만 아니라 이미지 분류, 세그멘테이션, 추적 작업까지 다양한 비전 응용에 활용 가능합니다.</li>
</ul>
<h4 id="_11"><strong>모듈화된 구조</strong>:</h4>
<ul>
<li>YOLOv8은 최신 딥러닝 프레임워크(PyTorch 등)를 기반으로 쉽게 커스터마이징할 수 있으며, 필요에 따라 모델 크기를 조정하거나 특정 작업에 맞게 튜닝할 수 있습니다.</li>
</ul>
<h3 id="yolov8_3">YOLOv8의 단점</h3>
<h4 id="_12"><strong>복잡한 객체나 배경 처리</strong></h4>
<ul>
<li><strong>복잡한 배경에서의 오탐지</strong>: YOLOv8은 여전히 단순한 장면에서 뛰어난 성능을 보이지만, 복잡한 배경이나 여러 객체가 겹치는 상황에서는 오탐지(False Positive)가 발생할 수 있습니다. 특히, 포트홀과 같은 작은 객체가 복잡한 도로 환경에서 다른 손상이나 그림자와 겹칠 경우, 탐지 성능이 떨어질 수 있습니다.</li>
</ul>
<h4 id="_13"><strong>앵커리스 방식의 한계</strong></h4>
<ul>
<li><strong>Anchor-Free 방식의 적응 문제</strong>: YOLOv8은 앵커리스(Anchor-Free) 탐지 방식을 채택하여 연산 효율을 높였지만, 이 방식이 모든 데이터셋에 최적화되지는 않을 수 있습니다. 특정 상황에서는 앵커 기반 탐지가 더 나은 성능을 보일 수 있으며, 작은 객체나 특이한 모양의 객체에서 성능 저하가 발생할 수 있습니다.</li>
</ul>
<h4 id="_14"><strong>모델 크기와 하드웨어 요구사항</strong></h4>
<ul>
<li><strong>경량 모델이 아님</strong>: YOLOv8은 이전 버전들에 비해 성능이 크게 향상되었지만, 여전히 계산량이 많고 메모리 사용량이 크기 때문에 고성능 하드웨어에서만 최적의 성능을 발휘할 수 있습니다. 임베디드 시스템이나 저전력 환경에서는 이로 인해 성능이 제한될 수 있습니다.</li>
<li><strong>모바일 환경에서의 제한</strong>: YOLOv8이 경량화된 버전도 제공하고 있지만, 일부 복잡한 애플리케이션에서는 여전히 모바일 기기에서의 실행이 어려울 수 있습니다.</li>
</ul>
<h4 id="_15"><strong>학습 시간 및 데이터셋 의존성</strong></h4>
<ul>
<li><strong>학습 시간 증가</strong>: YOLOv8은 더 깊은 네트워크와 더 많은 파라미터를 포함하고 있기 때문에, 대규모 데이터셋을 학습할 때 더 긴 학습 시간이 필요합니다. 이는 실시간 애플리케이션을 개발하는 경우 개발 속도를 늦출 수 있습니다.</li>
<li><strong>대량의 레이블링 데이터 필요</strong>: YOLOv8은 고성능 모델이지만, 이를 최대한 활용하기 위해서는 충분한 양의 고품질 레이블링된 데이터셋이 필요합니다. 특히 포트홀과 같이 특정 도메인에 특화된 탐지 작업에서는 적절한 데이터셋을 준비하는 것이 중요하며, 부족한 데이터는 모델 성능 저하로 이어질 수 있습니다.</li>
</ul>
<h4 id="_16"><strong>일반화 문제</strong></h4>
<ul>
<li><strong>도메인 특화 모델 성능</strong>: YOLOv8은 일반적인 객체 탐지 작업에서 우수한 성능을 보이지만, 특정 도메인에 특화된 문제(예: 의료 영상, 특정 도로 손상 패턴 탐지)에서는 성능이 기대에 못 미칠 수 있습니다. 도로 환경에서 발생하는 특정 유형의 손상을 정확하게 탐지하려면 YOLOv8을 해당 도메인에 맞게 적절히 튜닝하고, 추가적인 데이터 전처리 및 후처리가 필요할 수 있습니다.</li>
</ul>
<h4 id="_17"><strong>후처리 필요성</strong></h4>
<ul>
<li><strong>추가적인 후처리 작업</strong>: YOLOv8은 객체를 빠르게 탐지할 수 있지만, 포트홀 탐지와 같은 구체적인 작업에서는 탐지된 영역을 후처리해야 할 필요가 있을 수 있습니다. 예를 들어, 탐지된 포트홀의 크기나 심각도를 평가하거나 위치를 정확하게 결정하기 위해 추가적인 계산이나 알고리즘이 필요할 수 있습니다.</li>
</ul>
<p>이러한 단점들은 상황에 따라 모델 선택 시 고려해야 할 요소이며, YOLOv8이 모든 상황에서 완벽한 솔루션은 아닙니다. 특정 문제에 따라 다른 모델이나 추가적인 알고리즘을 활용하는 것이 더 나은 선택일 수 있습니다.</p>
<h3 id="yolov8_4">YOLOv8 요구 자원</h3>
<p><strong>YOLOv8</strong>을 사용하기 위한 요구 자원은 모델의 크기 및 응용 환경에 따라 다르지만, 기본적으로 다음과 같은 자원이 필요합니다.</p>
<h4 id="1">1. <strong>하드웨어 요구 사항</strong></h4>
<p>YOLOv8은 경량화된 구조를 가지고 있어 고성능 GPU뿐만 아니라 CPU에서도 실행할 수 있지만, 최적의 성능을 발휘하려면 GPU가 필요합니다.</p>
<ul>
<li><strong>GPU</strong>: </li>
<li>YOLOv8은 GPU 가속을 통해 실시간 객체 탐지 작업에서 최고의 성능을 발휘합니다. NVIDIA의 CUDA와 호환되는 GPU가 권장되며, <strong>NVIDIA GTX 1080 Ti</strong> 또는 그 이상을 사용하는 것이 좋습니다.</li>
<li>
<p>최신 <strong>NVIDIA RTX 시리즈 (RTX 3060, RTX 3080)</strong> 이상의 GPU를 사용하면 더 빠른 처리 속도와 더 높은 정확도를 얻을 수 있습니다.</p>
</li>
<li>
<p><strong>CPU</strong>: </p>
</li>
<li>
<p>CPU에서도 YOLOv8을 실행할 수 있으나, 실시간 성능이 요구되지 않는 상황에서 사용하는 것이 바람직합니다. <strong>Intel i5, i7, i9</strong>과 같은 고성능 CPU에서 빠른 추론 속도를 제공할 수 있습니다.</p>
</li>
<li>
<p><strong>RAM</strong>:</p>
</li>
<li>
<p>학습 및 추론을 위한 메모리 요구량은 입력 이미지의 크기와 모델의 크기에 따라 달라집니다. 일반적으로 <strong>8GB 이상의 RAM</strong>이 권장되며, 대규모 데이터셋을 처리할 경우 <strong>16GB 이상의 RAM</strong>이 더 안정적입니다.</p>
</li>
<li>
<p><strong>스토리지</strong>:</p>
</li>
<li>모델 학습을 위한 데이터셋 및 모델 파일의 크기를 감안할 때, 충분한 저장 공간이 필요합니다. 특히 대규모 데이터를 처리하는 경우 <strong>SSD</strong>와 같은 빠른 저장 장치를 사용하는 것이 좋습니다. YOLOv8의 가중치 파일은 수백 MB에서 기가바이트에 이를 수 있습니다.</li>
</ul>
<h4 id="2">2. <strong>소프트웨어 요구 사항</strong></h4>
<p>YOLOv8을 실행하기 위한 기본 소프트웨어 환경은 다음과 같습니다:</p>
<ul>
<li><strong>Python 3.8 이상</strong>:</li>
<li>
<p>YOLOv8은 Python 기반의 프레임워크로 제공되므로, Python 3.8 이상의 버전이 필요합니다.</p>
</li>
<li>
<p><strong>PyTorch</strong>:</p>
</li>
<li>
<p>YOLOv8은 주로 <strong>PyTorch</strong>에서 실행되므로, PyTorch의 최신 버전이 필요합니다. GPU 가속을 위한 <strong>CUDA</strong> 버전도 PyTorch와 호환되어야 합니다.</p>
</li>
<li>
<p><strong>CUDA 및 cuDNN</strong>:</p>
</li>
<li>GPU 가속을 사용하려면 <strong>CUDA Toolkit</strong>(최소 10.2 버전 이상)과 <strong>cuDNN</strong> 라이브러리가 설치되어야 합니다.</li>
<li>
<p>CUDA 버전은 사용 중인 GPU와 PyTorch 버전에 맞춰 설치해야 합니다.</p>
</li>
<li>
<p><strong>라이브러리</strong>:</p>
</li>
<li>YOLOv8을 사용하기 위해 다음과 같은 필수 라이브러리들이 필요합니다:<ul>
<li>numpy</li>
<li>OpenCV</li>
<li>torchvision</li>
<li>Pillow (이미지 처리용)</li>
<li>기타 추가적인 시각화 및 데이터 처리 라이브러리</li>
</ul>
</li>
</ul>
<h4 id="3-yolov8">3. <strong>YOLOv8 모델 크기</strong></h4>
<p>YOLOv8은 경량화된 모델부터 큰 모델까지 다양한 크기의 모델이 제공되므로, 사용 환경에 따라 적합한 모델을 선택할 수 있습니다.</p>
<ul>
<li><strong>YOLOv8n (Nano)</strong>: 가장 작은 크기의 모델로, 아주 적은 연산량으로 빠른 추론이 가능하지만, 정확도는 상대적으로 낮습니다. 모바일 장치나 임베디드 시스템에서 사용하기 적합합니다.</li>
<li><strong>YOLOv8s (Small)</strong>: 약간의 성능과 정확도를 희생하면서도 매우 빠른 속도를 제공하며, 실시간 객체 탐지에 적합합니다.</li>
<li><strong>YOLOv8m (Medium)</strong>: 중간 크기의 모델로, 정확도와 속도 간의 균형을 맞춘 모델입니다. </li>
<li><strong>YOLOv8l (Large)</strong>: 더 높은 정확도를 제공하지만 연산량이 더 필요합니다. 더 강력한 GPU가 요구되며, 리소스가 풍부한 환경에서 적합합니다.</li>
<li><strong>YOLOv8x (Extra Large)</strong>: 최고의 성능과 정확도를 제공하는 모델로, 매우 높은 연산 자원이 필요하며, 대규모 서버 환경에서 주로 사용됩니다.</li>
</ul>
<h4 id="4">4. <strong>실행 예시</strong>:</h4>
<p>다음은 YOLOv8을 실행하기 위한 기본적인 예시입니다.</p>
<pre><code class="language-bash"># PyTorch 및 필요한 라이브러리 설치
pip install torch torchvision torchaudio
pip install ultralytics

# YOLOv8 모델 불러오기 및 추론 실행
import torch
model = torch.hub.load('ultralytics/yolov8', 'yolov8n')  # YOLOv8 Nano 모델 불러오기
img = 'path/to/image.jpg'  # 이미지 경로
results = model(img)  # 추론 실행
results.show()  # 결과 시각화
</code></pre>
<p><strong>요약</strong>:<br/>
YOLOv8은 다양한 크기의 모델을 제공하며, GPU에서의 성능 최적화를 위해 CUDA 및 cuDNN 환경이 필요합니다. 메모리와 스토리지 요구량은 다루는 데이터의 크기에 따라 달라지며, 경량 모델은 모바일이나 임베디드 시스템에서도 실행할 수 있습니다.</p>
<h3 id="yolov8-pytorch">YOLOv8 객체 탐지 PyTorch 예제 코드</h3>
<p>다음은 <strong>YOLOv8</strong>을 사용하여 객체 탐지를 수행하는 PyTorch 예제 코드입니다. 이 코드는 <code>ultralytics</code> 라이브러리를 활용하여 YOLOv8 모델을 불러오고, 이미지에서 객체를 탐지하는 기본적인 방법을 보여줍니다.</p>
<h4 id="yolov8-pytorch_1">YOLOv8 객체 탐지 PyTorch 예제</h4>
<ol>
<li><strong>라이브러리 설치</strong><br/>
   먼저 YOLOv8 모델을 사용하려면 <code>ultralytics</code> 패키지를 설치해야 합니다.</li>
</ol>
<p><code>bash
   pip install ultralytics</code></p>
<ol>
<li><strong>YOLOv8 모델로 객체 탐지 수행하기</strong></li>
</ol>
<p>다음은 PyTorch에서 YOLOv8을 불러와 이미지를 입력으로 객체를 탐지하는 예제입니다.</p>
<p>```python
   # 필요한 라이브러리 불러오기
   import torch
   from matplotlib import pyplot as plt
   from PIL import Image</p>
<p># YOLOv8 모델 로드
   model = torch.hub.load('ultralytics/yolov8', 'yolov8s')  # 'yolov8s', 'yolov8m', 'yolov8l' 등으로 변경 가능</p>
<p># 테스트할 이미지 경로 또는 URL
   img_url = 'https://ultralytics.com/images/zidane.jpg'</p>
<p># 이미지로부터 객체 탐지 수행
   results = model(img_url)</p>
<p># 결과 출력 (이미지에 탐지된 객체 표시)
   results.show()</p>
<p># 결과를 텍스트로 출력
   results.print()</p>
<p># 탐지된 결과를 DataFrame 형식으로 얻기 (탐지된 객체 정보)
   df = results.pandas().xyxy[0]  # bounding box 좌표 (xmin, ymin, xmax, ymax) 및 기타 정보
   print(df)
   ```</p>
<ol>
<li><strong>모델 로드</strong>:</li>
<li>
<p><code>torch.hub.load</code>를 사용해 YOLOv8 모델을 로드합니다. <code>yolov8s</code>는 작은 모델을 의미하며, 이 외에도 <code>yolov8m</code>, <code>yolov8l</code> 등 더 큰 모델을 사용할 수 있습니다.</p>
</li>
<li>
<p><strong>이미지 입력</strong>:</p>
</li>
<li>
<p><code>img_url</code>을 통해 URL로부터 이미지를 가져오거나, 로컬 파일 경로를 사용할 수 있습니다.</p>
</li>
<li>
<p><strong>탐지 결과 표시</strong>:</p>
</li>
<li>
<p><code>results.show()</code>를 사용하여 이미지에 탐지된 객체를 시각적으로 표시합니다. 탐지된 객체의 경계 상자와 클래스가 표시됩니다.</p>
</li>
<li>
<p><strong>탐지 결과 텍스트 출력</strong>:</p>
</li>
<li>
<p><code>results.print()</code>로 탐지된 객체의 정보를 텍스트로 출력할 수 있습니다.</p>
</li>
<li>
<p><strong>탐지 결과 DataFrame</strong>:</p>
</li>
<li><code>results.pandas().xyxy[0]</code>를 통해 탐지된 객체의 정보를 <code>DataFrame</code> 형식으로 추출할 수 있습니다. 여기에는 경계 상자 좌표(xmin, ymin, xmax, ymax), 클래스 정보, 신뢰도 점수가 포함됩니다.</li>
</ol>
<h4 id="yolov8_5">YOLOv8 모델 옵션</h4>
<ul>
<li><code>yolov8s</code>: 작은 사이즈 모델 (빠르고 경량화됨)</li>
<li><code>yolov8m</code>: 중간 사이즈 모델</li>
<li><code>yolov8l</code>: 큰 사이즈 모델 (더 정확하지만 무거움)</li>
</ul>
<p>위 예제는 YOLOv8을 사용하는 가장 기본적인 객체 탐지 방법을 설명하며, 실제 프로젝트에서는 추가적인 후처리나 데이터 분석을 통해 더 세부적인 결과를 도출할 수 있습니다.</p>
<h3 id="yolov8-pytorch_2">YOLOv8 학습 PyTorch 예제</h3>
<p>YOLOv8 객체 탐지를 위한 PyTorch 학습 예제는 모델 설정, 데이터 준비, 학습 및 추론으로 구성됩니다. YOLOv8은 공식적으로 Ultralytics에서 제공하는 라이브러리로, PyTorch를 기반으로 쉽게 학습을 실행할 수 있습니다. 아래는 YOLOv8을 이용해 객체 탐지를 학습하는 기본 코드 예제입니다.</p>
<h4 id="1-yolov8">1. YOLOv8 설치</h4>
<p>YOLOv8은 <code>ultralytics</code> 라이브러리를 통해 사용할 수 있습니다. 먼저 PyTorch와 함께 YOLOv8 라이브러리를 설치합니다.</p>
<pre><code class="language-bash">pip install ultralytics
</code></pre>
<h4 id="2_1">2. 데이터 준비</h4>
<p>객체 탐지를 위해 필요한 데이터셋은 <strong>COCO</strong> 형식 또는 <strong>YOLO</strong> 형식으로 준비해야 합니다. 데이터셋은 이미지 파일과 함께 라벨 파일(.txt)이 필요하며, 라벨 파일에는 객체 클래스, 좌표(x, y, w, h)가 포함되어 있습니다.</p>
<h4 id="3">3. 모델 학습 코드</h4>
<p>아래 코드는 PyTorch에서 YOLOv8을 사용하여 객체 탐지를 학습시키는 기본 예제입니다.</p>
<pre><code class="language-python">from ultralytics import YOLO

# 1. YOLOv8 모델 로드 (pre-trained 모델을 사용할 수도 있습니다)
model = YOLO('yolov8s.pt')  # 'yolov8s.pt'는 작은 YOLOv8 모델의 사전 학습된 가중치 파일입니다.

# 2. 데이터셋 경로 설정
data = 'coco128.yaml'  # 또는 사용자 정의 데이터셋 경로 (예: 'dataset.yaml')

# 3. 모델 학습
model.train(
    data=data,            # 학습에 사용할 데이터셋 설정
    epochs=100,           # 학습할 에포크 수
    imgsz=640,            # 입력 이미지 크기
    batch=16,             # 배치 크기
    device=0,             # GPU 사용 (CPU는 'cpu')
    workers=4             # 데이터 로딩을 위한 워커 수
)

# 4. 학습 후 모델 검증 (optional)
metrics = model.val()  # 학습된 모델의 성능을 검증

# 5. 객체 탐지 추론 실행
results = model('path_to_image_or_video')  # 이미지나 비디오에 대해 객체 탐지 실행
results.show()  # 결과 시각화
</code></pre>
<h4 id="4_1">4. 데이터셋 구성</h4>
<p>사용자 정의 데이터셋을 사용할 때는 <code>data.yaml</code> 파일을 작성하여 경로를 정의해야 합니다.</p>
<pre><code class="language-yaml"># dataset.yaml 파일의 예시
train: ./data/train/images  # 학습 이미지 경로
val: ./data/val/images      # 검증 이미지 경로

# 클래스 정의 (예시: 1개의 클래스 - 포트홀)
names:
  0: pothole
</code></pre>
<p>각 이미지에 대해 동일한 이름의 <code>.txt</code> 파일이 있어야 하며, 그 안에는 아래와 같은 형식으로 객체의 클래스와 좌표가 포함되어야 합니다.</p>
<pre><code class="language-plaintext">0 0.5 0.5 0.2 0.2  # 클래스 0 (포트홀), 중심 좌표 (x=0.5, y=0.5), 너비 0.2, 높이 0.2
</code></pre>
<h4 id="5">5. 학습 파라미터</h4>
<ul>
<li><strong>epochs</strong>: 학습 에포크 수입니다. 일반적으로 큰 값으로 설정할수록 더 오래 학습하지만, 오버피팅을 주의해야 합니다.</li>
<li><strong>imgsz</strong>: 입력 이미지 크기를 조절하는 옵션입니다. 성능과 속도 사이에서 적절한 균형을 맞출 수 있습니다.</li>
<li><strong>batch</strong>: 배치 크기로, 한 번에 학습에 사용할 이미지 수입니다.</li>
<li><strong>device</strong>: 학습에 사용할 장치 설정 (GPU나 CPU).</li>
<li><strong>workers</strong>: 데이터 로딩을 위한 프로세스 수입니다.</li>
</ul>
<h4 id="6">6. 추론 결과 시각화</h4>
<p>학습된 모델을 사용해 테스트 이미지나 비디오에서 탐지를 수행한 결과를 <code>results.show()</code>를 통해 시각화할 수 있습니다. 결과는 탐지된 객체의 클래스와 바운딩 박스로 표시됩니다.</p>
<p>이 코드는 기본적인 학습 파이프라인을 보여주며, 더 나아가 모델 하이퍼파라미터 조정, 데이터 증강, 모델의 크기 변경 등 다양한 실험을 통해 성능을 최적화할 수 있습니다.</p>
<br/>
<br/>
</div>
<footer class="container-fluid wm-page-content">
<p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>
<script type="module">import mermaid from "https://unpkg.com/mermaid@10.4.0/dist/mermaid.esm.min.mjs";
mermaid.initialize({});</script></body>
</html>