<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/chatgpt_api/chapter_04/0403/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>다양한 파라미터 조합을 통한 결과 최적화 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "1. Temperature", url: "#_top", children: [
          ]},
          {title: "2. Top-p (Nucleus Sampling)", url: "#2-top-p-nucleus-sampling", children: [
          ]},
          {title: "3. Max Tokens", url: "#3-max-tokens", children: [
          ]},
          {title: "4. Frequency Penalty", url: "#4-frequency-penalty", children: [
          ]},
          {title: "5. Presence Penalty", url: "#5-presence-penalty", children: [
          ]},
          {title: "6. Stop Sequences", url: "#6-stop-sequences", children: [
          ]},
          {title: "7. Best Of", url: "#7-best-of", children: [
          ]},
          {title: "8. Logit Bias", url: "#8-logit-bias", children: [
          ]},
          {title: "9. Prompt Length and Context", url: "#9-prompt-length-and-context", children: [
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../0404/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../0404/" class="btn btn-xs btn-link">
        복잡한 프롬프트 작성 전략
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0402/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0402/" class="btn btn-xs btn-link">
        대화형 요청 및 상태 관리
      </a>
    </div>
    
  </div>

    

    <p>ChatGPT API의 강력함은 다양한 파라미터를 통해 출력 결과를 조정하고 최적화할 수 있다는 점에 있다. 이 섹션에서는 이러한 파라미터들이 어떻게 작동하는지, 그리고 특정 요구사항에 맞는 최적의 결과를 얻기 위해 어떻게 조합할 수 있는지에 대해 자세히 설명한다.</p>
<h3 id="1-temperature">1. Temperature</h3>
<p><strong>Temperature</strong> 파라미터는 출력의 무작위성을 조정하는 데 사용된다. 값이 낮을수록 모델의 응답이 더 결정론적이 되며, 높은 값은 응답을 더 다양하고 창의적으로 만든다.</p>
<ul>
<li><strong>Formula</strong>:  </li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
  P(x) = \frac{\exp\left(\frac{z(x)}{T}\right)}{\sum_{x'} \exp\left(\frac{z(x')}{T}\right)}
</div>
<script type="math/tex; mode=display">
  P(x) = \frac{\exp\left(\frac{z(x)}{T}\right)}{\sum_{x'} \exp\left(\frac{z(x')}{T}\right)}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">P(x)</span><script type="math/tex">P(x)</script></span>는 다음 단어로 선택될 확률을 의미하며, <span class="arithmatex"><span class="MathJax_Preview">z(x)</span><script type="math/tex">z(x)</script></span>는 단어 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>의 로짓(logit) 값이다. <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>는 temperature 파라미터이다.</p>
<ul>
<li><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Tell me a joke."}],
      temperature=0.7
  )</code></li>
<li><strong>Low Temperature</strong> (e.g., 0.2): 모델은 더 예측 가능한 답변을 생성한다.</li>
<li><strong>High Temperature</strong> (e.g., 1.0): 모델은 더 창의적이고 예상치 못한 답변을 생성할 수 있다.</li>
</ul>
<h3 id="2-top-p-nucleus-sampling">2. Top-p (Nucleus Sampling)</h3>
<p><strong>Top-p</strong>는 Nucleus Sampling이라고도 하며, 이 파라미터는 누적 확률이 <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 이하가 될 때까지 높은 확률의 단어들을 샘플링한다. 이 방법은 temperature 파라미터와 함께 사용되어 모델의 출력 분포를 조정할 수 있다.</p>
<ul>
<li>
<p><strong>Concept</strong>:<br />
  전체 확률 분포에서 상위 <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 퍼센트의 단어들만 고려한다. <span class="arithmatex"><span class="MathJax_Preview">p = 0.9</span><script type="math/tex">p = 0.9</script></span>라면, 상위 90%에 속하는 단어들만 다음 단어로 선택될 가능성이 있다.</p>
</li>
<li>
<p><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Give me a random fact."}],
      top_p=0.8
  )</code></p>
</li>
<li><strong>Low Top-p</strong> (e.g., 0.5): 응답이 매우 제한적이지만, 특정한 결과를 원할 때 유용하다.</li>
<li><strong>High Top-p</strong> (e.g., 0.9): 더 다양한 응답을 생성한다.</li>
</ul>
<h3 id="3-max-tokens">3. Max Tokens</h3>
<p><strong>Max Tokens</strong> 파라미터는 생성할 텍스트의 최대 길이를 정의한다. 이 파라미터를 통해 응답의 길이를 제어할 수 있다.</p>
<ul>
<li>
<p><strong>Considerations</strong>:<br />
  생성되는 응답이 너무 길어지지 않도록 하거나, 특정 길이 내에서 응답을 제한하고자 할 때 유용하다.</p>
</li>
<li>
<p><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Summarize the article."}],
      max_tokens=50
  )</code></p>
</li>
<li><strong>Low Max Tokens</strong> (e.g., 50): 짧고 간결한 응답을 유도한다.</li>
<li><strong>High Max Tokens</strong> (e.g., 300): 더 긴 응답이 필요할 때 사용한다.</li>
</ul>
<h3 id="4-frequency-penalty">4. Frequency Penalty</h3>
<p><strong>Frequency Penalty</strong>는 동일한 단어의 반복을 줄이기 위해 사용된다. 이 파라미터를 높게 설정하면, 모델은 동일한 단어를 반복해서 사용하는 것을 피하려고 한다.</p>
<ul>
<li><strong>Formula</strong>:<br />
  반복된 단어의 확률을 감소시키는 데 사용된다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
  P(x) \propto \frac{1}{1 + \text{count}(x) \cdot \lambda}
</div>
<script type="math/tex; mode=display">
  P(x) \propto \frac{1}{1 + \text{count}(x) \cdot \lambda}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\text{count}(x)</span><script type="math/tex">\text{count}(x)</script></span>는 단어 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>의 등장 횟수이며, <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>는 frequency penalty 파라미터이다.</p>
<ul>
<li><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Describe the sunset."}],
      frequency_penalty=0.5
  )</code></li>
<li><strong>Low Frequency Penalty</strong> (e.g., 0.0): 단어의 반복이 자유로울 수 있다.</li>
<li><strong>High Frequency Penalty</strong> (e.g., 2.0): 모델이 더 다양한 단어를 사용하도록 유도한다.</li>
</ul>
<h3 id="5-presence-penalty">5. Presence Penalty</h3>
<p><strong>Presence Penalty</strong>는 특정 단어가 얼마나 자주 등장했는지에 관계없이 단어가 생성될 확률을 조정한다. 이 파라미터는 새로운 주제를 도입하거나, 모델이 이전에 사용하지 않은 단어를 사용하도록 유도하는 데 유용하다.</p>
<ul>
<li><strong>Formula</strong>:<br />
  단어의 등장 여부에 따라 그 확률을 증가시키거나 감소시킨다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
  P(x) \propto \left(1 + \lambda \cdot \mathbf{1}(x \text{ is present})\right)
</div>
<script type="math/tex; mode=display">
  P(x) \propto \left(1 + \lambda \cdot \mathbf{1}(x \text{ is present})\right)
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{1}(x \text{ is present})</span><script type="math/tex">\mathbf{1}(x \text{ is present})</script></span>는 단어 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>가 이미 텍스트에 존재하는지 여부를 나타내는 지시 함수이며, <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>는 presence penalty 파라미터이다.</p>
<ul>
<li><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Start a new story."}],
      presence_penalty=0.6
  )</code></li>
<li><strong>Low Presence Penalty</strong> (e.g., 0.0): 모델이 반복적인 주제를 계속해서 사용할 수 있다.</li>
<li><strong>High Presence Penalty</strong> (e.g., 1.0): 모델이 새로운 주제나 단어를 사용하도록 유도한다.</li>
</ul>
<h3 id="6-stop-sequences">6. Stop Sequences</h3>
<p><strong>Stop Sequences</strong>는 생성된 텍스트를 중지시키기 위한 특정 텍스트 패턴이다. 이 파라미터를 설정하면, 모델은 지정된 패턴이 나타날 때까지 텍스트를 생성하며, 패턴이 발견되면 그 즉시 응답을 종료한다.</p>
<ul>
<li>
<p><strong>Concept</strong>:<br />
  Stop Sequences는 특정 구문이나 문장이 출력되는 즉시 응답 생성을 중지할 수 있도록 한다. 이는 문맥상 필요 없는 정보를 제거하거나, 특정 형식의 응답을 유지할 때 유용하다.</p>
</li>
<li>
<p><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "List three animals."}],
      stop=["\n"]
  )</code></p>
</li>
<li><strong>Single Stop Sequence</strong>: 예를 들어, 줄바꿈이 발생할 때 응답을 중지하도록 설정할 수 있다.</li>
<li><strong>Multiple Stop Sequences</strong>: 여러 패턴을 설정하여 응답을 보다 세밀하게 제어할 수 있다.</li>
</ul>
<h3 id="7-best-of">7. Best Of</h3>
<p><strong>Best Of</strong> 파라미터는 모델이 여러 개의 응답을 생성한 다음, 그 중에서 가장 적합한 응답을 선택하여 반환하도록 한다. 이 파라미터를 사용하면 더 높은 품질의 응답을 얻을 수 있다.</p>
<ul>
<li>
<p><strong>Concept</strong>:<br />
  모델이 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>개의 응답을 생성한 후, 그 중 가장 높은 점수를 받은 응답을 선택하여 반환한다. 이는 무작위성을 줄이고, 더 예측 가능한 결과를 얻는 데 유용하다.</p>
</li>
<li>
<p><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "Explain quantum mechanics."}],
      best_of=3
  )</code></p>
</li>
<li><strong>Best Of = 1</strong>: 단일 응답을 생성하여 반환한다. 이는 기본값이다.</li>
<li><strong>Best Of &gt; 1</strong>: 여러 응답 중 가장 좋은 결과를 선택한다.</li>
</ul>
<h3 id="8-logit-bias">8. Logit Bias</h3>
<p><strong>Logit Bias</strong>는 특정 토큰의 선택 확률을 조정하는 데 사용된다. 이 파라미터를 통해 특정 단어가 더 자주 선택되거나 덜 자주 선택되도록 할 수 있다.</p>
<ul>
<li>
<p><strong>Concept</strong>:<br />
  각 단어의 logit 값을 직접 조정하여, 해당 단어가 선택될 확률을 증가시키거나 감소시킬 수 있다. 예를 들어, 특정 단어가 반드시 포함되도록 하거나, 특정 단어의 사용을 피하도록 할 수 있다.</p>
</li>
<li>
<p><strong>Formula</strong>:</p>
</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
  P(x) = \frac{\exp(z(x) + \text{logit\_bias}(x))}{\sum_{x'} \exp(z(x') + \text{logit\_bias}(x'))}
</div>
<script type="math/tex; mode=display">
  P(x) = \frac{\exp(z(x) + \text{logit\_bias}(x))}{\sum_{x'} \exp(z(x') + \text{logit\_bias}(x'))}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\text{logit\_bias}(x)</span><script type="math/tex">\text{logit\_bias}(x)</script></span>는 특정 토큰 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>에 적용된 로그 확률 편향이다.</p>
<ul>
<li><strong>Example</strong>:
  <code>python
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": "What is the capital of France?"}],
      logit_bias={"Paris": 10}
  )</code></li>
<li><strong>Positive Bias</strong>: 특정 단어가 더 자주 선택되도록 설정할 수 있다.</li>
<li><strong>Negative Bias</strong>: 특정 단어가 선택될 가능성을 낮추거나 거의 선택되지 않도록 할 수 있다.</li>
</ul>
<h3 id="9-prompt-length-and-context">9. Prompt Length and Context</h3>
<p><strong>Prompt Length</strong>는 모델에 제공되는 입력 텍스트의 길이를 의미하며, 응답의 품질과 관련이 있다. 특히 긴 프롬프트는 모델이 문맥을 더 잘 이해하고, 더욱 일관된 응답을 생성하는 데 도움이 된다.</p>
<ul>
<li>
<p><strong>Considerations</strong>:<br />
  너무 긴 프롬프트는 비용을 증가시키고, 너무 짧은 프롬프트는 문맥을 제대로 제공하지 못해 응답 품질을 떨어뜨릴 수 있다. 적절한 길이의 프롬프트를 유지하는 것이 중요하다.</p>
</li>
<li>
<p><strong>Example</strong>:
  <code>python
  long_prompt = "In a galaxy far, far away, there was a small planet inhabited by beings with extraordinary abilities..."
  response = openai.ChatCompletion.create(
      model="gpt-4",
      messages=[{"role": "user", "content": long_prompt}],
      max_tokens=100
  )</code></p>
</li>
<li><strong>Short Prompt</strong>: 응답이 예상치 못한 방향으로 갈 수 있다.</li>
<li><strong>Long Prompt</strong>: 응답의 일관성과 품질이 높아질 수 있다.</li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../0404/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../0404/" class="btn btn-xs btn-link">
        복잡한 프롬프트 작성 전략
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0402/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0402/" class="btn btn-xs btn-link">
        대화형 요청 및 상태 관리
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>