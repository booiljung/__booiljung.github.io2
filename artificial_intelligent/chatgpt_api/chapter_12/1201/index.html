<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/chatgpt_api/chapter_12/1201/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>OpenAI API 공식 문서 해설 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "OpenAI API\uc758 \uc8fc\uc694 \uac1c\ub150", url: "#_top", children: [
              {title: "API \uae30\ubcf8 \uac1c\uc694", url: "#api" },
              {title: "\ubaa8\ub378 \uc120\ud0dd", url: "#_1" },
          ]},
          {title: "\uc694\uccad \ubc0f \uc751\ub2f5 \ud615\uc2dd", url: "#_2", children: [
              {title: "\uc694\uccad \ud615\uc2dd", url: "#_3" },
              {title: "\uc751\ub2f5 \ud615\uc2dd", url: "#_4" },
              {title: "\ud1a0\ud070 \uc774\ud574", url: "#_5" },
          ]},
          {title: "\ubaa8\ub378\uc758 \uc138\ubd80 \ub3d9\uc791 \uc6d0\ub9ac", url: "#_6", children: [
              {title: "\uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998", url: "#_7" },
              {title: "GPT\uc758 \uc791\ub3d9 \uacfc\uc815", url: "#gpt" },
          ]},
          {title: "\ud504\ub86c\ud504\ud2b8 \uc5d4\uc9c0\ub2c8\uc5b4\ub9c1\uc758 \uc911\uc694\uc131", url: "#_8", children: [
              {title: "\ud6a8\uacfc\uc801\uc778 \ud504\ub86c\ud504\ud2b8 \uc791\uc131 \uc804\ub7b5", url: "#_9" },
              {title: "\uc628\ub3c4\uc640 \uc0d8\ud50c\ub9c1 \uc804\ub7b5", url: "#_10" },
          ]},
          {title: "API \ud638\ucd9c \uc608\uc81c \ubd84\uc11d", url: "#api_1", children: [
              {title: "\uc8fc\uc694 \ud30c\ub77c\ubbf8\ud130 \ud574\uc124", url: "#_11" },
          ]},
          {title: "\uc751\ub2f5 \ub370\uc774\ud130 \ucc98\ub9ac \ubc0f \ud30c\uc2f1", url: "#_12", children: [
          ]},
          {title: "\uc694\uae08 \ubc0f \ud1a0\ud070 \uad00\ub9ac", url: "#_13", children: [
              {title: "\ud1a0\ud070 \uc0ac\uc6a9 \uacc4\uc0b0 \ubc29\ubc95", url: "#_14" },
          ]},
          {title: "\uc694\uae08 \ucd5c\uc801\ud654 \uc804\ub7b5", url: "#_15", children: [
          ]},
          {title: "\uc18d\ub3c4 \ubc0f \uc131\ub2a5 \ucd5c\uc801\ud654", url: "#_16", children: [
              {title: "\ubcd1\ub82c \ucc98\ub9ac", url: "#_17" },
              {title: "\uc694\uccad \ucd5c\uc801\ud654", url: "#_18" },
          ]},
          {title: "Rate Limit \uad00\ub9ac", url: "#rate-limit", children: [
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../1202/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../1202/" class="btn btn-xs btn-link">
        주요 코드 스니펫 모음
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../chapter_11/1103/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../chapter_11/1103/" class="btn btn-xs btn-link">
        커뮤니티 및 지원 리소스 소개
      </a>
    </div>
    
  </div>

    

    <p>OpenAI API는 자연어 처리 모델을 Python 환경에서 손쉽게 활용할 수 있도록 제공하는 RESTful API이다. 이를 사용하면 텍스트 생성, 요약, 번역, 질의응답과 같은 다양한 자연어 처리 작업을 수행할 수 있다. 이 부록에서는 OpenAI API의 공식 문서를 해설하며, 주요 개념과 기능을 상세하게 설명한다.</p>
<h3 id="openai-api">OpenAI API의 주요 개념</h3>
<h4 id="api">API 기본 개요</h4>
<p>OpenAI API는 크게 모델, 엔드포인트, 요청, 응답으로 구성된다. 이를 기반으로 다양한 작업을 수행할 수 있으며, 각 요소가 어떤 역할을 하는지 이해하는 것이 중요하다.</p>
<ul>
<li><strong>모델</strong>: OpenAI에서 제공하는 다양한 모델은 각각의 목적과 성능에 따라 구분된다. 예를 들어, GPT-4, GPT-3.5와 같은 모델들이 있으며, 용도에 따라 적절한 모델을 선택할 수 있다.</li>
<li><strong>엔드포인트</strong>: API 요청을 처리하는 서버의 주소이다. 각 엔드포인트는 고유한 URL로 제공되며, 이를 통해 API 요청이 전송된다.</li>
<li><strong>요청(Request)</strong>: 클라이언트가 API에 보낼 데이터이다. 보통은 텍스트, 파라미터, 설정 값이 포함된다.</li>
<li><strong>응답(Response)</strong>: API가 클라이언트로 반환하는 결과 데이터이다. 이는 일반적으로 텍스트 형태로 제공되며, 추가적인 메타데이터가 포함될 수 있다.</li>
</ul>
<h4 id="_1">모델 선택</h4>
<p>OpenAI API는 다양한 모델을 제공한다. 각 모델은 특정한 작업에서 더 나은 성능을 발휘하므로, 모델의 특성을 이해하고 적절히 선택하는 것이 중요하다.</p>
<ul>
<li><strong>GPT-4</strong>: 가장 강력한 성능을 제공하며, 복잡한 텍스트 생성, 고급 대화, 창의적인 작업에서 우수한 성능을 보이다.</li>
<li><strong>GPT-3.5</strong>: 빠르고 경제적인 모델로, 대규모 요청을 효율적으로 처리할 수 있다.</li>
</ul>
<h3 id="_2">요청 및 응답 형식</h3>
<p>OpenAI API에 요청을 보내고 응답을 받는 기본적인 구조를 살펴보겠다. 모든 요청은 HTTP POST 방식으로 전송되며, <code>Content-Type</code>은 <code>application/json</code>으로 설정된다.</p>
<h4 id="_3">요청 형식</h4>
<p>기본적인 API 요청은 다음과 같은 요소로 구성된다:</p>
<ol>
<li><strong>모델 선택</strong>: 사용할 모델의 이름을 지정한다.</li>
<li><strong>프롬프트</strong>: 모델에 제공할 텍스트이다. 예를 들어, <code>text-davinci-003</code> 모델에 텍스트 생성을 요청할 때 사용한다.</li>
<li><strong>최대 토큰 수</strong>: 생성할 텍스트의 최대 길이를 토큰 단위로 설정한다.</li>
<li><strong>온도(temperature)</strong>: 생성된 텍스트의 창의성이나 무작위성을 조절한다. 0에서 1 사이의 값을 가지며, 값이 클수록 결과가 다양해진다.</li>
<li><strong>탑 P(top_p)</strong>: 샘플링 방식으로, 확률이 높은 상위 P 퍼센트의 결과를 선택하는 방식이다.</li>
<li><strong>n</strong>: 몇 개의 응답을 받을지를 설정한다.</li>
<li><strong>stop</strong>: 응답 생성을 중단할 조건을 지정한다. 예를 들어, 특정 단어나 문자가 나오면 중단하게 할 수 있다.</li>
</ol>
<p>다음은 API 요청의 예시이다:</p>
<pre><code class="language-python">import openai

response = openai.Completion.create(
  model=&quot;text-davinci-003&quot;,
  prompt=&quot;Explain the theory of relativity.&quot;,
  max_tokens=150,
  temperature=0.7,
  top_p=1.0,
  n=1,
  stop=None
)
</code></pre>
<h4 id="_4">응답 형식</h4>
<p>응답은 JSON 형식으로 반환되며, 주요 필드는 다음과 같다:</p>
<ol>
<li><strong>id</strong>: 요청의 고유 식별자.</li>
<li><strong>object</strong>: 응답 객체의 타입. 예를 들어, <code>text_completion</code> 같은 값을 갖는다.</li>
<li><strong>created</strong>: 응답이 생성된 타임스탬프.</li>
<li><strong>model</strong>: 사용된 모델의 이름.</li>
<li><strong>choices</strong>: 생성된 텍스트 응답의 목록이다. <code>n</code> 값을 통해 여러 개의 응답을 받을 수 있으며, 각 응답은 다음과 같은 필드를 갖는다.<ul>
<li><strong>text</strong>: 생성된 텍스트.</li>
<li><strong>index</strong>: 응답의 순서.</li>
<li><strong>logprobs</strong>: 각 토큰에 대한 로그 확률 값. 생략될 수 있다.</li>
<li><strong>finish_reason</strong>: 응답이 종료된 이유. <code>stop</code> 값이 설정된 경우 중단된 이유를 나타낸다.</li>
</ul>
</li>
<li><strong>usage</strong>: 사용된 토큰의 양을 나타내며, <code>prompt_tokens</code>, <code>completion_tokens</code>, <code>total_tokens</code>로 세분화된다.</li>
</ol>
<p>응답 예시는 다음과 같다:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;cmpl-5es55pVBkjgjgjgXcybfu&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1609459200,
  &quot;model&quot;: &quot;text-davinci-003&quot;,
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;The theory of relativity...&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 5,
    &quot;completion_tokens&quot;: 150,
    &quot;total_tokens&quot;: 155
  }
}
</code></pre>
<h4 id="_5">토큰 이해</h4>
<p>토큰은 OpenAI 모델에서 텍스트를 처리하는 기본 단위이다. 토큰은 단어의 부분으로, 영어에서 한 단어는 보통 1~2개의 토큰으로 분할된다. 예를 들어, 단어 "ChatGPT"는 하나의 토큰이지만, 문장 "ChatGPT is amazing!"은 총 4개의 토큰으로 처리된다.</p>
<p>API 사용 시 요청과 응답의 토큰 수는 매우 중요한 요소이다. 사용된 토큰의 양에 따라 요금이 청구되기 때문에, 이를 최적화하는 것이 필요하다.</p>
<p>토큰의 수식으로는 다음과 같은 관계식을 정의할 수 있다:</p>
<p>$$</p>
<p>\text{total_tokens} = \text{prompt_tokens} + \text{completion_tokens}</p>
<p>$$</p>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">\text{prompt\_tokens}</span><script type="math/tex">\text{prompt\_tokens}</script></span>: 프롬프트에 사용된 토큰 수
- <span class="arithmatex"><span class="MathJax_Preview">\text{completion\_tokens}</span><script type="math/tex">\text{completion\_tokens}</script></span>: 모델이 생성한 응답에서 사용된 토큰 수</p>
<p>이를 통해 API 요청에 대한 전체 토큰 사용량을 관리할 수 있다.</p>
<h3 id="_6">모델의 세부 동작 원리</h3>
<p>OpenAI API에서 사용되는 GPT 모델은 <strong>Transformer</strong>라는 딥러닝 구조를 기반으로 작동한다. 이 구조는 텍스트 데이터를 토큰 단위로 처리하며, 주어진 프롬프트에 따라 텍스트를 생성한다. Transformer 모델의 주요 구성 요소는 <strong>어텐션 메커니즘</strong>으로, 이 메커니즘을 통해 문맥을 고려한 텍스트 생성을 수행한다.</p>
<h4 id="_7">어텐션 메커니즘</h4>
<p>Transformer의 핵심 요소인 어텐션 메커니즘은 주어진 입력 텍스트 내에서 중요한 부분을 더 잘 학습할 수 있도록 한다. 특히, GPT 모델은 <strong>셀프 어텐션(self-attention)</strong>을 사용하여 입력의 각 단어(토큰)가 문장의 다른 단어와 어떻게 연관되는지를 계산한다. 수학적으로, 어텐션 메커니즘은 다음과 같이 표현할 수 있다:</p>
<p>$$</p>
<p>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V</p>
<p>$$</p>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span>: Query 행렬
- <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>: Key 행렬
- <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span>: Value 행렬
- <span class="arithmatex"><span class="MathJax_Preview">d_k</span><script type="math/tex">d_k</script></span>: Key 벡터의 차원</p>
<p>이 어텐션 메커니즘을 통해 각 토큰이 문장의 다른 부분과 어떻게 상호작용하는지를 학습하게 되며, 이는 자연스러운 텍스트 생성의 기초가 된다.</p>
<h4 id="gpt">GPT의 작동 과정</h4>
<p>GPT 모델은 언어 모델로서, 주어진 프롬프트에 따라 다음에 나올 단어를 예측하는 방식으로 동작한다. 이를 위해 모델은 입력된 텍스트를 일련의 토큰으로 변환한 뒤, 이 토큰의 시퀀스를 기반으로 가장 확률이 높은 다음 토큰을 예측한다. 이 과정은 오토리그레시브(autoregressive) 방식으로 진행되며, 이전에 생성된 토큰들이 다음 토큰 생성에 영향을 미친다.</p>
<p>모델은 다음과 같은 확률 분포를 학습한다:</p>
<p>$$</p>
<p>P(w_i | w_1, w_2, \dots, w_{i-1})</p>
<p>$$</p>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">P(w_i)</span><script type="math/tex">P(w_i)</script></span>: 시퀀스에서 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>번째 단어 <span class="arithmatex"><span class="MathJax_Preview">w_i</span><script type="math/tex">w_i</script></span>가 등장할 확률
- <span class="arithmatex"><span class="MathJax_Preview">w_1, w_2, \dots, w_{i-1}</span><script type="math/tex">w_1, w_2, \dots, w_{i-1}</script></span>: 이전에 생성된 단어들</p>
<p>이 확률 분포를 기반으로 모델은 가장 적절한 토큰을 선택하여 텍스트를 생성한다.</p>
<h3 id="_8">프롬프트 엔지니어링의 중요성</h3>
<p>API 요청에서 프롬프트는 매우 중요한 역할을 한다. 프롬프트의 구성이 모델의 응답 품질에 직접적인 영향을 미치며, 이를 최적화하는 과정이 바로 <strong>프롬프트 엔지니어링</strong>이다.</p>
<h4 id="_9">효과적인 프롬프트 작성 전략</h4>
<p>효과적인 프롬프트는 명확하고 구체적이어야 한다. 특히, 원하는 응답의 유형을 명확히 정의하는 것이 중요하다. 예를 들어, 간단한 질문 대신 모델에게 구체적인 상황이나 문맥을 제공함으로써 더 정확한 응답을 얻을 수 있다.</p>
<p>프롬프트 작성 시에는 다음 사항을 고려할 수 있다:</p>
<ol>
<li><strong>명확성</strong>: 모호한 표현을 피하고 명확한 목표를 제공해야 한다.</li>
<li><strong>세부 사항 추가</strong>: 모델이 이해할 수 있도록 충분한 배경 정보를 제공한다.</li>
<li><strong>예시 활용</strong>: 응답의 형식을 미리 정의하거나 예시를 제시하는 것이 효과적이다.</li>
</ol>
<p>프롬프트 최적화를 통해 API 사용자는 더 일관된 결과를 얻을 수 있다.</p>
<h4 id="_10">온도와 샘플링 전략</h4>
<p>API 요청에서 <strong>온도</strong>와 <strong>탑 P</strong> 파라미터는 생성된 텍스트의 창의성에 큰 영향을 미친다. 모델이 생성하는 텍스트의 다양성을 제어하는 두 가지 주요 파라미터는 다음과 같다:</p>
<ol>
<li><strong>온도(temperature)</strong>: 이 값은 0에서 1 사이의 실수 값으로, 온도가 낮을수록 모델이 더 결정론적(deterministic)으로 동작하며, 온도가 높을수록 더 다양한 결과를 생성하게 된다. 수학적으로는 확률 분포의 평탄함을 조절하는 역할을 한다.</li>
</ol>
<p>$$</p>
<p>P'(w_i) = \frac{P(w_i)^{1/T}}{\sum_j P(w_j)^{1/T}}</p>
<p>$$</p>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">P'(w_i)</span><script type="math/tex">P'(w_i)</script></span>: 재조정된 확률
- <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>: 온도 값
- <span class="arithmatex"><span class="MathJax_Preview">P(w_i)</span><script type="math/tex">P(w_i)</script></span>: 원래의 확률 분포</p>
<ol>
<li><strong>탑 P(top_p)</strong>: 탑 P 샘플링은 상위 <span class="arithmatex"><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> 확률에 해당하는 토큰들만 선택하여 응답을 생성하는 방식이다. 예를 들어, <span class="arithmatex"><span class="MathJax_Preview">\text{top}_p = 0.9</span><script type="math/tex">\text{top}_p = 0.9</script></span>이면, 전체 확률의 90%를 차지하는 상위 토큰들만 선택된다.</li>
</ol>
<p>이 두 파라미터를 조합하여 모델의 출력 품질을 최적화할 수 있다.</p>
<h3 id="api_1">API 호출 예제 분석</h3>
<p>이제 OpenAI API에서 주로 사용되는 주요 기능과 그 사용법을 실제 예제를 통해 분석해보겠다. 아래는 API 호출의 기본적인 구조이다:</p>
<pre><code class="language-python">import openai

response = openai.Completion.create(
  model=&quot;text-davinci-003&quot;,
  prompt=&quot;Translate the following English text to French: '{}'&quot;,
  max_tokens=100,
  temperature=0.5
)
</code></pre>
<h4 id="_11">주요 파라미터 해설</h4>
<ol>
<li><strong>model</strong>: 사용할 GPT 모델을 지정한다. 이 예제에서는 "text-davinci-003"을 사용하고 있다.</li>
<li><strong>prompt</strong>: 모델에 제공할 입력 텍스트이다. 예시에서는 영어 문장을 프랑스어로 번역하라는 요청을 하고 있다.</li>
<li><strong>max_tokens</strong>: 모델이 생성할 응답의 최대 길이를 지정한다. 이 예제에서는 100 토큰까지의 응답을 허용한다.</li>
<li><strong>temperature</strong>: 응답의 무작위성을 제어하는 값이다. 0.5로 설정되어 있으며, 이는 중간 정도의 다양성을 의미한다.</li>
</ol>
<p>응답을 처리하는 과정에서는 JSON 형태로 반환된 데이터를 파싱하여 필요한 텍스트 부분을 추출하게 된다.</p>
<h3 id="_12">응답 데이터 처리 및 파싱</h3>
<p>API 호출 후에는 JSON 응답이 반환된다. 이 응답을 효과적으로 처리하기 위해서는 데이터 파싱 과정이 필요하다. 아래는 Python에서 응답을 처리하는 예시이다:</p>
<pre><code class="language-python">response_text = response['choices'][0]['text']
print(&quot;Generated Response: &quot;, response_text)
</code></pre>
<p>이 코드는 API의 응답에서 첫 번째 선택지(choice)의 텍스트를 추출하여 출력한다. 응답에서 각 선택지는 모델이 생성한 다양한 텍스트 응답을 포함할 수 있다.</p>
<h3 id="_13">요금 및 토큰 관리</h3>
<p>OpenAI API는 <strong>사용된 토큰 수</strong>에 따라 요금이 청구된다. API 요청을 보낼 때 <strong>프롬프트에 사용된 토큰</strong>과 <strong>모델이 생성한 응답에서 사용된 토큰</strong> 모두가 비용 계산에 포함된다. 따라서, API를 효율적으로 사용하기 위해서는 토큰 수를 적절히 관리하는 것이 중요하다.</p>
<h4 id="_14">토큰 사용 계산 방법</h4>
<p>토큰 수는 API 사용의 중요한 지표이다. 토큰 사용량을 효율적으로 관리하는 방법을 이해하기 위해, 요청 시 사용된 토큰의 양과 응답에서 사용된 토큰의 양을 각각 살펴봐야 한다. 이 둘을 합친 값이 <strong>총 토큰 수(total_tokens)</strong>이며, 총 토큰 수가 API 사용 요금의 기준이 된다.</p>
<p>$$</p>
<p>\text{total_tokens} = \text{prompt_tokens} + \text{completion_tokens}</p>
<p>$$</p>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">\text{prompt\_tokens}</span><script type="math/tex">\text{prompt\_tokens}</script></span>: 프롬프트에 사용된 토큰 수
- <span class="arithmatex"><span class="MathJax_Preview">\text{completion\_tokens}</span><script type="math/tex">\text{completion\_tokens}</script></span>: 모델이 생성한 응답에서 사용된 토큰 수</p>
<h3 id="_15">요금 최적화 전략</h3>
<p>API 비용을 최적화하는 몇 가지 방법은 다음과 같다:</p>
<ol>
<li><strong>프롬프트 길이 최적화</strong>: 짧고 명확한 프롬프트를 작성하여 불필요한 토큰을 줄이는 것이 중요하다. 프롬프트가 길어질수록 토큰 수가 증가하여 비용이 늘어나게 된다.</li>
<li><strong>응답 길이 제한</strong>: 응답에서 사용된 토큰 수도 중요한 요소이다. <code>max_tokens</code> 파라미터를 적절히 설정하여 응답의 길이를 제한함으로써 비용을 절감할 수 있다.</li>
<li><strong>모델 선택</strong>: GPT-4와 같은 고급 모델은 더 높은 비용을 요구하지만, 모든 작업에서 반드시 필요하지는 않는다. 작업의 성격에 따라 GPT-3.5와 같은 더 저렴한 모델을 선택하는 것도 비용을 절감하는 방법이다.</li>
<li><strong>캐싱</strong>: 동일한 프롬프트에 대해 반복적으로 요청할 경우, 이전 응답을 캐시하여 불필요한 API 호출을 줄일 수 있다.</li>
</ol>
<h3 id="_16">속도 및 성능 최적화</h3>
<p>OpenAI API는 대부분의 경우 실시간으로 응답을 제공하지만, 대규모 요청이나 복잡한 작업의 경우 응답 시간이 길어질 수 있다. 이를 개선하기 위한 몇 가지 방법을 소개한다.</p>
<h4 id="_17">병렬 처리</h4>
<p>여러 API 요청을 동시에 처리하는 방법으로 <strong>병렬 처리</strong>를 사용할 수 있다. Python에서는 <code>asyncio</code> 라이브러리나 멀티스레딩을 활용하여 API 호출을 병렬로 실행할 수 있다.</p>
<pre><code class="language-python">import asyncio
import openai

async def fetch_response(prompt):
    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,
        prompt=prompt,
        max_tokens=100
    )
    return response

async def main():
    prompts = [&quot;Prompt 1&quot;, &quot;Prompt 2&quot;, &quot;Prompt 3&quot;]
    tasks = [fetch_response(p) for p in prompts]
    responses = await asyncio.gather(*tasks)
    for r in responses:
        print(r)

asyncio.run(main())
</code></pre>
<h4 id="_18">요청 최적화</h4>
<p>요청을 최적화하는 또 다른 방법은 <strong>배치 처리(batch processing)</strong>이다. 한 번에 여러 프롬프트를 묶어 처리하면, 모델이 더 빠르게 응답을 생성할 수 있다. 이를 통해 응답 시간이 단축될 뿐 아니라 요금도 절감할 수 있다.</p>
<pre><code class="language-python">prompts = [&quot;Explain gravity.&quot;, &quot;What is quantum mechanics?&quot;]
response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=prompts,
    max_tokens=100
)
</code></pre>
<p>이처럼, 여러 요청을 하나로 묶어서 보내면 응답 시간이 개선될 수 있다.</p>
<h3 id="rate-limit">Rate Limit 관리</h3>
<p>OpenAI API는 사용자별로 <strong>Rate Limit</strong>를 설정한다. 이는 사용자가 일정 시간 동안 API를 호출할 수 있는 횟수를 제한하는 것으로, 과도한 요청을 방지하고 시스템 성능을 보호하기 위한 장치이다. Rate Limit를 초과하면 오류 응답이 반환되며, 이후 요청은 일정 시간이 지나야 다시 처리된다.</p>
<p>Rate Limit를 관리하기 위한 몇 가지 전략은 다음과 같다:</p>
<ol>
<li><strong>요청 빈도 관리</strong>: 일정 시간 내에 과도한 요청을 보내지 않도록 주의해야 한다. 필요한 경우, 요청 간에 일정 시간 간격을 두어 Rate Limit 초과를 방지할 수 있다.</li>
<li><strong>재시도 로직 구현</strong>: 네트워크 오류나 Rate Limit 초과가 발생했을 때 요청을 다시 시도하는 로직을 구현할 수 있다. 예를 들어, <code>time.sleep()</code> 함수를 사용하여 일정 시간 대기 후 다시 요청을 시도하는 방식이다.</li>
</ol>
<pre><code class="language-python">import time

def api_request(prompt):
    try:
        response = openai.Completion.create(
            model=&quot;text-davinci-003&quot;,
            prompt=prompt,
            max_tokens=100
        )
        return response
    except openai.error.RateLimitError:
        print(&quot;Rate limit exceeded. Retrying after 5 seconds...&quot;)
        time.sleep(5)
        return api_request(prompt)
</code></pre>
<p>이 코드는 Rate Limit가 초과되었을 때 일정 시간 대기 후 다시 요청을 시도하는 방식으로 Rate Limit 관리를 자동화한 예시이다.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../1202/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../1202/" class="btn btn-xs btn-link">
        주요 코드 스니펫 모음
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../../chapter_11/1103/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../../chapter_11/1103/" class="btn btn-xs btn-link">
        커뮤니티 및 지원 리소스 소개
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>