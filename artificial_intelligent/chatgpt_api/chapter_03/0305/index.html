<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/chatgpt_api/chapter_03/0305/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>응답 데이터 처리 및 파싱 - 실험 도서관</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "\uc751\ub2f5 \ub370\uc774\ud130 \uad6c\uc870 \uc774\ud574", url: "#_top", children: [
              {title: "JSON \uc751\ub2f5 \uad6c\uc870 \uc608\uc2dc", url: "#json" },
          ]},
          {title: "\ud14d\uc2a4\ud2b8 \ucd94\ucd9c \ubc0f \ud30c\uc2f1", url: "#_2", children: [
              {title: "\uae30\ubcf8 \ud14d\uc2a4\ud2b8 \ucd94\ucd9c", url: "#_3" },
              {title: "\uc751\ub2f5\uc5d0\uc11c \ucd94\uac00 \uc815\ubcf4 \ucd94\ucd9c", url: "#_4" },
          ]},
          {title: "JSON \ub370\uc774\ud130\uc758 \ubcf5\uc7a1\ud55c \ud30c\uc2f1", url: "#json_1", children: [
          ]},
          {title: "\ud30c\uc2f1\ub41c \ub370\uc774\ud130\ub97c \uc774\uc6a9\ud55c \ucd94\uac00 \ucc98\ub9ac", url: "#_5", children: [
              {title: "\uc815\uaddc \ud45c\ud604\uc2dd\uc744 \uc774\uc6a9\ud55c \ud14d\uc2a4\ud2b8 \ubd84\uc11d", url: "#_6" },
          ]},
          {title: "API \uc751\ub2f5\uc758 \uc548\uc804\uc131 \uac80\uc0ac", url: "#api", children: [
          ]},
          {title: "\uacb0\uce21\uac12 \ucc98\ub9ac \ubc0f \uae30\ubcf8\uac12 \uc124\uc815", url: "#_7", children: [
          ]},
          {title: "\ub2e4\uc591\ud55c \uc751\ub2f5 \ud3ec\ub9f7 \ucc98\ub9ac", url: "#_8", children: [
              {title: "\ub2e4\uc911 \uc120\ud0dd \ud56d\ubaa9 \ucc98\ub9ac", url: "#_9" },
          ]},
          {title: "\ube44\ub3d9\uae30 \uc751\ub2f5 \ucc98\ub9ac", url: "#_10", children: [
              {title: "\ube44\ub3d9\uae30 \ud638\ucd9c \uc608\uc81c", url: "#_11" },
          ]},
          {title: "\ub300\uaddc\ubaa8 \ub370\uc774\ud130 \ucc98\ub9ac", url: "#_12", children: [
              {title: "\ubc30\uce58 \ucc98\ub9ac", url: "#_13" },
          ]},
          {title: "\ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc758 \ud6c4\ucc98\ub9ac", url: "#_14", children: [
              {title: "\ud14d\uc2a4\ud2b8 \uc815\ub9ac \ubc0f \ud3ec\ub9f7\ud305", url: "#_15" },
              {title: "\ud0a4\uc6cc\ub4dc \ucd94\ucd9c \ubc0f \uc694\uc57d", url: "#_16" },
          ]},
          {title: "\ub85c\uadf8 \uad00\ub9ac \ubc0f \ub514\ubc84\uae45", url: "#_17", children: [
              {title: "\uae30\ubcf8 \ub85c\uadf8 \uc124\uc815", url: "#_18" },
          ]},
          {title: "\uc885\ud569\uc801\uc778 \uc751\ub2f5 \ucc98\ub9ac \uc608\uc81c", url: "#_19", children: [
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_04/0401/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_04/0401/" class="btn btn-xs btn-link">
        프롬프트 엔지니어링 기초
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0304/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0304/" class="btn btn-xs btn-link">
        요청 파라미터의 이해
      </a>
    </div>
    
  </div>

    

    <p>ChatGPT API를 활용하여 생성된 텍스트 응답을 처리하고 파싱하는 과정은 응용 프로그램의 성능과 사용성을 결정하는 중요한 단계이다. 이 절에서는 응답 데이터를 효율적으로 처리하고, 파싱하는 방법에 대해 자세히 설명한다.</p>
<h3 id="_1">응답 데이터 구조 이해</h3>
<p>OpenAI의 ChatGPT API는 호출에 대한 응답으로 JSON 형식의 데이터를 반환한다. 이 JSON 데이터는 여러 가지 정보를 담고 있으며, 이를 적절히 파싱하여 원하는 텍스트를 추출하는 것이 중요하다.</p>
<h4 id="json">JSON 응답 구조 예시</h4>
<p>아래는 ChatGPT API로부터 반환된 일반적인 응답의 구조를 보여준다:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;chatcmpl-123&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;created&quot;: 1691234567,
  &quot;model&quot;: &quot;gpt-4&quot;,
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;This is the generated text.&quot;
      },
      &quot;finish_reason&quot;: &quot;stop&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 10,
    &quot;completion_tokens&quot;: 7,
    &quot;total_tokens&quot;: 17
  }
}
</code></pre>
<p>이 JSON 객체에서 주요 관심 대상은 <code>choices</code> 배열 내의 <code>message</code> 객체이다. 이 객체에는 모델이 생성한 텍스트가 포함되어 있다.</p>
<h3 id="_2">텍스트 추출 및 파싱</h3>
<h4 id="_3">기본 텍스트 추출</h4>
<p>가장 기본적인 파싱 작업은 <code>choices</code> 배열의 첫 번째 요소에서 생성된 텍스트를 추출하는 것이다. 이를 위해 다음과 같은 파이썬 코드를 사용할 수 있다.</p>
<pre><code class="language-python">response = {
    &quot;id&quot;: &quot;chatcmpl-123&quot;,
    &quot;object&quot;: &quot;chat.completion&quot;,
    &quot;created&quot;: 1691234567,
    &quot;model&quot;: &quot;gpt-4&quot;,
    &quot;choices&quot;: [
        {
            &quot;index&quot;: 0,
            &quot;message&quot;: {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;This is the generated text.&quot;
            },
            &quot;finish_reason&quot;: &quot;stop&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 10,
        &quot;completion_tokens&quot;: 7,
        &quot;total_tokens&quot;: 17
    }
}

generated_text = response['choices'][0]['message']['content']
print(generated_text)
</code></pre>
<p>이 코드는 <code>choices</code> 배열의 첫 번째 항목에서 <code>content</code> 필드를 추출하여 생성된 텍스트를 출력한다.</p>
<h4 id="_4">응답에서 추가 정보 추출</h4>
<p>응답에는 생성된 텍스트 외에도 다양한 유용한 정보가 포함되어 있다. 예를 들어, 사용된 토큰 수나 <code>finish_reason</code> 같은 정보는 API 사용량을 추적하거나 응답이 어떻게 종료되었는지 파악하는 데 도움이 된다.</p>
<pre><code class="language-python">usage_info = response['usage']
prompt_tokens = usage_info['prompt_tokens']
completion_tokens = usage_info['completion_tokens']
total_tokens = usage_info['total_tokens']

finish_reason = response['choices'][0]['finish_reason']

print(f&quot;Prompt Tokens: {prompt_tokens}, Completion Tokens: {completion_tokens}, Total Tokens: {total_tokens}&quot;)
print(f&quot;Finish Reason: {finish_reason}&quot;)
</code></pre>
<h3 id="json_1">JSON 데이터의 복잡한 파싱</h3>
<p>때로는 응답 데이터가 더 복잡할 수 있으며, 여러 <code>choices</code> 항목이 포함될 수 있다. 이 경우 각 항목을 순회하며 데이터를 처리해야 한다.</p>
<pre><code class="language-python">for choice in response['choices']:
    generated_text = choice['message']['content']
    print(generated_text)
</code></pre>
<p>이 코드는 모든 <code>choices</code> 항목을 순회하며 각각의 생성된 텍스트를 출력한다.</p>
<h3 id="_5">파싱된 데이터를 이용한 추가 처리</h3>
<p>파싱된 텍스트는 다양한 용도로 활용될 수 있다. 예를 들어, 텍스트를 특정 패턴에 따라 분할하거나, 특정 키워드의 존재 여부를 확인하는 등의 작업을 수행할 수 있다.</p>
<h4 id="_6">정규 표현식을 이용한 텍스트 분석</h4>
<p>정규 표현식(Regular Expressions, RegEx)을 사용하여 생성된 텍스트에서 특정 패턴을 추출하거나 분석할 수 있다.</p>
<pre><code class="language-python">import re

pattern = r'\d+'  # 숫자를 찾는 정규 표현식 패턴
matches = re.findall(pattern, generated_text)

print(f&quot;Found numbers: {matches}&quot;)
</code></pre>
<p>이 예시는 생성된 텍스트에서 모든 숫자를 찾아 출력하는 코드이다.</p>
<h3 id="api">API 응답의 안전성 검사</h3>
<p>파싱하기 전에 응답 데이터가 예상된 구조를 가지고 있는지 확인하는 것은 매우 중요하다. API 응답의 구조가 변경되었거나 예상치 못한 오류가 발생할 수 있으므로, 안전성 검사를 통해 예외 처리를 해야 한다.</p>
<pre><code class="language-python">if 'choices' in response and len(response['choices']) &gt; 0:
    generated_text = response['choices'][0]['message']['content']
else:
    generated_text = &quot;No content generated.&quot;

print(generated_text)
</code></pre>
<p>이 코드는 <code>choices</code> 배열이 존재하고 비어 있지 않은지 확인한 후에 텍스트를 추출한다.</p>
<h3 id="_7">결측값 처리 및 기본값 설정</h3>
<p>응답 데이터에서 특정 필드가 없거나 <code>null</code> 값일 경우를 대비해 기본값을 설정하는 방법도 필요하다.</p>
<pre><code class="language-python">generated_text = response.get('choices', [{}])[0].get('message', {}).get('content', 'No content generated.')
print(generated_text)
</code></pre>
<p>이 코드는 안전하게 값을 추출하며, 데이터가 없을 경우 <code>"No content generated."</code>라는 기본값을 반환한다.</p>
<h3 id="_8">다양한 응답 포맷 처리</h3>
<p>때로는 OpenAI API의 응답이 복잡한 형식이거나, 여러 가지 시나리오에 따라 다르게 구성될 수 있다. 이러한 경우를 대비하여 다양한 응답 포맷을 처리할 수 있는 유연한 파싱 로직을 구현하는 것이 필요하다.</p>
<h4 id="_9">다중 선택 항목 처리</h4>
<p>API 응답에서 <code>choices</code> 배열이 여러 개의 선택지를 포함하고 있을 때, 각 선택지를 모두 처리해야 할 수 있다. 예를 들어, 대화형 챗봇에서 여러 응답을 제시할 때 유용하다.</p>
<pre><code class="language-python">responses = []
for choice in response.get('choices', []):
    text = choice.get('message', {}).get('content', '')
    responses.append(text)

print(&quot;Generated responses:&quot;)
for i, text in enumerate(responses, 1):
    print(f&quot;Response {i}: {text}&quot;)
</code></pre>
<p>이 코드는 여러 <code>choices</code> 항목을 처리하여 각각의 생성된 텍스트를 리스트에 저장한 후, 이를 출력한다.</p>
<h3 id="_10">비동기 응답 처리</h3>
<p>대규모 애플리케이션에서는 API 호출과 응답 처리의 비동기 처리가 필요할 수 있다. Python의 <code>asyncio</code> 라이브러리와 함께 OpenAI API를 비동기적으로 호출하고 응답을 처리하는 방법을 살펴보겠다.</p>
<h4 id="_11">비동기 호출 예제</h4>
<pre><code class="language-python">import asyncio
import openai

async def fetch_response(prompt):
    response = await openai.ChatCompletion.create(
        model=&quot;gpt-4&quot;,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
    )
    return response['choices'][0]['message']['content']

async def main():
    prompts = [&quot;Hello!&quot;, &quot;How are you?&quot;, &quot;Tell me a joke.&quot;]
    tasks = [fetch_response(prompt) for prompt in prompts]
    responses = await asyncio.gather(*tasks)

    for i, response in enumerate(responses, 1):
        print(f&quot;Response {i}: {response}&quot;)

asyncio.run(main())
</code></pre>
<p>이 코드는 여러 프롬프트에 대해 비동기적으로 API 호출을 수행하고, 응답을 병렬로 처리한다.</p>
<h3 id="_12">대규모 데이터 처리</h3>
<p>API 응답을 대규모 데이터로 처리할 때, 예를 들어 수천 개의 텍스트 생성 작업을 병렬로 수행해야 할 때, 데이터를 효율적으로 처리하는 방법을 고려해야 한다.</p>
<h4 id="_13">배치 처리</h4>
<p>대규모 데이터에서 성능을 최적화하기 위해 응답을 배치로 처리할 수 있다.</p>
<pre><code class="language-python">batch_size = 5
prompts = [&quot;Prompt 1&quot;, &quot;Prompt 2&quot;, &quot;Prompt 3&quot;, &quot;Prompt 4&quot;, &quot;Prompt 5&quot;, &quot;Prompt 6&quot;]
batches = [prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]

for batch in batches:
    tasks = [fetch_response(prompt) for prompt in batch]
    responses = asyncio.run(asyncio.gather(*tasks))
    print(&quot;Batch responses:&quot;, responses)
</code></pre>
<p>이 코드는 프롬프트를 배치로 나누어 비동기적으로 처리한다. 대규모 데이터 세트를 효율적으로 처리할 때 유용한 기법이다.</p>
<h3 id="_14">텍스트 데이터의 후처리</h3>
<p>생성된 텍스트를 받아 다양한 형태로 후처리할 수 있다. 예를 들어, 특정 포맷으로 변환하거나, 텍스트를 분석하여 추가적인 정보를 추출하는 작업을 수행할 수 있다.</p>
<h4 id="_15">텍스트 정리 및 포맷팅</h4>
<p>생성된 텍스트에 불필요한 공백이나 특수 문자가 포함될 수 있다. 이를 정리하여 사용하기 적합한 형태로 변환한다.</p>
<pre><code class="language-python">cleaned_text = generated_text.strip().replace(&quot;\n&quot;, &quot; &quot;)
print(f&quot;Cleaned Text: {cleaned_text}&quot;)
</code></pre>
<p>이 코드는 텍스트 양쪽의 공백을 제거하고, 줄 바꿈 문자를 공백으로 대체한다.</p>
<h4 id="_16">키워드 추출 및 요약</h4>
<p>생성된 텍스트에서 중요한 키워드를 추출하거나 요약할 수 있다.</p>
<pre><code class="language-python">import re

def extract_keywords(text, top_n=3):
    words = re.findall(r'\b\w+\b', text.lower())
    freq = {}
    for word in words:
        freq[word] = freq.get(word, 0) + 1
    sorted_keywords = sorted(freq.items(), key=lambda item: item[1], reverse=True)
    return sorted_keywords[:top_n]

keywords = extract_keywords(generated_text)
print(f&quot;Keywords: {keywords}&quot;)
</code></pre>
<p>이 코드는 텍스트에서 가장 빈도가 높은 키워드를 추출하여 출력한다.</p>
<h3 id="_17">로그 관리 및 디버깅</h3>
<p>API 응답의 파싱 및 처리를 디버깅하기 위해 로그를 남기는 것은 매우 중요하다. 특히 비동기 처리와 같이 복잡한 응답을 처리할 때 로그는 문제를 추적하는 데 유용하다.</p>
<h4 id="_18">기본 로그 설정</h4>
<pre><code class="language-python">import logging

logging.basicConfig(level=logging.INFO)

def log_response(response):
    logging.info(f&quot;API Response: {response}&quot;)

log_response(response)
</code></pre>
<p>이 코드는 기본적인 로그 설정을 통해 API 응답을 기록한다. 로그를 통해 각 단계에서 발생하는 데이터를 추적할 수 있다.</p>
<h3 id="_19">종합적인 응답 처리 예제</h3>
<p>위에서 언급한 다양한 방법을 결합하여, 보다 종합적이고 실용적인 응답 처리 시스템을 구축할 수 있다.</p>
<pre><code class="language-python">import logging

logging.basicConfig(level=logging.INFO)

async def handle_api_response(prompt):
    try:
        response = await openai.ChatCompletion.create(
            model=&quot;gpt-4&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}]
        )
        text = response['choices'][0]['message']['content']
        logging.info(f&quot;Generated Text: {text.strip()}&quot;)

        # 추가적인 텍스트 후처리 및 분석
        cleaned_text = text.strip().replace(&quot;\n&quot;, &quot; &quot;)
        keywords = extract_keywords(cleaned_text)
        logging.info(f&quot;Keywords: {keywords}&quot;)

        return cleaned_text, keywords
    except Exception as e:
        logging.error(f&quot;Error processing API response: {str(e)}&quot;)
        return None, None

async def main():
    prompts = [&quot;First prompt&quot;, &quot;Second prompt&quot;, &quot;Third prompt&quot;]
    tasks = [handle_api_response(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)

    for i, (cleaned_text, keywords) in enumerate(results, 1):
        print(f&quot;Result {i}: Text - {cleaned_text}, Keywords - {keywords}&quot;)

asyncio.run(main())
</code></pre>
<p>이 코드는 API 호출에서부터 응답 파싱, 후처리, 키워드 추출, 그리고 로그 관리까지 포함하는 종합적인 처리 파이프라인을 제공한다.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_04/0401/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_04/0401/" class="btn btn-xs btn-link">
        프롬프트 엔지니어링 기초
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0304/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0304/" class="btn btn-xs btn-link">
        요청 파라미터의 이해
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>