<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/artificial_intelligent/chatgpt_api/chapter_06/0603/">
    <link rel="shortcut icon" href="../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>생성된 텍스트 기반 데이터 분석 - 소프트웨어 융합</title>
    <link href="../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../css/highlight.css">
    <link href="../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "\ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc758 \uc804\ucc98\ub9ac", url: "#_top", children: [
          ]},
          {title: "n-\uadf8\ub7a8 \ubd84\uc11d", url: "#n-", children: [
          ]},
          {title: "TF-IDF \ubd84\uc11d", url: "#tf-idf", children: [
          ]},
          {title: "\uac10\uc131 \ubd84\uc11d", url: "#_2", children: [
          ]},
          {title: "\uc8fc\uc81c \ubaa8\ub378\ub9c1", url: "#_3", children: [
          ]},
          {title: "\ubb38\uc11c \uc784\ubca0\ub529", url: "#_4", children: [
          ]},
          {title: "\uad70\uc9d1\ud654 \ubc0f \ubd84\ub958", url: "#_5", children: [
          ]},
          {title: "\ubb38\uc11c \uc694\uc57d", url: "#_6", children: [
          ]},
          {title: "\uc8fc\uc131\ubd84 \ubd84\uc11d (PCA) \ubc0f \ucc28\uc6d0 \ucd95\uc18c", url: "#pca", children: [
          ]},
          {title: "Word Cloud \uc0dd\uc131", url: "#word-cloud", children: [
          ]},
          {title: "\ud074\ub7ec\uc2a4\ud130 \ud574\uc11d", url: "#_7", children: [
          ]},
        ];

    </script>
    <script src="../../../../js/base.js"></script>
      <script src="../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../0604/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../0604/" class="btn btn-xs btn-link">
        대화형 봇 제작
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0602/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0602/" class="btn btn-xs btn-link">
        자동화된 고객 지원 시스템 구현
      </a>
    </div>
    
  </div>

    

    <p>생성된 텍스트 기반 데이터 분석은 ChatGPT API를 통해 생성된 텍스트 데이터를 활용하여 다양한 분석을 수행하는 과정이다. 이 장에서는 생성된 텍스트 데이터를 활용한 자연어 처리 기법과 데이터 분석의 주요 개념을 다룬다.</p>
<h3 id="_1">텍스트 데이터의 전처리</h3>
<p>생성된 텍스트 데이터를 분석하기 전에, 텍스트 데이터를 효과적으로 처리하기 위한 전처리 과정이 필요하다. 전처리 단계는 다음과 같은 과정을 포함할 수 있다.</p>
<ul>
<li>
<p><strong>토큰화 (Tokenization)</strong>: 텍스트를 단어 또는 서브워드 단위로 분리한다. 예를 들어, 문장 "ChatGPT is powerful."는 ['ChatGPT', 'is', 'powerful', '.']로 토큰화될 수 있다.</p>
</li>
<li>
<p><strong>불용어 제거 (Stopword Removal)</strong>: 'the', 'is', 'and'와 같이 분석에 크게 기여하지 않는 단어들을 제거한다.</p>
</li>
<li>
<p><strong>어간 추출 및 표제어 추출 (Stemming and Lemmatization)</strong>: 단어의 어간이나 기본 형태로 변환하여 어휘를 표준화한다. 예를 들어, "running"은 "run"으로 변환될 수 있다.</p>
</li>
<li>
<p><strong>텍스트 정규화 (Text Normalization)</strong>: 대소문자 통일, 특수 문자 제거, 숫자 변환 등을 통해 텍스트를 정규화한다.</p>
</li>
</ul>
<h3 id="n-">n-그램 분석</h3>
<p>n-그램은 연속된 n개의 단어로 구성된 단위를 말한다. n-그램 분석은 텍스트에서 특정 단어 조합의 빈도를 분석하는 데 유용하다. </p>
<ul>
<li>
<p><strong>1-그램 (유니그램, Unigram)</strong>: 각 단어를 독립적으로 분석한다. 예를 들어, 문장 "ChatGPT is powerful"의 유니그램은 ['ChatGPT', 'is', 'powerful']이다.</p>
</li>
<li>
<p><strong>2-그램 (바이그램, Bigram)</strong>: 연속된 두 단어 쌍을 분석한다. 예를 들어, 문장 "ChatGPT is powerful"의 바이그램은 [('ChatGPT', 'is'), ('is', 'powerful')]이다.</p>
</li>
<li>
<p><strong>3-그램 (트라이그램, Trigram)</strong>: 연속된 세 단어의 조합을 분석한다. 예를 들어, "ChatGPT is powerful and versatile"의 트라이그램은 [('ChatGPT', 'is', 'powerful'), ('is', 'powerful', 'and'), ('powerful', 'and', 'versatile')]이다.</p>
</li>
</ul>
<p>n-그램 분석을 통해 텍스트에서 자주 나타나는 단어 패턴을 발견하고, 이를 바탕으로 텍스트의 주요 주제나 특징을 추출할 수 있다.</p>
<h3 id="tf-idf">TF-IDF 분석</h3>
<p>TF-IDF (Term Frequency-Inverse Document Frequency)는 특정 단어가 문서 내에서 얼마나 중요한지를 평가하는 통계적 수치이다. 이는 텍스트 데이터에서 중요한 단어를 식별하는 데 사용된다.</p>
<ul>
<li><strong>TF (Term Frequency)</strong>: 단어의 빈도수를 의미한다. 특정 단어 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>가 문서 <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>에서 등장하는 빈도를 <span class="arithmatex"><span class="MathJax_Preview">tf(t, d)</span><script type="math/tex">tf(t, d)</script></span>로 나타낸다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
tf(t, d) = \frac{\text{해당 문서에서 단어 } t \text{의 등장 횟수}}{\text{해당 문서의 총 단어 수}}
</div>
<script type="math/tex; mode=display">
tf(t, d) = \frac{\text{해당 문서에서 단어 } t \text{의 등장 횟수}}{\text{해당 문서의 총 단어 수}}
</script>
</div>
<ul>
<li><strong>IDF (Inverse Document Frequency)</strong>: 단어의 중요도를 계산한다. 단어가 여러 문서에 걸쳐 자주 등장할수록 그 단어는 중요도가 낮다고 판단한다. 특정 단어 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>에 대한 IDF는 다음과 같이 계산된다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
idf(t, D) = \log \left(\frac{N}{|\{d \in D : t \in d\}|} \right)
</div>
<script type="math/tex; mode=display">
idf(t, D) = \log \left(\frac{N}{|\{d \in D : t \in d\}|} \right)
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span>은 전체 문서의 수를 의미하고, <span class="arithmatex"><span class="MathJax_Preview">|\{d \in D : t \in d\}|</span><script type="math/tex">|\{d \in D : t \in d\}|</script></span>는 단어 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>가 등장한 문서의 수를 의미한다.</p>
<ul>
<li><strong>TF-IDF 계산</strong>: 특정 단어 <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>에 대한 TF-IDF 값은 다음과 같이 계산된다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
tfidf(t, d, D) = tf(t, d) \times idf(t, D)
</div>
<script type="math/tex; mode=display">
tfidf(t, d, D) = tf(t, d) \times idf(t, D)
</script>
</div>
<p>TF-IDF는 단어의 빈도가 높고, 동시에 특정 문서에서만 자주 등장하는 단어를 강조한다. 이를 통해 텍스트 데이터에서 중요한 키워드를 식별할 수 있다.</p>
<h3 id="_2">감성 분석</h3>
<p>감성 분석(Sentiment Analysis)은 텍스트에서 감정을 추출하고, 이를 긍정적, 부정적, 중립적 등으로 분류하는 과정이다. 생성된 텍스트 데이터를 분석하여 사용자 의견, 리뷰, 소셜 미디어 글 등에서 감성을 파악할 수 있다.</p>
<ul>
<li>
<p><strong>감성 사전 기반 접근법</strong>: 감성 사전에 기반하여 각 단어의 감성 점수를 부여한 후, 문서 전체의 감성을 계산하는 방법이다. 예를 들어, 특정 단어가 긍정적인 의미를 갖는다면 그 단어에 높은 긍정적 점수를 부여한다. 문서 내의 모든 단어의 점수를 합산하여 전체 문서의 감성을 결정한다.</p>
</li>
<li>
<p><strong>머신 러닝 기반 접근법</strong>: 사전 학습된 감성 분석 모델을 활용하여 텍스트 데이터를 분석한다. 이 방법에서는 데이터셋을 이용하여 모델을 학습시킨 후, 새로운 텍스트 데이터에 대해 감성을 예측한다. 주로 Naive Bayes, SVM, 또는 최근에는 딥러닝 기반 모델이 사용된다.</p>
</li>
</ul>
<h3 id="_3">주제 모델링</h3>
<p>주제 모델링(Topic Modeling)은 텍스트 데이터에서 숨겨진 주제를 발견하고, 이를 문서별로 분류하는 기술이다. 대표적인 주제 모델링 기법으로는 LDA(Latent Dirichlet Allocation)가 있다.</p>
<ul>
<li><strong>LDA(Latent Dirichlet Allocation)</strong>: LDA는 문서가 여러 주제의 혼합물이라는 가정 하에, 각 문서 내에서 단어의 분포를 학습하여 주제를 추출한다. LDA는 주제와 단어의 관계를 설명하는 확률 모델이다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\mathbf{w}|\mathbf{z}, \mathbf{\theta}, \mathbf{\beta}) = \prod_{n=1}^{N} p(w_n|z_n, \mathbf{\beta}) p(z_n|\mathbf{\theta})
</div>
<script type="math/tex; mode=display">
p(\mathbf{w}|\mathbf{z}, \mathbf{\theta}, \mathbf{\beta}) = \prod_{n=1}^{N} p(w_n|z_n, \mathbf{\beta}) p(z_n|\mathbf{\theta})
</script>
</div>
<p>여기서, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{w}</span><script type="math/tex">\mathbf{w}</script></span>는 단어의 집합, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{z}</span><script type="math/tex">\mathbf{z}</script></span>는 주제, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\theta}</span><script type="math/tex">\mathbf{\theta}</script></span>는 문서의 주제 분포, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\beta}</span><script type="math/tex">\mathbf{\beta}</script></span>는 주제별 단어 분포를 나타낸다.</p>
<p>LDA를 사용하여 문서 내 주제 분포를 추정하면, 문서가 어떤 주제들로 구성되어 있는지 파악할 수 있으며, 이 정보를 바탕으로 문서를 분류하거나 유사한 문서끼리 군집화할 수 있다.</p>
<h3 id="_4">문서 임베딩</h3>
<p>문서 임베딩(Document Embedding)은 문서 전체를 고차원 벡터 공간으로 매핑하여 유사성을 계산할 수 있도록 하는 기법이다. 이를 통해 생성된 텍스트 데이터를 수치화하고, 다양한 분석 작업을 수행할 수 있다.</p>
<ul>
<li><strong>TF-IDF 벡터화</strong>: 앞서 설명한 TF-IDF를 이용하여 문서를 벡터로 변환한다. 각 문서의 단어 빈도 및 중요도를 반영한 고차원 벡터를 생성한다. 이 벡터를 이용해 문서 간의 코사인 유사도(Cosine Similarity) 등을 계산할 수 있다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\text{Cosine Similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \times ||\mathbf{B}||}
</div>
<script type="math/tex; mode=display">
\text{Cosine Similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \times ||\mathbf{B}||}
</script>
</div>
<ul>
<li>
<p><strong>Word2Vec 및 Doc2Vec</strong>: Word2Vec은 개별 단어를 벡터로 표현하는 모델이며, 이를 확장한 Doc2Vec은 문서 전체를 벡터로 표현한다. Doc2Vec은 문서 ID를 단어로 간주하여 학습하므로, 각 문서의 의미를 잘 반영한 벡터를 생성할 수 있다.</p>
</li>
<li>
<p><strong>BERT 임베딩</strong>: BERT와 같은 사전 학습된 트랜스포머 모델을 이용해 문서를 임베딩할 수 있다. BERT는 문맥을 반영하여 단어를 임베딩하므로, 문서의 문맥 정보를 포함한 고차원 벡터를 생성한다.</p>
</li>
</ul>
<h3 id="_5">군집화 및 분류</h3>
<p>생성된 텍스트 데이터를 분석한 후, 이 데이터를 기반으로 군집화(Clustering) 및 분류(Classification) 작업을 수행할 수 있다.</p>
<ul>
<li><strong>K-Means 군집화</strong>: K-Means 알고리즘은 문서 임베딩을 이용해 문서를 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>개의 군집으로 분류한다. 각 문서는 가장 가까운 군집 중심(centroid)에 할당되며, 군집 중심은 반복적으로 업데이트된다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\mathbf{C}} \sum_{i=1}^{k} \sum_{\mathbf{x} \in \mathbf{C}_i} ||\mathbf{x} - \mu_i||^2
</div>
<script type="math/tex; mode=display">
\min_{\mathbf{C}} \sum_{i=1}^{k} \sum_{\mathbf{x} \in \mathbf{C}_i} ||\mathbf{x} - \mu_i||^2
</script>
</div>
<p>여기서, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{C}_i</span><script type="math/tex">\mathbf{C}_i</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>번째 군집, <span class="arithmatex"><span class="MathJax_Preview">\mu_i</span><script type="math/tex">\mu_i</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>번째 군집의 중심이다.</p>
<ul>
<li><strong>SVM을 이용한 분류</strong>: SVM(서포트 벡터 머신)은 문서를 특정 카테고리로 분류하는 데 사용된다. 문서 임베딩 벡터를 사용하여, 각 문서를 이진 또는 다중 클래스 분류 작업을 수행한다.</li>
</ul>
<h3 id="_6">문서 요약</h3>
<p>생성된 텍스트 데이터를 분석하는 또 다른 방법으로, 텍스트 요약(Summarization)을 통해 긴 문서에서 핵심 정보를 추출할 수 있다. 문서 요약은 크게 두 가지 방법으로 나뉜다: 추출적 요약(Extractive Summarization)과 생성적 요약(Abstractive Summarization).</p>
<ul>
<li>
<p><strong>추출적 요약</strong>: 원문에서 중요한 문장이나 구절을 추출하여 요약을 생성한다. 이 방법은 원문에서 핵심 문장을 그대로 가져오는 방식으로, 단순하지만 원문에서 중요한 내용을 놓치지 않고 전달할 수 있다.</p>
</li>
<li>
<p><strong>텍스트 랭크 (TextRank)</strong>: 그래프 기반 알고리즘으로, 문서 내 문장 간의 유사성을 그래프 형태로 나타내고, 중요도가 높은 문장을 요약으로 선택한다.</p>
</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\text{TextRank}(V_i) = (1-d) + d \sum_{j \in \text{In}(V_i)} \frac{1}{|\text{Out}(V_j)|} \text{TextRank}(V_j)
</div>
<script type="math/tex; mode=display">
\text{TextRank}(V_i) = (1-d) + d \sum_{j \in \text{In}(V_i)} \frac{1}{|\text{Out}(V_j)|} \text{TextRank}(V_j)
</script>
</div>
<p>여기서, <span class="arithmatex"><span class="MathJax_Preview">V_i</span><script type="math/tex">V_i</script></span>는 문장 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>를 나타내고, <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>는 감쇄 계수이다.</p>
<ul>
<li>
<p><strong>생성적 요약</strong>: 원문의 내용을 이해하고, 그 내용을 바탕으로 새로운 문장을 생성하여 요약을 만든다. 이 방법은 자연어 처리 모델이 원문을 재구성하여 새로운 표현으로 요약을 생성하는 방식으로, GPT-3 또는 BERT 기반 모델을 활용하여 수행할 수 있다.</p>
</li>
<li>
<p><strong>Sequence-to-Sequence 모델</strong>: 인코더-디코더 구조를 사용하여 원문을 입력받고, 디코더가 요약 문장을 생성한다. 이 방법은 문장의 의미를 유지하면서 원문과는 다른 표현으로 요약을 생성할 수 있다.</p>
</li>
</ul>
<h3 id="pca">주성분 분석 (PCA) 및 차원 축소</h3>
<p>고차원 벡터 공간에서 분석을 수행할 때, 차원 축소 기법을 통해 데이터를 시각화하거나 분석 속도를 향상시킬 수 있다. 주성분 분석(PCA)은 차원 축소를 위해 자주 사용되는 기법이다.</p>
<ul>
<li><strong>PCA (Principal Component Analysis)</strong>: 데이터의 분산을 최대화하는 축을 찾아 고차원 데이터를 저차원으로 변환한다. 문서 임베딩 벡터에 PCA를 적용하면, 주요 특징을 유지하면서 차원을 줄일 수 있다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{Z} = \mathbf{X} \mathbf{W}
</div>
<script type="math/tex; mode=display">
\mathbf{Z} = \mathbf{X} \mathbf{W}
</script>
</div>
<p>여기서, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{X}</span><script type="math/tex">\mathbf{X}</script></span>는 원본 데이터 행렬, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span>는 고유 벡터 행렬, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{Z}</span><script type="math/tex">\mathbf{Z}</script></span>는 변환된 저차원 데이터이다.</p>
<ul>
<li><strong>t-SNE</strong>: t-SNE(t-Distributed Stochastic Neighbor Embedding)는 데이터의 고차원 구조를 저차원 공간에서 시각화하는데 유용한 비선형 차원 축소 기법이다. 문서 간의 관계를 시각적으로 표현할 때 유용하게 사용된다.</li>
</ul>
<h3 id="word-cloud">Word Cloud 생성</h3>
<p>생성된 텍스트 데이터에서 중요한 단어를 시각적으로 표현하기 위해 워드 클라우드(Word Cloud)를 생성할 수 있다. 워드 클라우드는 단어의 빈도에 따라 크기가 달라지는 시각적 표현으로, 텍스트 데이터의 주요 주제를 직관적으로 파악할 수 있다.</p>
<ul>
<li><strong>워드 클라우드 생성 과정</strong>: 텍스트 데이터에서 중요한 단어를 추출하고, 각 단어의 빈도에 따라 크기를 결정하여 시각화한다. Python에서는 <code>wordcloud</code> 라이브러리를 이용해 간단히 워드 클라우드를 생성할 수 있다.</li>
</ul>
<p>```python
  from wordcloud import WordCloud
  import matplotlib.pyplot as plt</p>
<p>text = "생성된 텍스트 데이터를 활용한 분석 기법"
  wordcloud = WordCloud(width=800, height=400).generate(text)</p>
<p>plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis("off")
  plt.show()
  ```</p>
<h3 id="_7">클러스터 해석</h3>
<p>문서 군집화나 분류 작업 후에는 각 클러스터의 특징을 해석하는 과정이 필요하다. 이는 각 군집에 속한 문서들의 공통점이나 주제를 파악하여, 군집의 의미를 명확히 하는 작업이다.</p>
<ul>
<li>
<p><strong>클러스터 내 주요 단어 추출</strong>: 각 클러스터에서 자주 등장하는 단어를 추출하여 클러스터의 주제를 파악한다. TF-IDF나 Word2Vec 등의 임베딩 기법을 활용해 클러스터 내 중요한 단어를 선정할 수 있다.</p>
</li>
<li>
<p><strong>클러스터 내 문서 분석</strong>: 클러스터에 속한 문서들을 샘플링하여 직접 읽고 분석한다. 이를 통해 클러스터가 어떤 종류의 문서들로 이루어져 있는지, 공통된 주제가 무엇인지 파악할 수 있다.</p>
</li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../0604/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../0604/" class="btn btn-xs btn-link">
        대화형 봇 제작
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0602/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0602/" class="btn btn-xs btn-link">
        자동화된 고객 지원 시스템 구현
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>