<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/sensor_data_processing/pointcloud/introductions_to_pointcloud_processing_with_pcl_library/chapter_05/0505/">
    <link rel="shortcut icon" href="../../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>주요 분할 알고리즘의 특징 - 실험 도서관</title>
    <link href="../../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../../css/highlight.css">
    <link href="../../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "RANSAC (Random Sample Consensus) \uc54c\uace0\ub9ac\uc998", url: "#_top", children: [
          ]},
          {title: "Region Growing \uc54c\uace0\ub9ac\uc998", url: "#region-growing", children: [
          ]},
          {title: "K-\ud3c9\uade0 (K-Means) \uc54c\uace0\ub9ac\uc998", url: "#k-k-means", children: [
          ]},
          {title: "DBSCAN (Density-Based Spatial Clustering of Applications with Noise)", url: "#dbscan-density-based-spatial-clustering-of-applications-with-noise", children: [
          ]},
          {title: "Mean Shift \uc54c\uace0\ub9ac\uc998", url: "#mean-shift", children: [
          ]},
          {title: "Spectral Clustering \uc54c\uace0\ub9ac\uc998", url: "#spectral-clustering", children: [
          ]},
          {title: "7. Hierarchical Clustering (\uacc4\uce35\uc801 \ud074\ub7ec\uc2a4\ud130\ub9c1)", url: "#7-hierarchical-clustering", children: [
          ]},
          {title: "Gaussian Mixture Model (GMM) \uc54c\uace0\ub9ac\uc998", url: "#gaussian-mixture-model-gmm", children: [
          ]},
          {title: "Fuzzy C-Means (FCM) \uc54c\uace0\ub9ac\uc998", url: "#fuzzy-c-means-fcm", children: [
          ]},
          {title: "Agglomerative Clustering (\ubcd1\ud569\uc801 \ud074\ub7ec\uc2a4\ud130\ub9c1)", url: "#agglomerative-clustering", children: [
          ]},
          {title: "OPTICS (Ordering Points To Identify the Clustering Structure)", url: "#optics-ordering-points-to-identify-the-clustering-structure", children: [
          ]},
        ];

    </script>
    <script src="../../../../../js/base.js"></script>
      <script src="../../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_06/0601/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_06/0601/" class="btn btn-xs btn-link">
        포인트 특징의 개념과 정의
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0504/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0504/" class="btn btn-xs btn-link">
        유사성 기반 클러스터링 기법
      </a>
    </div>
    
  </div>

    

    <p>분할 알고리즘은 포인트 클라우드 데이터에서 특정 구조를 찾아내기 위한 기법으로, 여러 알고리즘이 다양한 방식으로 데이터를 처리한다. 각 알고리즘은 데이터를 처리하는 기준과 특징이 다르며, 이러한 차이점은 특정 응용에 더 적합한 알고리즘을 선택하는 데 중요한 역할을 한다.</p>
<h3 id="ransac-random-sample-consensus"><strong>RANSAC (Random Sample Consensus) 알고리즘</strong></h3>
<p>RANSAC은 데이터에서 이상치(outlier)를 제거하고 모델을 찾는 데 사용되는 매우 강력한 기법이다. RANSAC은 반복적으로 랜덤한 샘플을 선택하여 모델을 구축하고, 해당 모델에 가장 적합한 포인트를 찾는 방식으로 동작한다. 이 알고리즘의 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>이상치에 강건함</strong>: 포인트 클라우드 데이터에서 노이즈나 이상치가 있을 때도 유효한 모델을 찾는 데 강력한다.</li>
<li><strong>랜덤 샘플링</strong>: 데이터 중 일부만을 사용하여 모델을 구축하고, 모델에 적합한 포인트를 식별한다.</li>
<li><strong>모델의 반복 평가</strong>: 샘플링된 데이터로부터 추정된 모델이 전체 데이터에 얼마나 적합한지 평가한다.</li>
</ul>
<p>RANSAC의 수학적 모델은 다음과 같이 표현된다. 주어진 포인트 클라우드에서 모델을 추정하려면, 예를 들어, 평면 모델의 경우 평면 방정식은 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{n} \cdot \mathbf{p} + d = 0
</div>
<script type="math/tex; mode=display">
\mathbf{n} \cdot \mathbf{p} + d = 0
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{n}</span><script type="math/tex">\mathbf{n}</script></span>은 평면의 노말 벡터, <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}</span><script type="math/tex">\mathbf{p}</script></span>는 포인트, <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span>는 평면의 거리이다. RANSAC은 이 평면 방정식을 추정하기 위해 랜덤 샘플링을 수행하고, 이상치로부터 강건한 결과를 제공한다.</p>
<h3 id="region-growing"><strong>Region Growing 알고리즘</strong></h3>
<p>Region Growing 알고리즘은 포인트 클라우드에서 인접한 포인트들이 유사한 특성을 가지고 있을 때, 해당 포인트들을 하나의 클러스터로 그룹화하는 방식이다. 이 알고리즘은 초기 시드(seed) 포인트를 설정하고, 시드에 인접한 포인트들을 확장해가며 클러스터를 형성한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>인접성 기반</strong>: 각 포인트의 인접 포인트들과의 거리를 계산하여 클러스터를 확장한다.</li>
<li><strong>유사성 기준</strong>: 포인트 간의 기하학적 또는 색상 유사성을 기준으로 그룹을 형성한다.</li>
<li><strong>연속성</strong>: 시드 포인트를 중심으로 연속적인 영역을 형성하는데 적합한다.</li>
</ul>
<p>이 알고리즘의 수학적 표현은 인접 포인트 간의 거리 <span class="arithmatex"><span class="MathJax_Preview">d(\mathbf{p}_i, \mathbf{p}_j)</span><script type="math/tex">d(\mathbf{p}_i, \mathbf{p}_j)</script></span>로 나타낼 수 있다. 여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_j</span><script type="math/tex">\mathbf{p}_j</script></span>는 두 포인트를 나타내며, 유사성 기준은 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
d(\mathbf{p}_i, \mathbf{p}_j) &lt; \epsilon
</div>
<script type="math/tex; mode=display">
d(\mathbf{p}_i, \mathbf{p}_j) < \epsilon
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>은 유사성을 정의하는 임계값이다. 이 조건을 만족하는 포인트들은 동일한 클러스터로 그룹화된다.</p>
<h3 id="k-k-means"><strong>K-평균 (K-Means) 알고리즘</strong></h3>
<p>K-평균은 매우 널리 사용되는 분할 알고리즘 중 하나로, 데이터를 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>개의 클러스터로 나누는 데 사용된다. 이 알고리즘은 클러스터의 중심(centroid)을 기준으로 데이터를 분할하며, 반복적인 계산을 통해 클러스터 중심을 업데이트한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>클러스터 수 지정</strong>: 알고리즘이 실행되기 전에 클러스터의 개수 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>를 미리 정해야 한다.</li>
<li><strong>중심 기반 분할</strong>: 각 포인트는 가장 가까운 중심점에 할당되며, 반복적으로 클러스터 중심이 재계산된다.</li>
<li><strong>빠른 계산</strong>: 비교적 빠르게 작동하며, 대규모 데이터셋에서도 효과적이다.</li>
</ul>
<p>K-평균 알고리즘의 수학적 모델은 클러스터의 중심을 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{c}_k</span><script type="math/tex">\mathbf{c}_k</script></span>라 할 때, 각 데이터 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>와 중심 사이의 거리를 최소화하는 방식으로 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\arg \min_{\mathbf{c}_1, \dots, \mathbf{c}_K} \sum_{i=1}^{N} \sum_{k=1}^{K} \mathbf{1}\{\mathbf{p}_i \in \mathcal{C}_k\} \|\mathbf{p}_i - \mathbf{c}_k\|^2
</div>
<script type="math/tex; mode=display">
\arg \min_{\mathbf{c}_1, \dots, \mathbf{c}_K} \sum_{i=1}^{N} \sum_{k=1}^{K} \mathbf{1}\{\mathbf{p}_i \in \mathcal{C}_k\} \|\mathbf{p}_i - \mathbf{c}_k\|^2
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{C}_k</span><script type="math/tex">\mathcal{C}_k</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>번째 클러스터를 나타내며, <span class="arithmatex"><span class="MathJax_Preview">\|\cdot\|</span><script type="math/tex">\|\cdot\|</script></span>는 유클리드 거리이다. 각 반복 단계에서 중심은 포인트들의 평균으로 갱신된다.</p>
<h3 id="dbscan-density-based-spatial-clustering-of-applications-with-noise"><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong></h3>
<p>DBSCAN은 밀도 기반 클러스터링 알고리즘으로, 밀도가 높은 영역에서 클러스터를 찾는 방식으로 동작한다. 이 알고리즘은 클러스터의 개수를 미리 지정할 필요가 없으며, 포인트 간의 밀도 차이를 통해 클러스터를 구분한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>밀도 기반</strong>: 일정 밀도 이상의 포인트들을 클러스터로 간주하고, 밀도가 낮은 포인트들은 이상치로 처리한다.</li>
<li><strong>클러스터 수 자동 결정</strong>: 클러스터의 개수를 미리 지정하지 않고, 밀도에 따라 자동으로 클러스터를 찾는다.</li>
<li><strong>이상치 처리</strong>: 밀도가 낮은 포인트는 이상치로 간주되며, 이는 분할 결과에 강건한 특성을 제공한다.</li>
</ul>
<p>DBSCAN의 수학적 모델은 다음과 같이 정의된다. 주어진 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>에 대해, 반경 <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> 내에서 밀도 기준을 만족하는 이웃 포인트의 개수 <span class="arithmatex"><span class="MathJax_Preview">\text{minPts}</span><script type="math/tex">\text{minPts}</script></span>가 클러스터 형성의 기준이 된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
|\{\mathbf{p}_j \in \mathcal{N}_\epsilon(\mathbf{p}_i) : \|\mathbf{p}_i - \mathbf{p}_j\| &lt; \epsilon\}| \geq \text{minPts}
</div>
<script type="math/tex; mode=display">
|\{\mathbf{p}_j \in \mathcal{N}_\epsilon(\mathbf{p}_i) : \|\mathbf{p}_i - \mathbf{p}_j\| < \epsilon\}| \geq \text{minPts}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{N}_\epsilon(\mathbf{p}_i)</span><script type="math/tex">\mathcal{N}_\epsilon(\mathbf{p}_i)</script></span>는 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>의 <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>-이웃을 나타낸다. 이러한 밀도 기준을 만족하지 못하는 포인트는 이상치로 분류된다.</p>
<h3 id="mean-shift"><strong>Mean Shift 알고리즘</strong></h3>
<p>Mean Shift 알고리즘은 밀도가 높은 영역을 탐색하는 방법으로, 포인트 클라우드에서 밀집된 영역을 클러스터로 구분하는 비지도 학습 알고리즘이다. 이 알고리즘은 포인트들이 있는 밀도 중심으로 이동하며 클러스터를 형성한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>밀도 추정 기반</strong>: 밀도가 높은 지역으로 포인트를 이동시켜 클러스터를 형성한다.</li>
<li><strong>파라미터가 적음</strong>: 다른 알고리즘에 비해 설정해야 하는 파라미터가 적으며, 자동으로 클러스터를 형성할 수 있다.</li>
<li><strong>유연성</strong>: 클러스터의 모양이나 크기에 제한이 없어 다양한 구조를 처리할 수 있다.</li>
</ul>
<p>Mean Shift의 수학적 표현은 다음과 같다. 각 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>에 대해 밀도 중심으로의 이동은 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{p}_i \leftarrow \frac{\sum_{j \in \mathcal{N}_h(\mathbf{p}_i)} \mathbf{p}_j K\left( \frac{\|\mathbf{p}_i - \mathbf{p}_j\|}{h} \right)}{\sum_{j \in \mathcal{N}_h(\mathbf{p}_i)} K\left( \frac{\|\mathbf{p}_i - \mathbf{p}_j\|}{h} \right)}
</div>
<script type="math/tex; mode=display">
\mathbf{p}_i \leftarrow \frac{\sum_{j \in \mathcal{N}_h(\mathbf{p}_i)} \mathbf{p}_j K\left( \frac{\|\mathbf{p}_i - \mathbf{p}_j\|}{h} \right)}{\sum_{j \in \mathcal{N}_h(\mathbf{p}_i)} K\left( \frac{\|\mathbf{p}_i - \mathbf{p}_j\|}{h} \right)}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>는 커널 함수, <span class="arithmatex"><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span>는 커널의 대역폭을 나타내며, 포인트들은 반복적으로 밀집된 중심으로 이동한다.</p>
<h3 id="spectral-clustering"><strong>Spectral Clustering 알고리즘</strong></h3>
<p>Spectral Clustering은 그래프 이론에 기반한 클러스터링 알고리즘으로, 데이터의 기하학적 구조를 파악하는 데 효과적이다. 이 알고리즘은 데이터 포인트 간의 연결성을 나타내는 유사도 그래프를 구성하고, 그래프의 스펙트럼(고유값과 고유벡터)을 이용하여 클러스터링을 수행한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>비선형 구조 탐지</strong>: 비선형적으로 분포된 데이터도 효과적으로 클러스터링할 수 있다.</li>
<li><strong>그래프 이론 활용</strong>: 데이터 포인트 간의 유사도를 그래프의 형태로 표현하고, 이를 통해 클러스터를 찾는다.</li>
<li><strong>고유값 문제</strong>: 고유값 분해를 통해 데이터를 저차원 공간으로 투영하여 클러스터를 구분한다.</li>
</ul>
<p>Spectral Clustering의 수학적 표현은 유사도 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span>를 사용하여 라플라시안 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{L}</span><script type="math/tex">\mathbf{L}</script></span>을 정의하는 것에서 시작된다. 라플라시안 행렬은 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{L} = \mathbf{D} - \mathbf{W}
</div>
<script type="math/tex; mode=display">
\mathbf{L} = \mathbf{D} - \mathbf{W}
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{D}</span><script type="math/tex">\mathbf{D}</script></span>는 유사도 그래프의 대각행렬로, 각 대각 원소 <span class="arithmatex"><span class="MathJax_Preview">D_{ii}</span><script type="math/tex">D_{ii}</script></span>는 노드 <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>에 연결된 간선의 합을 나타낸다. Spectral Clustering은 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{L}</span><script type="math/tex">\mathbf{L}</script></span>의 가장 작은 고유값에 대응하는 고유벡터들을 사용하여 데이터를 클러스터링한다.</p>
<h3 id="7-hierarchical-clustering">7. <strong>Hierarchical Clustering (계층적 클러스터링)</strong></h3>
<p>계층적 클러스터링은 데이터 포인트 간의 유사성에 따라 클러스터를 계층적으로 구성하는 알고리즘이다. 이 알고리즘은 병합(agglomerative) 방식과 분리(divisive) 방식으로 나뉘며, 클러스터 간의 유사성 또는 거리 기준으로 클러스터를 병합하거나 분리한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>계층 구조</strong>: 클러스터들이 트리 구조로 형성되며, 덴드로그램(dendrogram) 형태로 표현된다.</li>
<li><strong>병합과 분리</strong>: 병합 방식에서는 작은 클러스터를 차례로 병합하여 큰 클러스터를 형성하고, 분리 방식에서는 큰 클러스터를 점차 작은 클러스터로 분리한다.</li>
<li><strong>유연한 클러스터링</strong>: 트리의 깊이에 따라 클러스터의 수를 유연하게 조정할 수 있다.</li>
</ul>
<p>계층적 클러스터링에서 병합 과정은 두 클러스터 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{C}_i</span><script type="math/tex">\mathcal{C}_i</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{C}_j</span><script type="math/tex">\mathcal{C}_j</script></span> 간의 거리 <span class="arithmatex"><span class="MathJax_Preview">d(\mathcal{C}_i, \mathcal{C}_j)</span><script type="math/tex">d(\mathcal{C}_i, \mathcal{C}_j)</script></span>를 최소화하는 방식으로 진행된다. 클러스터 간의 거리는 여러 방식으로 정의될 수 있으며, 가장 일반적인 방식은 다음과 같다:</p>
<ul>
<li><strong>단일 연결법</strong>: 두 클러스터 간의 가장 가까운 포인트들 간의 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{single}}(\mathcal{C}_i, \mathcal{C}_j) = \min_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{single}}(\mathcal{C}_i, \mathcal{C}_j) = \min_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<ul>
<li><strong>완전 연결법</strong>: 두 클러스터 간의 가장 먼 포인트들 간의 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{complete}}(\mathcal{C}_i, \mathcal{C}_j) = \max_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{complete}}(\mathcal{C}_i, \mathcal{C}_j) = \max_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<ul>
<li><strong>평균 연결법</strong>: 클러스터 내의 모든 포인트 쌍의 평균 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{average}}(\mathcal{C}_i, \mathcal{C}_j) = \frac{1}{|\mathcal{C}_i||\mathcal{C}_j|} \sum_{\mathbf{p} \in \mathcal{C}_i} \sum_{\mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{average}}(\mathcal{C}_i, \mathcal{C}_j) = \frac{1}{|\mathcal{C}_i||\mathcal{C}_j|} \sum_{\mathbf{p} \in \mathcal{C}_i} \sum_{\mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<p>이러한 방식으로 클러스터들이 병합되며, 덴드로그램을 통해 클러스터 구조를 시각화할 수 있다.</p>
<h3 id="gaussian-mixture-model-gmm"><strong>Gaussian Mixture Model (GMM) 알고리즘</strong></h3>
<p>Gaussian Mixture Model (GMM)은 확률 모델을 기반으로 클러스터링을 수행하는 알고리즘이다. GMM은 각 클러스터가 가우시안 분포(정규 분포)를 따르는 것으로 가정하고, 데이터를 여러 개의 가우시안 분포로 모델링한다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>확률 기반 분할</strong>: 각 데이터 포인트가 특정 클러스터에 속할 확률을 계산하여 클러스터링을 수행한다.</li>
<li><strong>가우시안 분포 가정</strong>: 클러스터들이 가우시안 분포로 표현되며, 각 클러스터의 평균과 공분산 행렬을 통해 클러스터의 모양과 크기가 결정된다.</li>
<li><strong>Expectation-Maximization (EM) 알고리즘 사용</strong>: GMM은 EM 알고리즘을 사용하여 반복적으로 가우시안 분포의 파라미터를 최적화한다.</li>
</ul>
<p>GMM의 수학적 모델은 각 데이터 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>가 여러 가우시안 분포의 혼합으로부터 발생한다고 가정한다. 데이터의 확률 밀도 함수는 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(\mathbf{p}_i) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
</div>
<script type="math/tex; mode=display">
P(\mathbf{p}_i) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">\pi_k</span><script type="math/tex">\pi_k</script></span>는 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>번째 가우시안 분포의 혼합 계수, <span class="arithmatex"><span class="MathJax_Preview">\mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)</span><script type="math/tex">\mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)</script></span>는 평균 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\mu}_k</span><script type="math/tex">\mathbf{\mu}_k</script></span>와 공분산 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\Sigma}_k</span><script type="math/tex">\mathbf{\Sigma}_k</script></span>를 가지는 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>번째 가우시안 분포의 확률 밀도 함수이다.</p>
<p>EM 알고리즘은 다음과 같은 두 단계로 이루어진다:</p>
<ol>
<li><strong>Expectation (E) 단계</strong>: 각 데이터 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>가 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>번째 가우시안 분포에 속할 확률을 계산한다.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_j, \mathbf{\Sigma}_j)}
</div>
<script type="math/tex; mode=display">
\gamma_{ik} = \frac{\pi_k \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_k, \mathbf{\Sigma}_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(\mathbf{p}_i | \mathbf{\mu}_j, \mathbf{\Sigma}_j)}
</script>
</div>
<ol>
<li><strong>Maximization (M) 단계</strong>: 각 가우시안 분포의 평균 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\mu}_k</span><script type="math/tex">\mathbf{\mu}_k</script></span>, 공분산 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{\Sigma}_k</span><script type="math/tex">\mathbf{\Sigma}_k</script></span>, 및 혼합 계수 <span class="arithmatex"><span class="MathJax_Preview">\pi_k</span><script type="math/tex">\pi_k</script></span>를 최적화한다.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{\mu}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} \mathbf{p}_i}{\sum_{i=1}^{N} \gamma_{ik}}, \quad \mathbf{\Sigma}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (\mathbf{p}_i - \mathbf{\mu}_k)(\mathbf{p}_i - \mathbf{\mu}_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}, \quad \pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
</div>
<script type="math/tex; mode=display">
\mathbf{\mu}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} \mathbf{p}_i}{\sum_{i=1}^{N} \gamma_{ik}}, \quad \mathbf{\Sigma}_k = \frac{\sum_{i=1}^{N} \gamma_{ik} (\mathbf{p}_i - \mathbf{\mu}_k)(\mathbf{p}_i - \mathbf{\mu}_k)^T}{\sum_{i=1}^{N} \gamma_{ik}}, \quad \pi_k = \frac{1}{N} \sum_{i=1}^{N} \gamma_{ik}
</script>
</div>
<p>이 과정을 반복하여 GMM의 파라미터가 수렴할 때까지 클러스터링을 수행한다.</p>
<h3 id="fuzzy-c-means-fcm"><strong>Fuzzy C-Means (FCM) 알고리즘</strong></h3>
<p>Fuzzy C-Means는 퍼지 클러스터링 기법으로, 데이터 포인트가 여러 클러스터에 소속될 수 있는 확률을 기반으로 클러스터링을 수행한다. 이는 K-평균과 유사하지만, 각 데이터 포인트가 오직 하나의 클러스터에 할당되는 것이 아니라, 여러 클러스터에 속할 수 있는 소속도 값을 갖는다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>퍼지 소속도</strong>: 각 포인트는 여러 클러스터에 부분적으로 속할 수 있으며, 소속도 값으로 표현된다.</li>
<li><strong>유연성</strong>: 데이터가 명확하게 구분되지 않을 때, 클러스터링 결과에 유연성을 부여한다.</li>
<li><strong>중심 기반 분할</strong>: K-평균과 유사하게 각 클러스터의 중심을 기반으로 데이터를 분할하지만, 소속도 값이 중심을 업데이트하는 데 반영된다.</li>
</ul>
<p>FCM의 수학적 모델은 각 데이터 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>와 클러스터 중심 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{c}_k</span><script type="math/tex">\mathbf{c}_k</script></span> 간의 거리를 최소화하는 방식으로 정의된다. FCM의 목적 함수는 다음과 같이 표현된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J = \sum_{i=1}^{N} \sum_{k=1}^{K} u_{ik}^m \|\mathbf{p}_i - \mathbf{c}_k\|^2
</div>
<script type="math/tex; mode=display">
J = \sum_{i=1}^{N} \sum_{k=1}^{K} u_{ik}^m \|\mathbf{p}_i - \mathbf{c}_k\|^2
</script>
</div>
<p>여기서 <span class="arithmatex"><span class="MathJax_Preview">u_{ik}</span><script type="math/tex">u_{ik}</script></span>는 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>가 클러스터 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>에 속할 소속도 값, <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>은 퍼지화 지수로, 일반적으로 2로 설정된다. 소속도 <span class="arithmatex"><span class="MathJax_Preview">u_{ik}</span><script type="math/tex">u_{ik}</script></span>는 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
u_{ik} = \frac{1}{\sum_{j=1}^{K} \left( \frac{\|\mathbf{p}_i - \mathbf{c}_k\|}{\|\mathbf{p}_i - \mathbf{c}_j\|} \right)^{\frac{2}{m-1}}}
</div>
<script type="math/tex; mode=display">
u_{ik} = \frac{1}{\sum_{j=1}^{K} \left( \frac{\|\mathbf{p}_i - \mathbf{c}_k\|}{\|\mathbf{p}_i - \mathbf{c}_j\|} \right)^{\frac{2}{m-1}}}
</script>
</div>
<p>이 과정은 클러스터 중심과 소속도 값이 수렴할 때까지 반복된다.</p>
<h3 id="agglomerative-clustering"><strong>Agglomerative Clustering (병합적 클러스터링)</strong></h3>
<p>Agglomerative Clustering은 계층적 클러스터링 방법 중 하나로, 초기에는 각 데이터 포인트를 독립적인 클러스터로 시작하여, 점진적으로 유사한 클러스터들을 병합해가는 방식이다. 이 방식은 트리 구조인 덴드로그램을 통해 클러스터링 과정을 시각화할 수 있다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>하향식 접근</strong>: 클러스터가 병합될수록 더 큰 클러스터가 생성되며, 최종적으로 하나의 클러스터가 될 때까지 병합이 진행된다.</li>
<li><strong>유연한 거리 측정 방식</strong>: 클러스터 간의 유사성을 측정하는 방식에 따라 다양한 클러스터링 결과를 얻을 수 있다.</li>
<li><strong>덴드로그램 기반</strong>: 클러스터 간의 병합 순서를 시각적으로 표현하여 데이터 간의 관계를 파악할 수 있다.</li>
</ul>
<p>Agglomerative Clustering의 수학적 표현은 두 클러스터 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{C}_i</span><script type="math/tex">\mathcal{C}_i</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{C}_j</span><script type="math/tex">\mathcal{C}_j</script></span> 간의 거리를 측정하는 방식에 따라 다르다. 가장 일반적인 거리 측정 방식은 다음과 같다:</p>
<ul>
<li><strong>단일 연결법 (Single Linkage)</strong>: 두 클러스터 간의 최소 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{single}}(\mathcal{C}_i, \mathcal{C}_j) = \min_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{single}}(\mathcal{C}_i, \mathcal{C}_j) = \min_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<ul>
<li><strong>완전 연결법 (Complete Linkage)</strong>: 두 클러스터 간의 최대 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{complete}}(\mathcal{C}_i, \mathcal{C}_j) = \max_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{complete}}(\mathcal{C}_i, \mathcal{C}_j) = \max_{\mathbf{p} \in \mathcal{C}_i, \mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<ul>
<li><strong>평균 연결법 (Average Linkage)</strong>: 두 클러스터의 모든 포인트 쌍 간의 평균 거리</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
d_{\text{average}}(\mathcal{C}_i, \mathcal{C}_j) = \frac{1}{|\mathcal{C}_i||\mathcal{C}_j|} \sum_{\mathbf{p} \in \mathcal{C}_i} \sum_{\mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</div>
<script type="math/tex; mode=display">
d_{\text{average}}(\mathcal{C}_i, \mathcal{C}_j) = \frac{1}{|\mathcal{C}_i||\mathcal{C}_j|} \sum_{\mathbf{p} \in \mathcal{C}_i} \sum_{\mathbf{q} \in \mathcal{C}_j} \|\mathbf{p} - \mathbf{q}\|
</script>
</div>
<p>덴드로그램을 통해 클러스터들이 병합되는 과정을 시각적으로 확인할 수 있으며, 최종적으로 얻은 클러스터의 수는 덴드로그램에서 특정 수준의 병합을 잘라내는 방식으로 결정할 수 있다.</p>
<h3 id="optics-ordering-points-to-identify-the-clustering-structure"><strong>OPTICS (Ordering Points To Identify the Clustering Structure)</strong></h3>
<p>OPTICS는 DBSCAN 알고리즘과 유사한 밀도 기반 클러스터링 알고리즘으로, 밀집된 영역을 기반으로 클러스터를 형성하는 방식이다. 그러나 OPTICS는 DBSCAN과 달리 클러스터의 크기나 밀도에 따라 클러스터의 개수를 자동으로 결정할 수 있으며, 다양한 밀도를 가지는 클러스터도 효과적으로 처리할 수 있다. 주요 특징은 다음과 같다:</p>
<ul>
<li><strong>다양한 밀도 처리</strong>: 다양한 밀도를 가진 클러스터들도 효과적으로 탐지할 수 있다.</li>
<li><strong>밀도 기반 정렬</strong>: 데이터 포인트를 밀도 기반으로 정렬하여 클러스터 구조를 파악한다.</li>
<li><strong>파라미터 설정의 유연성</strong>: DBSCAN과 달리 클러스터의 크기나 밀도에 따른 민감성을 조절할 수 있는 파라미터 설정이 유연한다.</li>
</ul>
<p>OPTICS의 수학적 모델은 다음과 같이 정의된다. 각 포인트 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{p}_i</span><script type="math/tex">\mathbf{p}_i</script></span>에 대해, 반경 <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> 내에서 충분한 밀도(<span class="arithmatex"><span class="MathJax_Preview">\text{minPts}</span><script type="math/tex">\text{minPts}</script></span>)를 만족하는 이웃 포인트를 찾는다. 각 포인트의 핵심 거리(core distance)와 도달 가능 거리(reachability distance)는 다음과 같이 계산된다:</p>
<ul>
<li>핵심 거리:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\text{core distance}(\mathbf{p}_i) = \min_{\mathbf{p}_j \in \mathcal{N}_\epsilon(\mathbf{p}_i)} \|\mathbf{p}_i - \mathbf{p}_j\|
</div>
<script type="math/tex; mode=display">
\text{core distance}(\mathbf{p}_i) = \min_{\mathbf{p}_j \in \mathcal{N}_\epsilon(\mathbf{p}_i)} \|\mathbf{p}_i - \mathbf{p}_j\|
</script>
</div>
<ul>
<li>도달 가능 거리:</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\text{reachability distance}(\mathbf{p}_i, \mathbf{p}_j) = \max(\text{core distance}(\mathbf{p}_i), \|\mathbf{p}_i - \mathbf{p}_j\|)
</div>
<script type="math/tex; mode=display">
\text{reachability distance}(\mathbf{p}_i, \mathbf{p}_j) = \max(\text{core distance}(\mathbf{p}_i), \|\mathbf{p}_i - \mathbf{p}_j\|)
</script>
</div>
<p>OPTICS는 데이터를 이러한 거리 기준으로 정렬하여 클러스터를 자동으로 식별하며, 다양한 밀도를 가진 클러스터들을 처리할 수 있다.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_06/0601/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_06/0601/" class="btn btn-xs btn-link">
        포인트 특징의 개념과 정의
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0504/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0504/" class="btn btn-xs btn-link">
        유사성 기반 클러스터링 기법
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>