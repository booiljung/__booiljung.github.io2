<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    <link rel="canonical" href="https://booiljung.github.io/sensor_data_processing/image/camera_calibration/chapter_06/0603/">
    <link rel="shortcut icon" href="../../../../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Levenberg-Marquardt 최적화 - 소프트웨어 융합</title>
    <link href="../../../../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../../../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../../../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../../../../css/highlight.css">
    <link href="../../../../../css/custom.css" rel="stylesheet">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../../../../js/jquery-3.2.1.min.js"></script>
    <script src="../../../../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../../../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '../../../../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "\ube44\uc120\ud615 \ucd5c\uc18c\uc790\uc2b9 \ubb38\uc81c \uc815\uc758", url: "#_top", children: [
          ]},
          {title: "Gauss-Newton \ubc29\ubc95\uacfc Levenberg-Marquardt\uc758 \ucc28\uc774\uc810", url: "#gauss-newton-levenberg-marquardt", children: [
          ]},
          {title: "Jacobian \ud589\ub82c \uacc4\uc0b0", url: "#jacobian", children: [
          ]},
          {title: "\uc5c5\ub370\uc774\ud2b8 \uaddc\uce59", url: "#_2", children: [
          ]},
          {title: "\uac10\uc1e0 \uacc4\uc218 \\lambda\\lambda\uc758 \uc870\uc815", url: "#lambdalambda", children: [
          ]},
          {title: "\uc218\ub834 \uc870\uac74", url: "#_3", children: [
          ]},
          {title: "Levenberg-Marquardt \uc54c\uace0\ub9ac\uc998\uc758 \uc7a5\uc810", url: "#levenberg-marquardt", children: [
          ]},
        ];

    </script>
    <script src="../../../../../js/base.js"></script>
      <script src="../../../../../js/google_analytics.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="https://www.googletagmanager.com/gtag/js?id=G-3F4LHCTF88"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_07/0701/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_07/0701/" class="btn btn-xs btn-link">
        OpenCV를 활용한 카메라 캘리브레이션
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0602/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0602/" class="btn btn-xs btn-link">
        Gauss-Newton 방법
      </a>
    </div>
    
  </div>

    

    <p>Levenberg-Marquardt 알고리즘은 비선형 최소자승 문제를 해결하는데 주로 사용되는 최적화 기법으로, 카메라 캘리브레이션에서 매개변수 최적화에 매우 유용하다. 이 알고리즘은 Gauss-Newton 방법과 Gradient Descent 방법의 장점을 결합한 방식으로, 안정성과 수렴 속도를 모두 고려한 비선형 최적화에 적합한다.</p>
<h3 id="_1">비선형 최소자승 문제 정의</h3>
<p>카메라 캘리브레이션 과정에서 우리는 모델 파라미터를 추정하여 관찰된 데이터와 모델이 예측하는 데이터 간의 차이를 최소화하려고 한다. 이 과정은 다음과 같은 비선형 최소자승 문제로 정의할 수 있다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\mathbf{x}} \sum_{i=1}^{n} r_i(\mathbf{x})^2
</div>
<script type="math/tex; mode=display">
\min_{\mathbf{x}} \sum_{i=1}^{n} r_i(\mathbf{x})^2
</script>
</div>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>는 최적화해야 할 파라미터 벡터
- <span class="arithmatex"><span class="MathJax_Preview">r_i(\mathbf{x})</span><script type="math/tex">r_i(\mathbf{x})</script></span>는 관찰값과 모델 예측값 간의 잔차(residual)이다.</p>
<h3 id="gauss-newton-levenberg-marquardt">Gauss-Newton 방법과 Levenberg-Marquardt의 차이점</h3>
<p>Gauss-Newton 방법은 잔차 함수의 Jacobian을 사용하여 비선형 최소자승 문제를 해결한다. 그러나, Gauss-Newton 방법은 잔차가 비선형일 때 수렴이 어려울 수 있다. 이러한 문제를 해결하기 위해 Levenberg-Marquardt 방법은 다음과 같이 수정된 해를 제안한다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
(\mathbf{J}^\top \mathbf{J} + \lambda \mathbf{I}) \Delta \mathbf{x} = -\mathbf{J}^\top \mathbf{r}
</div>
<script type="math/tex; mode=display">
(\mathbf{J}^\top \mathbf{J} + \lambda \mathbf{I}) \Delta \mathbf{x} = -\mathbf{J}^\top \mathbf{r}
</script>
</div>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">\mathbf{J}</span><script type="math/tex">\mathbf{J}</script></span>는 잔차 <span class="arithmatex"><span class="MathJax_Preview">r_i(\mathbf{x})</span><script type="math/tex">r_i(\mathbf{x})</script></span>의 Jacobian 행렬
- <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>는 가변적인 감쇠 계수(damping factor)
- <span class="arithmatex"><span class="MathJax_Preview">\mathbf{I}</span><script type="math/tex">\mathbf{I}</script></span>는 항등 행렬
- <span class="arithmatex"><span class="MathJax_Preview">\Delta \mathbf{x}</span><script type="math/tex">\Delta \mathbf{x}</script></span>는 파라미터의 업데이트 값
- <span class="arithmatex"><span class="MathJax_Preview">\mathbf{r}</span><script type="math/tex">\mathbf{r}</script></span>는 잔차 벡터</p>
<p>Levenberg-Marquardt 알고리즘은 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 조절하여 Gauss-Newton과 Gradient Descent 방법 사이에서 동적으로 전환한다. <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 값이 작으면 Gauss-Newton 방법과 유사하게 동작하며, <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 값이 크면 Gradient Descent와 유사하게 동작한다. 이를 통해 수렴이 느린 문제에서도 안정적인 수렴을 보장한다.</p>
<h3 id="jacobian">Jacobian 행렬 계산</h3>
<p>Jacobian 행렬 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{J}</span><script type="math/tex">\mathbf{J}</script></span>는 잔차 함수 <span class="arithmatex"><span class="MathJax_Preview">r_i(\mathbf{x})</span><script type="math/tex">r_i(\mathbf{x})</script></span>를 파라미터 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}</span><script type="math/tex">\mathbf{x}</script></span>에 대해 편미분한 값들로 구성된다. 즉, 각 잔차 <span class="arithmatex"><span class="MathJax_Preview">r_i</span><script type="math/tex">r_i</script></span>에 대한 파라미터들의 기울기로 이루어진 행렬이다. 이 Jacobian은 다음과 같이 정의된다:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathbf{J} = 
\begin{bmatrix}
\frac{\partial r_1}{\partial x_1} &amp; \frac{\partial r_1}{\partial x_2} &amp; \dots &amp; \frac{\partial r_1}{\partial x_n} \\
\frac{\partial r_2}{\partial x_1} &amp; \frac{\partial r_2}{\partial x_2} &amp; \dots &amp; \frac{\partial r_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial r_m}{\partial x_1} &amp; \frac{\partial r_m}{\partial x_2} &amp; \dots &amp; \frac{\partial r_m}{\partial x_n} \\
\end{bmatrix}
</div>
<script type="math/tex; mode=display">
\mathbf{J} = 
\begin{bmatrix}
\frac{\partial r_1}{\partial x_1} & \frac{\partial r_1}{\partial x_2} & \dots & \frac{\partial r_1}{\partial x_n} \\
\frac{\partial r_2}{\partial x_1} & \frac{\partial r_2}{\partial x_2} & \dots & \frac{\partial r_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial r_m}{\partial x_1} & \frac{\partial r_m}{\partial x_2} & \dots & \frac{\partial r_m}{\partial x_n} \\
\end{bmatrix}
</script>
</div>
<p>여기서:
- <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>은 관찰된 데이터의 수 (즉, 잔차의 수)
- <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>은 최적화할 파라미터의 수</p>
<h3 id="_2">업데이트 규칙</h3>
<p>Levenberg-Marquardt 알고리즘은 다음과 같은 업데이트 규칙을 따른다:</p>
<ol>
<li>초기 파라미터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_0</span><script type="math/tex">\mathbf{x}_0</script></span>와 감쇠 계수 <span class="arithmatex"><span class="MathJax_Preview">\lambda_0</span><script type="math/tex">\lambda_0</script></span>를 설정한다.</li>
<li>현재 파라미터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_k</span><script type="math/tex">\mathbf{x}_k</script></span>에서 잔차 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{r}(\mathbf{x}_k)</span><script type="math/tex">\mathbf{r}(\mathbf{x}_k)</script></span>와 Jacobian <span class="arithmatex"><span class="MathJax_Preview">\mathbf{J}_k</span><script type="math/tex">\mathbf{J}_k</script></span>를 계산한다.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Delta \mathbf{x}_k</span><script type="math/tex">\Delta \mathbf{x}_k</script></span>를 다음 식을 통해 계산한다:</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
(\mathbf{J}_k^\top \mathbf{J}_k + \lambda_k \mathbf{I}) \Delta \mathbf{x}_k = -\mathbf{J}_k^\top \mathbf{r}_k
</div>
<script type="math/tex; mode=display">
(\mathbf{J}_k^\top \mathbf{J}_k + \lambda_k \mathbf{I}) \Delta \mathbf{x}_k = -\mathbf{J}_k^\top \mathbf{r}_k
</script>
</div>
<ol>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_{k+1} = \mathbf{x}_k + \Delta \mathbf{x}_k</span><script type="math/tex">\mathbf{x}_{k+1} = \mathbf{x}_k + \Delta \mathbf{x}_k</script></span>로 파라미터를 업데이트한다.</li>
<li>잔차의 크기에 따라 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 조정한다:</li>
<li>만약 잔차가 줄어들면 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 줄이고 Gauss-Newton 방법에 가깝게 만든다.</li>
<li>잔차가 줄어들지 않으면 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 늘려 Gradient Descent 방식에 가깝게 만든다.</li>
</ol>
<h3 id="lambdalambda">감쇠 계수 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>의 조정</h3>
<p>Levenberg-Marquardt 알고리즘의 핵심 요소 중 하나는 감쇠 계수 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>의 조정 방식이다. <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>는 최적화 과정에서 중요한 역할을 하며, 문제의 비선형성에 따라 그 값이 동적으로 조정된다.</p>
<ul>
<li><strong>잔차가 줄어드는 경우:</strong> </li>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_{k+1}</span><script type="math/tex">\mathbf{x}_{k+1}</script></span>에서의 잔차가 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_k</span><script type="math/tex">\mathbf{x}_k</script></span>에서의 잔차보다 작아지면, 모델은 목표 함수의 최소값에 더 가까워졌다는 의미이다.</li>
<li>이 경우, Gauss-Newton 방법의 영향을 강화하기 위해 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 감소시킨다. <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>가 0에 가까워질수록, 업데이트 식은 Gauss-Newton 방법과 비슷하게 작동한다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\lambda_{k+1} = \frac{\lambda_k}{\nu}, \quad \nu &gt; 1
</div>
<script type="math/tex; mode=display">
\lambda_{k+1} = \frac{\lambda_k}{\nu}, \quad \nu > 1
</script>
</div>
<ul>
<li><strong>잔차가 줄어들지 않는 경우:</strong> </li>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_{k+1}</span><script type="math/tex">\mathbf{x}_{k+1}</script></span>에서 잔차가 줄어들지 않으면, Newton의 방법은 수렴하지 않을 가능성이 높다는 것을 의미한다.</li>
<li>이 경우, 더 안정적인 Gradient Descent 방법으로 전환하기 위해 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>를 증가시킨다. <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>가 커질수록 항등 행렬의 영향이 커지며, 이는 Gradient Descent와 유사하게 작동한다.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\lambda_{k+1} = \lambda_k \cdot \nu, \quad \nu &gt; 1
</div>
<script type="math/tex; mode=display">
\lambda_{k+1} = \lambda_k \cdot \nu, \quad \nu > 1
</script>
</div>
<h3 id="_3">수렴 조건</h3>
<p>Levenberg-Marquardt 알고리즘은 반복적인 갱신을 통해 최적화 문제를 해결하지만, 최적화가 완료되는 시점을 결정하는 수렴 조건이 필요하다. 일반적으로 다음과 같은 조건을 만족할 때 알고리즘은 종료된다:</p>
<ol>
<li><strong>목표 함수의 변화량이 매우 작을 때:</strong></li>
<li>목표 함수의 변화량 <span class="arithmatex"><span class="MathJax_Preview">|\mathbf{r}(\mathbf{x}_k) - \mathbf{r}(\mathbf{x}_{k+1})|</span><script type="math/tex">|\mathbf{r}(\mathbf{x}_k) - \mathbf{r}(\mathbf{x}_{k+1})|</script></span>이 사전에 정의된 임계값 <span class="arithmatex"><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span>보다 작을 때.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
|\mathbf{r}(\mathbf{x}_k) - \mathbf{r}(\mathbf{x}_{k+1})| &lt; \epsilon
</div>
<script type="math/tex; mode=display">
|\mathbf{r}(\mathbf{x}_k) - \mathbf{r}(\mathbf{x}_{k+1})| < \epsilon
</script>
</div>
<ol>
<li><strong>파라미터 벡터의 변화량이 매우 작을 때:</strong></li>
<li>파라미터 벡터 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_k</span><script type="math/tex">\mathbf{x}_k</script></span>와 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{x}_{k+1}</span><script type="math/tex">\mathbf{x}_{k+1}</script></span> 간의 변화량이 매우 작으면, 파라미터가 거의 수렴한 것으로 간주한다.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
|\mathbf{x}_k - \mathbf{x}_{k+1}| &lt; \epsilon
</div>
<script type="math/tex; mode=display">
|\mathbf{x}_k - \mathbf{x}_{k+1}| < \epsilon
</script>
</div>
<ol>
<li><strong>최대 반복 횟수에 도달했을 때:</strong></li>
<li>주어진 반복 횟수 <span class="arithmatex"><span class="MathJax_Preview">K_{max}</span><script type="math/tex">K_{max}</script></span>에 도달하면 알고리즘은 자동으로 종료된다. 이 방법은 수렴하지 않는 문제에서 무한 루프를 방지하는 역할을 한다.</li>
</ol>
<h3 id="levenberg-marquardt">Levenberg-Marquardt 알고리즘의 장점</h3>
<p>Levenberg-Marquardt 알고리즘의 가장 큰 장점은 비선형 문제에 대한 빠른 수렴 속도와 안정성이다. 특히, 다음과 같은 점에서 유용하다:</p>
<ul>
<li><strong>빠른 수렴:</strong> 초기 근사값이 목표값에 충분히 가깝다면, Gauss-Newton 방법의 특성 덕분에 빠르게 수렴한다.</li>
<li><strong>안정성:</strong> 초기 근사값이 목표값에서 멀어도 Gradient Descent 방식으로 전환하여 안정적인 수렴을 보장한다.</li>
<li><strong>비선형성 처리:</strong> 비선형 문제에서의 잔차 함수의 특성을 잘 반영하여 보다 정확한 결과를 도출할 수 있다.</li>
</ul>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../chapter_07/0701/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../chapter_07/0701/" class="btn btn-xs btn-link">
        OpenCV를 활용한 카메라 캘리브레이션
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../0602/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../0602/" class="btn btn-xs btn-link">
        Gauss-Newton 방법
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>